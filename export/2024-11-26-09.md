# reinforcement learning
## Self-Generated Critiques Boost Reward Modeling for Language Models
- **Url**: http://arxiv.org/abs/2411.16646v1
- **Authors**: ['Yue Yu', 'Zhengxing Chen', 'Aston Zhang', 'Liang Tan', 'Chenguang Zhu', 'Richard Yuanzhe Pang', 'Yundi Qian', 'Xuewei Wang', 'Suchin Gururangan', 'Chao Zhang', 'Melanie Kambadur', 'Dhruv Mahajan', 'Rui Hou']
- **Abstrat**: Reward modeling is crucial for aligning large language models (LLMs) with human preferences, especially in reinforcement learning from human feedback (RLHF). However, current reward models mainly produce scalar scores and struggle to incorporate critiques in a natural language format. We hypothesize that predicting both critiques and the scalar reward would improve reward modeling ability. Motivated by this, we propose Critic-RM, a framework that improves reward models using self-generated critiques without extra supervision. Critic-RM employs a two-stage process: generating and filtering high-quality critiques, followed by joint fine-tuning on reward prediction and critique generation. Experiments across benchmarks show that Critic-RM improves reward modeling accuracy by 3.7%-7.3% compared to standard reward models and LLM judges, demonstrating strong performance and data efficiency. Additional studies further validate the effectiveness of generated critiques in rectifying flawed reasoning steps with 2.5%-3.2% gains in improving reasoning accuracy.





## Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions
- **Url**: http://arxiv.org/abs/2411.14405v2
- **Authors**: ['Yu Zhao', 'Huifeng Yin', 'Bo Zeng', 'Hao Wang', 'Tianqi Shi', 'Chenyang Lyu', 'Longyue Wang', 'Weihua Luo', 'Kaifu Zhang']
- **Abstrat**: Currently OpenAI o1 sparks a surge of interest in the study of large reasoning models (LRM). Building on this momentum, Marco-o1 not only focuses on disciplines with standard answers, such as mathematics, physics, and coding --which are well-suited for reinforcement learning (RL) -- but also places greater emphasis on open-ended resolutions. We aim to address the question: ''Can the o1 model effectively generalize to broader domains where clear standards are absent and rewards are challenging to quantify?'' Marco-o1 is powered by Chain-of-Thought (CoT) fine-tuning, Monte Carlo Tree Search (MCTS), reflection mechanisms, and innovative reasoning strategies -- optimized for complex real-world problem-solving tasks.





## BenchMARL: Benchmarking Multi-Agent Reinforcement Learning
- **Url**: http://arxiv.org/abs/2312.01472v3
- **Authors**: ['Matteo Bettini', 'Amanda Prorok', 'Vincent Moens']
- **Abstrat**: The field of Multi-Agent Reinforcement Learning (MARL) is currently facing a reproducibility crisis. While solutions for standardized reporting have been proposed to address the issue, we still lack a benchmarking tool that enables standardization and reproducibility, while leveraging cutting-edge Reinforcement Learning (RL) implementations. In this paper, we introduce BenchMARL, the first MARL training library created to enable standardized benchmarking across different algorithms, models, and environments. BenchMARL uses TorchRL as its backend, granting it high performance and maintained state-of-the-art implementations while addressing the broad community of MARL PyTorch users. Its design enables systematic configuration and reporting, thus allowing users to create and run complex benchmarks from simple one-line inputs. BenchMARL is open-sourced on GitHub: https://github.com/facebookresearch/BenchMARL





## Accelerating Task Generalisation with Multi-Level Hierarchical Options
- **Url**: http://arxiv.org/abs/2411.02998v2
- **Authors**: ['Thomas P Cannon', 'Özgür Simsek']
- **Abstrat**: Creating reinforcement learning agents that generalise effectively to new tasks is a key challenge in AI research. This paper introduces Fracture Cluster Options (FraCOs), a multi-level hierarchical reinforcement learning method that achieves state-of-the-art performance on difficult generalisation tasks. FraCOs identifies patterns in agent behaviour and forms options based on the expected future usefulness of those patterns, enabling rapid adaptation to new tasks. In tabular settings, FraCOs demonstrates effective transfer and improves performance as it grows in hierarchical depth. We evaluate FraCOs against state-of-the-art deep reinforcement learning algorithms in several complex procedurally generated environments. Our results show that FraCOs achieves higher in-distribution and out-of-distribution performance than competitors.





## Reinforcement Learning for Bidding Strategy Optimization in Day-Ahead Energy Market
- **Url**: http://arxiv.org/abs/2411.16519v1
- **Authors**: ['Luca Di Persio', 'Matteo Garbelli', 'Luca M. Giordano']
- **Abstrat**: In a day-ahead market, energy buyers and sellers submit their bids for a particular future time, including the amount of energy they wish to buy or sell and the price they are prepared to pay or receive. However, the dynamic for forming the Market Clearing Price (MCP) dictated by the bidding mechanism is frequently overlooked in the literature on energy market modelling. Forecasting models usually focus on predicting the MCP rather than trying to build the optimal supply and demand curves for a given price scenario. Following this approach, the article focuses on developing a bidding strategy for a seller in a continuous action space through a single agent Reinforcement Learning algorithm, specifically the Deep Deterministic Policy Gradient. The algorithm controls the offering curve (action) based on past data (state) to optimize future payoffs (rewards). The participant can access historical data on production costs, capacity, and prices for various sources, including renewable and fossil fuels. The participant gains the ability to operate in the market with greater efficiency over time to maximize individual payout.





## OffLight: An Offline Multi-Agent Reinforcement Learning Framework for Traffic Signal Control
- **Url**: http://arxiv.org/abs/2411.06601v2
- **Authors**: ['Rohit Bokade', 'Xiaoning Jin']
- **Abstrat**: Efficient traffic control (TSC) is essential for urban mobility, but traditional systems struggle to handle the complexity of real-world traffic. Multi-agent Reinforcement Learning (MARL) offers adaptive solutions, but online MARL requires extensive interactions with the environment, making it costly and impractical. Offline MARL mitigates these challenges by using historical traffic data for training but faces significant difficulties with heterogeneous behavior policies in real-world datasets, where mixed-quality data complicates learning. We introduce OffLight, a novel offline MARL framework designed to handle heterogeneous behavior policies in TSC datasets. To improve learning efficiency, OffLight incorporates Importance Sampling (IS) to correct for distributional shifts and Return-Based Prioritized Sampling (RBPS) to focus on high-quality experiences. OffLight utilizes a Gaussian Mixture Variational Graph Autoencoder (GMM-VGAE) to capture the diverse distribution of behavior policies from local observations. Extensive experiments across real-world urban traffic scenarios show that OffLight outperforms existing offline RL methods, achieving up to a 7.8% reduction in average travel time and 11.2% decrease in queue length. Ablation studies confirm the effectiveness of OffLight's components in handling heterogeneous data and improving policy performance. These results highlight OffLight's scalability and potential to improve urban traffic management without the risks of online learning.





## Language Grounded Multi-agent Reinforcement Learning with Human-interpretable Communication
- **Url**: http://arxiv.org/abs/2409.17348v2
- **Authors**: ['Huao Li', 'Hossein Nourkhiz Mahjoub', 'Behdad Chalaki', 'Vaishnav Tadiparthi', 'Kwonjoon Lee', 'Ehsan Moradi-Pari', 'Charles Michael Lewis', 'Katia P Sycara']
- **Abstrat**: Multi-Agent Reinforcement Learning (MARL) methods have shown promise in enabling agents to learn a shared communication protocol from scratch and accomplish challenging team tasks. However, the learned language is usually not interpretable to humans or other agents not co-trained together, limiting its applicability in ad-hoc teamwork scenarios. In this work, we propose a novel computational pipeline that aligns the communication space between MARL agents with an embedding space of human natural language by grounding agent communications on synthetic data generated by embodied Large Language Models (LLMs) in interactive teamwork scenarios. Our results demonstrate that introducing language grounding not only maintains task performance but also accelerates the emergence of communication. Furthermore, the learned communication protocols exhibit zero-shot generalization capabilities in ad-hoc teamwork scenarios with unseen teammates and novel task states. This work presents a significant step toward enabling effective communication and collaboration between artificial agents and humans in real-world teamwork settings.





## Tuning Synaptic Connections instead of Weights by Genetic Algorithm in Spiking Policy Network
- **Url**: http://arxiv.org/abs/2301.10292v2
- **Authors**: ['Duzhen Zhang', 'Tielin Zhang', 'Shuncheng Jia', 'Qingyu Wang', 'Bo Xu']
- **Abstrat**: Learning from interaction is the primary way that biological agents acquire knowledge about their environment and themselves. Modern deep reinforcement learning (DRL) explores a computational approach to learning from interaction and has made significant progress in solving various tasks. However, despite its power, DRL still falls short of biological agents in terms of energy efficiency. Although the underlying mechanisms are not fully understood, we believe that the integration of spiking communication between neurons and biologically-plausible synaptic plasticity plays a prominent role in achieving greater energy efficiency. Following this biological intuition, we optimized a spiking policy network (SPN) using a genetic algorithm as an energy-efficient alternative to DRL. Our SPN mimics the sensorimotor neuron pathway of insects and communicates through event-based spikes. Inspired by biological research showing that the brain forms memories by creating new synaptic connections and rewiring these connections based on new experiences, we tuned the synaptic connections instead of weights in the SPN to solve given tasks. Experimental results on several robotic control tasks demonstrate that our method can achieve the same level of performance as mainstream DRL methods while exhibiting significantly higher energy efficiency.





## Can Learned Optimization Make Reinforcement Learning Less Difficult?
- **Url**: http://arxiv.org/abs/2407.07082v2
- **Authors**: ['Alexander David Goldie', 'Chris Lu', 'Matthew Thomas Jackson', 'Shimon Whiteson', 'Jakob Nicolaus Foerster']
- **Abstrat**: While reinforcement learning (RL) holds great potential for decision making in the real world, it suffers from a number of unique difficulties which often need specific consideration. In particular: it is highly non-stationary; suffers from high degrees of plasticity loss; and requires exploration to prevent premature convergence to local optima and maximize return. In this paper, we consider whether learned optimization can help overcome these problems. Our method, Learned Optimization for Plasticity, Exploration and Non-stationarity (OPEN), meta-learns an update rule whose input features and output structure are informed by previously proposed solutions to these difficulties. We show that our parameterization is flexible enough to enable meta-learning in diverse learning contexts, including the ability to use stochasticity for exploration. Our experiments demonstrate that when meta-trained on single and small sets of environments, OPEN outperforms or equals traditionally used optimizers. Furthermore, OPEN shows strong generalization characteristics across a range of environments and agent architectures.





## Unsupervised Event Outlier Detection in Continuous Time
- **Url**: http://arxiv.org/abs/2411.16427v1
- **Authors**: ['Somjit Nath', 'Yik Chau Lui', 'Siqi Liu']
- **Abstrat**: Event sequence data record the occurrences of events in continuous time. Event sequence forecasting based on temporal point processes (TPPs) has been extensively studied, but outlier or anomaly detection, especially without any supervision from humans, is still underexplored. In this work, we develop, to the best our knowledge, the first unsupervised outlier detection approach to detecting abnormal events. Our novel unsupervised outlier detection framework is based on ideas from generative adversarial networks (GANs) and reinforcement learning (RL). We train a 'generator' that corrects outliers in the data with a 'discriminator' that learns to discriminate the corrected data from the real data, which may contain outliers. A key insight is that if the generator made a mistake in the correction, it would generate anomalies that are different from the anomalies in the real data, so it serves as data augmentation for the discriminator learning. Different from typical GAN-based outlier detection approaches, our method employs the generator to detect outliers in an online manner. The experimental results show that our method can detect event outliers more accurately than the state-of-the-art approaches.





## CATP-LLM: Empowering Large Language Models for Cost-Aware Tool Planning
- **Url**: http://arxiv.org/abs/2411.16313v1
- **Authors**: ['Duo Wu', 'Jinghe Wang', 'Yuan Meng', 'Yanning Zhang', 'Le Sun', 'Zhi Wang']
- **Abstrat**: Utilizing large language models (LLMs) for tool planning has emerged as a promising avenue for developing general AI systems, where LLMs automatically schedule external tools (e.g. vision models) to tackle complex tasks based on task descriptions. To push this paradigm toward practical applications, it is crucial for LLMs to consider tool execution costs (e.g. execution time) for tool planning. Unfortunately, prior studies overlook the tool execution costs, leading to the generation of expensive plans of which the costs outweigh task performance. To fill this gap, we propose the Cost-Aware Tool Planning with LLMs (CATP-LLM) framework, which for the first time provides a coherent design to empower LLMs for cost-aware tool planning. Specifically, CATP-LLM incorporates a tool planning language to enhance the LLM to generate non-sequential plans of multiple branches for efficient concurrent tool execution and cost reduction. Moreover, it further designs a cost-aware offline reinforcement learning algorithm to fine-tune the LLM to optimize the performance-cost trade-off in tool planning. In lack of public cost-related datasets, we further present OpenCATP, the first platform for cost-aware planning evaluation. Experiments on OpenCATP show that CATP-LLM outperforms GPT-4 even when using Llama2-7B as its backbone, with the average improvement of 28.2%-30.2% higher plan performance and 24.7%-45.8% lower costs even on the challenging planning tasks. The codes of CATP-LLM and OpenCATP will be publicly available.





## Offline reinforcement learning for job-shop scheduling problems
- **Url**: http://arxiv.org/abs/2410.15714v2
- **Authors**: ['Imanol Echeverria', 'Maialen Murua', 'Roberto Santana']
- **Abstrat**: Recent advances in deep learning have shown significant potential for solving combinatorial optimization problems in real-time. Unlike traditional methods, deep learning can generate high-quality solutions efficiently, which is crucial for applications like routing and scheduling. However, existing approaches like deep reinforcement learning (RL) and behavioral cloning have notable limitations, with deep RL suffering from slow learning and behavioral cloning relying solely on expert actions, which can lead to generalization issues and neglect of the optimization objective. This paper introduces a novel offline RL method designed for combinatorial optimization problems with complex constraints, where the state is represented as a heterogeneous graph and the action space is variable. Our approach encodes actions in edge attributes and balances expected rewards with the imitation of expert solutions. We demonstrate the effectiveness of this method on job-shop scheduling and flexible job-shop scheduling benchmarks, achieving superior performance compared to state-of-the-art techniques.





## Towards an Information Theoretic Framework of Context-Based Offline Meta-Reinforcement Learning
- **Url**: http://arxiv.org/abs/2402.02429v2
- **Authors**: ['Lanqing Li', 'Hai Zhang', 'Xinyu Zhang', 'Shatong Zhu', 'Yang Yu', 'Junqiao Zhao', 'Pheng-Ann Heng']
- **Abstrat**: As a marriage between offline RL and meta-RL, the advent of offline meta-reinforcement learning (OMRL) has shown great promise in enabling RL agents to multi-task and quickly adapt while acquiring knowledge safely. Among which, context-based OMRL (COMRL) as a popular paradigm, aims to learn a universal policy conditioned on effective task representations. In this work, by examining several key milestones in the field of COMRL, we propose to integrate these seemingly independent methodologies into a unified framework. Most importantly, we show that the pre-existing COMRL algorithms are essentially optimizing the same mutual information objective between the task variable $M$ and its latent representation $Z$ by implementing various approximate bounds. Such theoretical insight offers ample design freedom for novel algorithms. As demonstrations, we propose a supervised and a self-supervised implementation of $I(Z; M)$, and empirically show that the corresponding optimization algorithms exhibit remarkable generalization across a broad spectrum of RL benchmarks, context shift scenarios, data qualities and deep learning architectures. This work lays the information theoretic foundation for COMRL methods, leading to a better understanding of task representation learning in the context of reinforcement learning.





## Probing for Consciousness in Machines
- **Url**: http://arxiv.org/abs/2411.16262v1
- **Authors**: ['Mathis Immertreu', 'Achim Schilling', 'Andreas Maier', 'Patrick Krauss']
- **Abstrat**: This study explores the potential for artificial agents to develop core consciousness, as proposed by Antonio Damasio's theory of consciousness. According to Damasio, the emergence of core consciousness relies on the integration of a self model, informed by representations of emotions and feelings, and a world model. We hypothesize that an artificial agent, trained via reinforcement learning (RL) in a virtual environment, can develop preliminary forms of these models as a byproduct of its primary task. The agent's main objective is to learn to play a video game and explore the environment. To evaluate the emergence of world and self models, we employ probes-feedforward classifiers that use the activations of the trained agent's neural networks to predict the spatial positions of the agent itself. Our results demonstrate that the agent can form rudimentary world and self models, suggesting a pathway toward developing machine consciousness. This research provides foundational insights into the capabilities of artificial agents in mirroring aspects of human consciousness, with implications for future advancements in artificial intelligence.





## Real-world validation of safe reinforcement learning, model predictive control and decision tree-based home energy management systems
- **Url**: http://arxiv.org/abs/2408.07435v2
- **Authors**: ['Julian Ruddick', 'Glenn Ceusters', 'Gilles Van Kriekinge', 'Evgenii Genov', 'Cedric De Cauwer', 'Thierry Coosemans', 'Maarten Messagie']
- **Abstrat**: Recent advancements in machine learning based energy management approaches, specifically reinforcement learning with a safety layer (OptLayerPolicy) and a metaheuristic algorithm generating a decision tree control policy (TreeC), have shown promise. However, their effectiveness has only been demonstrated in computer simulations. This paper presents the real-world validation of these methods, comparing against model predictive control and simple rule-based control benchmark. The experiments were conducted on the electrical installation of 4 reproductions of residential houses, which all have their own battery, photovoltaic and dynamic load system emulating a non-controllable electrical load and a controllable electric vehicle charger. The results show that the simple rules, TreeC, and model predictive control-based methods achieved similar costs, with a difference of only 0.6%. The reinforcement learning based method, still in its training phase, obtained a cost 25.5\% higher to the other methods. Additional simulations show that the costs can be further reduced by using a more representative training dataset for TreeC and addressing errors in the model predictive control implementation caused by its reliance on accurate data from various sources. The OptLayerPolicy safety layer allows safe online training of a reinforcement learning agent in the real-world, given an accurate constraint function formulation. The proposed safety layer method remains error-prone, nonetheless, it is found beneficial for all investigated methods. The TreeC method, which does require building a realistic simulation for training, exhibits the safest operational performance, exceeding the grid limit by only 27.1 Wh compared to 593.9 Wh for reinforcement learning.





## StepTool: A Step-grained Reinforcement Learning Framework for Tool Learning in LLMs
- **Url**: http://arxiv.org/abs/2410.07745v2
- **Authors**: ['Yuanqing Yu', 'Zhefan Wang', 'Weizhi Ma', 'Zhicheng Guo', 'Jingtao Zhan', 'Shuai Wang', 'Chuhan Wu', 'Zhiqiang Guo', 'Min Zhang']
- **Abstrat**: Despite having powerful reasoning and inference capabilities, Large Language Models (LLMs) still need external tools to acquire real-time information retrieval or domain-specific expertise to solve complex tasks, which is referred to as tool learning. Existing tool learning methods primarily rely on tuning with expert trajectories, focusing on token-sequence learning from a linguistic perspective. However, there are several challenges: 1) imitating static trajectories limits their ability to generalize to new tasks. 2) even expert trajectories can be suboptimal, and better solution paths may exist. In this work, we introduce StepTool, a novel step-grained reinforcement learning framework to improve tool learning in LLMs. It consists of two components: Step-grained Reward Shaping, which assigns rewards at each tool interaction based on tool invocation success and its contribution to the task, and Step-grained Optimization, which uses policy gradient methods to optimize the model in a multi-step manner. Experimental results demonstrate that StepTool significantly outperforms existing methods in multi-step, tool-based tasks, providing a robust solution for complex task environments. Codes are available at https://github.com/yuyq18/StepTool.





## Fostering Intrinsic Motivation in Reinforcement Learning with Pretrained Foundation Models
- **Url**: http://arxiv.org/abs/2410.07404v2
- **Authors**: ['Alain Andres', 'Javier Del Ser']
- **Abstrat**: Exploration remains a significant challenge in reinforcement learning, especially in environments where extrinsic rewards are sparse or non-existent. The recent rise of foundation models, such as CLIP, offers an opportunity to leverage pretrained, semantically rich embeddings that encapsulate broad and reusable knowledge. In this work we explore the potential of these foundation models not just to drive exploration, but also to analyze the critical role of the episodic novelty term in enhancing exploration effectiveness of the agent. We also investigate whether providing the intrinsic module with complete state information -- rather than just partial observations -- can improve exploration, despite the difficulties in handling small variations within large state spaces. Our experiments in the MiniGrid domain reveal that intrinsic modules can effectively utilize full state information, significantly increasing sample efficiency while learning an optimal policy. Moreover, we show that the embeddings provided by foundation models are sometimes even better than those constructed by the agent during training, further accelerating the learning process, especially when coupled with the episodic novelty term to enhance exploration.





# TD3
# Prioritized Experience Replay
# path planning
## Shortest Path Lengths in Poisson Line Cox Processes: Approximations and Applications
- **Url**: http://arxiv.org/abs/2411.16441v1
- **Authors**: ['Gourab Ghatak', 'Sanjoy Kumar Jhawar', 'Martin Haenggi']
- **Abstrat**: We derive exact expressions for the shortest path length to a point of a Poisson line Cox process (PLCP) from the typical point of the PLCP and from the typical intersection of the underlying Poisson line process (PLP), restricted to a single turn. For the two turns case, we derive a bound on the shortest path length from the typical point and demonstrate conditions under which the bound is tight. We also highlight the line process and point process densities for which the shortest path from the typical intersection under the one turn restriction may be shorter than the shortest path from the typical point under the two turns restriction. Finally, we discuss two applications where our results can be employed for a statistical characterization of system performance: in a re-configurable intelligent surface (RIS) enabled vehicle-to-vehicle (V2V) communication system and in electric vehicle charging point deployment planning in urban streets.





## Open-Vocabulary Octree-Graph for 3D Scene Understanding
- **Url**: http://arxiv.org/abs/2411.16253v1
- **Authors**: ['Zhigang Wang', 'Yifei Su', 'Chenhui Li', 'Dong Wang', 'Yan Huang', 'Bin Zhao', 'Xuelong Li']
- **Abstrat**: Open-vocabulary 3D scene understanding is indispensable for embodied agents. Recent works leverage pretrained vision-language models (VLMs) for object segmentation and project them to point clouds to build 3D maps. Despite progress, a point cloud is a set of unordered coordinates that requires substantial storage space and does not directly convey occupancy information or spatial relation, making existing methods inefficient for downstream tasks, e.g., path planning and complex text-based object retrieval. To address these issues, we propose Octree-Graph, a novel scene representation for open-vocabulary 3D scene understanding. Specifically, a Chronological Group-wise Segment Merging (CGSM) strategy and an Instance Feature Aggregation (IFA) algorithm are first designed to get 3D instances and corresponding semantic features. Subsequently, an adaptive-octree structure is developed that stores semantics and depicts the occupancy of an object adjustably according to its shape. Finally, the Octree-Graph is constructed where each adaptive-octree acts as a graph node, and edges describe the spatial relations among nodes. Extensive experiments on various tasks are conducted on several widely-used datasets, demonstrating the versatility and effectiveness of our method.




