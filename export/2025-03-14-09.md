# reinforcement learning
## NIL: No-data Imitation Learning by Leveraging Pre-trained Video Diffusion Models
- **Url**: http://arxiv.org/abs/2503.10626v1
- **Authors**: ['Mert Albaba', 'Chenhao Li', 'Markos Diomataris', 'Omid Taheri', 'Andreas Krause', 'Michael Black']
- **Abstrat**: Acquiring physically plausible motor skills across diverse and unconventional morphologies-including humanoid robots, quadrupeds, and animals-is essential for advancing character simulation and robotics. Traditional methods, such as reinforcement learning (RL) are task- and body-specific, require extensive reward function engineering, and do not generalize well. Imitation learning offers an alternative but relies heavily on high-quality expert demonstrations, which are difficult to obtain for non-human morphologies. Video diffusion models, on the other hand, are capable of generating realistic videos of various morphologies, from humans to ants. Leveraging this capability, we propose a data-independent approach for skill acquisition that learns 3D motor skills from 2D-generated videos, with generalization capability to unconventional and non-human forms. Specifically, we guide the imitation learning process by leveraging vision transformers for video-based comparisons by calculating pair-wise distance between video embeddings. Along with video-encoding distance, we also use a computed similarity between segmented video frames as a guidance reward. We validate our method on locomotion tasks involving unique body configurations. In humanoid robot locomotion tasks, we demonstrate that 'No-data Imitation Learning' (NIL) outperforms baselines trained on 3D motion-capture data. Our results highlight the potential of leveraging generative video models for physically plausible skill learning with diverse morphologies, effectively replacing data collection with data generation for imitation learning.





## R1-Onevision: Advancing Generalized Multimodal Reasoning through Cross-Modal Formalization
- **Url**: http://arxiv.org/abs/2503.10615v1
- **Authors**: ['Yi Yang', 'Xiaoxuan He', 'Hongkun Pan', 'Xiyan Jiang', 'Yan Deng', 'Xingtao Yang', 'Haoyu Lu', 'Dacheng Yin', 'Fengyun Rao', 'Minfeng Zhu', 'Bo Zhang', 'Wei Chen']
- **Abstrat**: Large Language Models have demonstrated remarkable reasoning capability in complex textual tasks. However, multimodal reasoning, which requires integrating visual and textual information, remains a significant challenge. Existing visual-language models often struggle to effectively analyze and reason visual content, resulting in suboptimal performance on complex reasoning tasks. Moreover, the absence of comprehensive benchmarks hinders the accurate assessment of multimodal reasoning capabilities. In this paper, we introduce R1-Onevision, a multimodal reasoning model designed to bridge the gap between visual perception and deep reasoning. To achieve this, we propose a cross-modal reasoning pipeline that transforms images into formal textural representations, enabling precise language-based reasoning. Leveraging this pipeline, we construct the R1-Onevision dataset which provides detailed, step-by-step multimodal reasoning annotations across diverse domains. We further develop the R1-Onevision model through supervised fine-tuning and reinforcement learning to cultivate advanced reasoning and robust generalization abilities. To comprehensively evaluate multimodal reasoning performance across different grades, we introduce R1-Onevision-Bench, a benchmark aligned with human educational stages, covering exams from junior high school to university and beyond. Experimental results show that R1-Onevision achieves state-of-the-art performance, outperforming models such as GPT-4o and Qwen2.5-VL on multiple challenging multimodal reasoning benchmarks.





## Correlated Proxies: A New Definition and Improved Mitigation for Reward Hacking
- **Url**: http://arxiv.org/abs/2403.03185v4
- **Authors**: ['Cassidy Laidlaw', 'Shivam Singhal', 'Anca Dragan']
- **Abstrat**: Because it is difficult to precisely specify complex objectives, reinforcement learning policies are often optimized using proxy reward functions that only approximate the true goal. However, optimizing proxy rewards frequently leads to reward hacking: the optimized reward function ceases to be a good proxy and the resulting policy performs poorly with respect to the unspecified true reward. Principled solutions to reward hacking have been impeded by the lack of a good definition for the problem. To address this gap, we introduce a definition of reward hacking based on the correlation between proxy and true rewards for states and actions seen by a "reference policy" that breaks down under optimization. We show that this definition captures reward hacking behavior across several realistic settings, including in reinforcement learning from human feedback (RLHF). Using our formulation, we show theoretically that regularization to the reference policy can effectively prevent reward hacking. While the current practice in RLHF applies a KL penalty between action distributions for this purpose, our theory suggests regularizing the $\chi^2$ divergence between the policies' occupancy measures can be more effective. We intuitively show the benefits of this type of regularization and demonstrate that it better mitigates reward hacking in practice across four realistic settings, including RLHF. Our code is available at https://github.com/cassidylaidlaw/orpo.





## The Lagrangian Method for Solving Constrained Markov Games
- **Url**: http://arxiv.org/abs/2503.10561v1
- **Authors**: ['Soham Das', 'Santiago Paternain', 'Luiz F. O. Chamon', 'Ceyhun Eksin']
- **Abstrat**: We propose the concept of a Lagrangian game to solve constrained Markov games. Such games model scenarios where agents face cost constraints in addition to their individual rewards, that depend on both agent joint actions and the evolving environment state over time. Constrained Markov games form the formal mechanism behind safe multiagent reinforcement learning, providing a structured model for dynamic multiagent interactions in a multitude of settings, such as autonomous teams operating under local energy and time constraints, for example. We develop a primal-dual approach in which agents solve a Lagrangian game associated with the current Lagrange multiplier, simulate cost and reward trajectories over a fixed horizon, and update the multiplier using accrued experience. This update rule generates a new Lagrangian game, initiating the next iteration. Our key result consists in showing that the sequence of solutions to these Lagrangian games yields a nonstationary Nash solution for the original constrained Markov game.





## Towards Safe Path Tracking Using the Simplex Architecture
- **Url**: http://arxiv.org/abs/2503.10559v1
- **Authors**: ['Georg Jäger', 'Nils-Jonathan Friedrich', 'Hauke Petersen', 'Benjamin Noack']
- **Abstrat**: Robot navigation in complex environments necessitates controllers that are adaptive and safe. Traditional controllers like Regulated Pure Pursuit, Dynamic Window Approach, and Model-Predictive Path Integral, while reliable, struggle to adapt to dynamic conditions. Reinforcement Learning offers adaptability but lacks formal safety guarantees. To address this, we propose a path tracking controller leveraging the Simplex architecture. It combines a Reinforcement Learning controller for adaptiveness and performance with a high-assurance controller providing safety and stability. Our contribution is twofold. We firstly discuss general stability and safety considerations for designing controllers using the Simplex architecture. Secondly, we present a Simplex-based path tracking controller. Our simulation results, supported by preliminary in-field tests, demonstrate the controller's effectiveness in maintaining safety while achieving comparable performance to state-of-the-art methods.





## What is the Alignment Objective of GRPO?
- **Url**: http://arxiv.org/abs/2502.18548v3
- **Authors**: ['Milan Vojnovic', 'Se-Young Yun']
- **Abstrat**: In this note, we examine the aggregation of preferences achieved by the Group Policy Optimisation (GRPO) algorithm, a reinforcement learning method used to train advanced artificial intelligence models such as DeepSeek-R1-Zero and DeepSeekMath. The GRPO algorithm trains a policy using a reward preference model, which is computed by sampling a set of outputs for a given context, observing the corresponding rewards, and applying shift-and-scale normalisation to these reward values. Additionally, it incorporates a penalty function to discourage deviations from a reference policy.   We present a framework that enables us to characterise the stationary policies of the GRPO algorithm. This analysis reveals that the aggregation of preferences differs fundamentally from standard logarithmic pooling, which is implemented by other approaches such as RLHF. The precise form of preference aggregation arises from the way the reward preference model is defined and from the penalty function, which we show to essentially correspond to the reverse Kullback-Leibler (KL) divergence between the aggregation policy and the reference policy.   Interestingly, we demonstrate that for groups of size two, the reward preference model corresponds to pairwise comparison preferences, similar to those in other alignment methods based on pairwise comparison feedback. We provide explicit characterisations of the aggregate preference for binary questions, for groups of size two, and in the limit of large group size. This provides insights into the dependence of the aggregate preference on parameters such as the regularisation constant and the confidence margin of question answers.   Finally, we discuss the aggregation of preferences obtained by modifying the GRPO algorithm to use direct KL divergence as the penalty or to use rewards without scale normalisation.





## SySLLM: Generating Synthesized Policy Summaries for Reinforcement Learning Agents Using Large Language Models
- **Url**: http://arxiv.org/abs/2503.10509v1
- **Authors**: ['Sahar Admoni', 'Omer Ben-Porat', 'Ofra Amir']
- **Abstrat**: Policies generated by Reinforcement Learning (RL) algorithms can be difficult to describe to users, as they result from the interplay between complex reward structures and neural network-based representations. This combination often leads to unpredictable behaviors, making policies challenging to analyze and posing significant obstacles to fostering human trust in real-world applications. Global policy summarization methods aim to describe agent behavior through a demonstration of actions in a subset of world-states. However, users can only watch a limited number of demonstrations, restricting their understanding of policies. Moreover, those methods overly rely on user interpretation, as they do not synthesize observations into coherent patterns. In this work, we present SySLLM (Synthesized Summary using LLMs), a novel method that employs synthesis summarization, utilizing large language models' (LLMs) extensive world knowledge and ability to capture patterns, to generate textual summaries of policies. Specifically, an expert evaluation demonstrates that the proposed approach generates summaries that capture the main insights generated by experts while not resulting in significant hallucinations. Additionally, a user study shows that SySLLM summaries are preferred over demonstration-based policy summaries and match or surpass their performance in objective agent identification tasks.





## Confidence-Controlled Exploration: Efficient Sparse-Reward Policy Learning for Robot Navigation
- **Url**: http://arxiv.org/abs/2306.06192v9
- **Authors**: ['Bhrij Patel', 'Kasun Weerakoon', 'Wesley A. Suttle', 'Alec Koppel', 'Brian M. Sadler', 'Tianyi Zhou', 'Amrit Singh Bedi', 'Dinesh Manocha']
- **Abstrat**: Reinforcement learning (RL) is a promising approach for robotic navigation, allowing robots to learn through trial and error. However, real-world robotic tasks often suffer from sparse rewards, leading to inefficient exploration and suboptimal policies due to sample inefficiency of RL. In this work, we introduce Confidence-Controlled Exploration (CCE), a novel method that improves sample efficiency in RL-based robotic navigation without modifying the reward function. Unlike existing approaches, such as entropy regularization and reward shaping, which can introduce instability by altering rewards, CCE dynamically adjusts trajectory length based on policy entropy. Specifically, it shortens trajectories when uncertainty is high to enhance exploration and extends them when confidence is high to prioritize exploitation. CCE is a principled and practical solution inspired by a theoretical connection between policy entropy and gradient estimation. It integrates seamlessly with on-policy and off-policy RL methods and requires minimal modifications. We validate CCE across REINFORCE, PPO, and SAC in both simulated and real-world navigation tasks. CCE outperforms fixed-trajectory and entropy-regularized baselines, achieving an 18\% higher success rate, 20-38\% shorter paths, and 9.32\% lower elevation costs under a fixed training sample budget. Finally, we deploy CCE on a Clearpath Husky robot, demonstrating its effectiveness in complex outdoor environments.





## Learning Robotic Policy with Imagined Transition: Mitigating the Trade-off between Robustness and Optimality
- **Url**: http://arxiv.org/abs/2503.10484v1
- **Authors**: ['Wei Xiao', 'Shangke Lyu', 'Zhefei Gong', 'Renjie Wang', 'Donglin Wang']
- **Abstrat**: Existing quadrupedal locomotion learning paradigms usually rely on extensive domain randomization to alleviate the sim2real gap and enhance robustness. It trains policies with a wide range of environment parameters and sensor noises to perform reliably under uncertainty. However, since optimal performance under ideal conditions often conflicts with the need to handle worst-case scenarios, there is a trade-off between optimality and robustness. This trade-off forces the learned policy to prioritize stability in diverse and challenging conditions over efficiency and accuracy in ideal ones, leading to overly conservative behaviors that sacrifice peak performance. In this paper, we propose a two-stage framework that mitigates this trade-off by integrating policy learning with imagined transitions. This framework enhances the conventional reinforcement learning (RL) approach by incorporating imagined transitions as demonstrative inputs. These imagined transitions are derived from an optimal policy and a dynamics model operating within an idealized setting. Our findings indicate that this approach significantly mitigates the domain randomization-induced negative impact of existing RL algorithms. It leads to accelerated training, reduced tracking errors within the distribution, and enhanced robustness outside the distribution.





## SortingEnv: An Extendable RL-Environment for an Industrial Sorting Process
- **Url**: http://arxiv.org/abs/2503.10466v1
- **Authors**: ['Tom Maus', 'Nico Zengeler', 'Tobias Glasmachers']
- **Abstrat**: We present a novel reinforcement learning (RL) environment designed to both optimize industrial sorting systems and study agent behavior in evolving spaces. In simulating material flow within a sorting process our environment follows the idea of a digital twin, with operational parameters like belt speed and occupancy level. To reflect real-world challenges, we integrate common upgrades to industrial setups, like new sensors or advanced machinery. It thus includes two variants: a basic version focusing on discrete belt speed adjustments and an advanced version introducing multiple sorting modes and enhanced material composition observations. We detail the observation spaces, state update mechanisms, and reward functions for both environments. We further evaluate the efficiency of common RL algorithms like Proximal Policy Optimization (PPO), Deep-Q-Networks (DQN), and Advantage Actor Critic (A2C) in comparison to a classical rule-based agent (RBA). This framework not only aids in optimizing industrial processes but also provides a foundation for studying agent behavior and transferability in evolving environments, offering insights into model performance and practical implications for real-world RL applications.





## Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and Beyond
- **Url**: http://arxiv.org/abs/2503.10460v1
- **Authors**: ['Liang Wen', 'Yunke Cai', 'Fenrui Xiao', 'Xin He', 'Qi An', 'Zhenyu Duan', 'Yimin Du', 'Junchen Liu', 'Lifu Tang', 'Xiaowei Lv', 'Haosheng Zou', 'Yongchao Deng', 'Shousheng Jia', 'Xiangzheng Zhang']
- **Abstrat**: This paper presents our work on the Light-R1 series, with models, data, and code all released.   We first focus on training long COT models from scratch, specifically starting from models initially lacking long COT capabilities. Using a curriculum training recipe consisting of two-stage SFT and semi-on-policy DPO, we train our model Light-R1-32B from Qwen2.5-32B-Instruct, resulting in superior math performance compared to DeepSeek-R1-Distill-Qwen-32B. Despite being trained exclusively on math data, Light-R1-32B shows strong generalization across other domains. In the subsequent phase of this work, we highlight the significant benefit of the 3k dataset constructed for the second SFT stage on enhancing other models. By fine-tuning DeepSeek-R1-Distilled models using this dataset, we obtain new SOTA models in 7B and 14B, while the 32B model, Light-R1-32B-DS performed comparably to QwQ-32B and DeepSeek-R1.   Furthermore, we extend our work by applying reinforcement learning, specifically GRPO, on long-COT models to further improve reasoning performance. We successfully train our final Light-R1-14B-DS with RL, achieving SOTA performance among 14B parameter models in math. With AIME24 & 25 scores of 74.0 and 60.2 respectively, Light-R1-14B-DS surpasses even many 32B models and DeepSeek-R1-Distill-Llama-70B. Its RL training also exhibits well expected behavior, showing simultaneous increase in response length and reward score.   The Light-R1 series of work validates training long-COT models from scratch, showcases the art in SFT data and releases SOTA models from RL.





## Finetuning Generative Trajectory Model with Reinforcement Learning from Human Feedback
- **Url**: http://arxiv.org/abs/2503.10434v1
- **Authors**: ['Derun Li', 'Jianwei Ren', 'Yue Wang', 'Xin Wen', 'Pengxiang Li', 'Leimeng Xu', 'Kun Zhan', 'Zhongpu Xia', 'Peng Jia', 'Xianpeng Lang', 'Ningyi Xu', 'Hang Zhao']
- **Abstrat**: Generating human-like and adaptive trajectories is essential for autonomous driving in dynamic environments. While generative models have shown promise in synthesizing feasible trajectories, they often fail to capture the nuanced variability of human driving styles due to dataset biases and distributional shifts. To address this, we introduce TrajHF, a human feedback-driven finetuning framework for generative trajectory models, designed to align motion planning with diverse driving preferences. TrajHF incorporates multi-conditional denoiser and reinforcement learning with human feedback to refine multi-modal trajectory generation beyond conventional imitation learning. This enables better alignment with human driving preferences while maintaining safety and feasibility constraints. TrajHF achieves PDMS of 93.95 on NavSim benchmark, significantly exceeding other methods. TrajHF sets a new paradigm for personalized and adaptable trajectory generation in autonomous driving.





## Towards Constraint-Based Adaptive Hypergraph Learning for Solving Vehicle Routing: An End-to-End Solution
- **Url**: http://arxiv.org/abs/2503.10421v1
- **Authors**: ['Zhenwei Wang', 'Ruibin Bai', 'Tiehua Zhang']
- **Abstrat**: The application of learning based methods to vehicle routing problems has emerged as a pivotal area of research in combinatorial optimization. These problems are characterized by vast solution spaces and intricate constraints, making traditional approaches such as exact mathematical models or heuristic methods prone to high computational overhead or reliant on the design of complex heuristic operators to achieve optimal or near optimal solutions. Meanwhile, although some recent learning-based methods can produce good performance for VRP with straightforward constraint scenarios, they often fail to effectively handle hard constraints that are common in practice. This study introduces a novel end-to-end framework that combines constraint-oriented hypergraphs with reinforcement learning to address vehicle routing problems. A central innovation of this work is the development of a constraint-oriented dynamic hyperedge reconstruction strategy within an encoder, which significantly enhances hypergraph representation learning. Additionally, the decoder leverages a double-pointer attention mechanism to iteratively generate solutions. The proposed model is trained by incorporating asynchronous parameter updates informed by hypergraph constraints and optimizing a dual loss function comprising constraint loss and policy gradient loss. The experiment results on benchmark datasets demonstrate that the proposed approach not only eliminates the need for sophisticated heuristic operators but also achieves substantial improvements in solution quality.





## A nonlinear real time capable motion cueing algorithm based on deep reinforcement learning
- **Url**: http://arxiv.org/abs/2503.10419v1
- **Authors**: ['Hendrik Scheidel', 'Camilo Gonzalez', 'Houshyar Asadi', 'Tobias Bellmann', 'Andreas Seefried', 'Shady Mohamed', 'Saeid Nahavandi']
- **Abstrat**: In motion simulation, motion cueing algorithms are used for the trajectory planning of the motion simulator platform, where workspace limitations prevent direct reproduction of reference trajectories. Strategies such as motion washout, which return the platform to its center, are crucial in these settings. For serial robotic MSPs with highly nonlinear workspaces, it is essential to maximize the efficient utilization of the MSPs kinematic and dynamic capabilities. Traditional approaches, including classical washout filtering and linear model predictive control, fail to consider platform-specific, nonlinear properties, while nonlinear model predictive control, though comprehensive, imposes high computational demands that hinder real-time, pilot-in-the-loop application without further simplification. To overcome these limitations, we introduce a novel approach using deep reinforcement learning for motion cueing, demonstrated here for the first time in a 6-degree-of-freedom setting with full consideration of the MSPs kinematic nonlinearities. Previous work by the authors successfully demonstrated the application of DRL to a simplified 2-DOF setup, which did not consider kinematic or dynamic constraints. This approach has been extended to all 6 DOF by incorporating a complete kinematic model of the MSP into the algorithm, a crucial step for enabling its application on a real motion simulator. The training of the DRL-MCA is based on Proximal Policy Optimization in an actor-critic implementation combined with an automated hyperparameter optimization. After detailing the necessary training framework and the algorithm itself, we provide a comprehensive validation, demonstrating that the DRL MCA achieves competitive performance against established algorithms. Moreover, it generates feasible trajectories by respecting all system constraints and meets all real-time requirements with low...





## Training Hybrid Deep Quantum Neural Network for Reinforced Learning Efficiently
- **Url**: http://arxiv.org/abs/2503.09119v2
- **Authors**: ['Jie Luo', 'Xueyin Chen']
- **Abstrat**: Quantum computing offers a new paradigm for computation, exploiting an exponentially growing Hilbert space for data representation and operation. Computing results are obtained from measurement based stochastic sampling over nontrivial distributions produced by the quantum computing process with complex correlations from quantum entanglement. Quantum machine learning (QML) emerged as a promising method, potentially surpassing classical counterparts in efficiency and accuracy. While large scale fault tolerant quantum (LSFTQ) machines are not yet available, recent works on hybrid quantum machine learning models, compatible with noisy intermediate scale quantum (NISQ) computers, have hinted at improved generalizability. Such hybrid deep quantum neural networks (hDQNNs) integrate GPU CPU based deep neural networks (DNNs) with forward parameterized quantum circuits (PQC) that can be straightforwardly executed on quantum processors. However, backpropagating through forward PQCs remains computationally inefficient if it is simulated on classical hardware, and current quantum hardware constraints impede batched backpropagation even with dedicated PQCs for calculating gradients of forward PQCs, limiting scalability for modern machine learning tasks. Here, we present a scalable quantum machine learning architecture that overcomes these challenges with efficient backpropagation, enabling scalable hDQNN training with PQCs on physical quantum computers or classical PQC simulators. Applied to reinforcement learning benchmarks, our method highlights that hDQNNs could exhibit potentially improved generalizability compared to purely classical models. These findings offer a pathway toward near term hybrid quantum classical computing systems for large scale machine learning and underscore the potential of hybrid quantum classical architectures in advancing artificial intelligence.





## Optical stabilization for laser communication satellite systems through proportional-integral-derivative (PID) control and reinforcement learning approach
- **Url**: http://arxiv.org/abs/2503.10395v1
- **Authors**: ['A. Reutov', 'S. Vorobey', 'A. Katanskiy', 'V. Balakirev', 'R. Bakhshaliev', 'K. Barbyshev', 'V. Merzlinkin', 'V. Tekaev']
- **Abstrat**: One of the main issues of the satellite-to-ground optical communication, including free-space satellite quantum key distribution (QKD), is an achievement of the reasonable accuracy of positioning, navigation and optical stabilization. Proportional-integral-derivative (PID) controllers can handle with various control tasks in optical systems. Recent research shows the promising results in the area of composite control systems including classical control via PID controllers and reinforcement learning (RL) approach. In this work we apply RL agent to an experimental stand of the optical stabilization system of QKD terminal. We find via agent control history more precise PID parameters and also provide effective combined RL-PID dynamic control approach for the optical stabilization of satellite-to-ground communication system.





## NotaGen: Advancing Musicality in Symbolic Music Generation with Large Language Model Training Paradigms
- **Url**: http://arxiv.org/abs/2502.18008v4
- **Authors**: ['Yashan Wang', 'Shangda Wu', 'Jianhuai Hu', 'Xingjian Du', 'Yueqi Peng', 'Yongxin Huang', 'Shuai Fan', 'Xiaobing Li', 'Feng Yu', 'Maosong Sun']
- **Abstrat**: We introduce NotaGen, a symbolic music generation model aiming to explore the potential of producing high-quality classical sheet music. Inspired by the success of Large Language Models (LLMs), NotaGen adopts pre-training, fine-tuning, and reinforcement learning paradigms (henceforth referred to as the LLM training paradigms). It is pre-trained on 1.6M pieces of music in ABC notation, and then fine-tuned on approximately 9K high-quality classical compositions conditioned on "period-composer-instrumentation" prompts. For reinforcement learning, we propose the CLaMP-DPO method, which further enhances generation quality and controllability without requiring human annotations or predefined rewards. Our experiments demonstrate the efficacy of CLaMP-DPO in symbolic music generation models with different architectures and encoding schemes. Furthermore, subjective A/B tests show that NotaGen outperforms baseline models against human compositions, greatly advancing musical aesthetics in symbolic music generation.





## Safe exploration in reproducing kernel Hilbert spaces
- **Url**: http://arxiv.org/abs/2503.10352v1
- **Authors**: ['Abdullah Tokmak', 'Kiran G. Krishnan', 'Thomas B. Schön', 'Dominik Baumann']
- **Abstrat**: Popular safe Bayesian optimization (BO) algorithms learn control policies for safety-critical systems in unknown environments. However, most algorithms make a smoothness assumption, which is encoded by a known bounded norm in a reproducing kernel Hilbert space (RKHS). The RKHS is a potentially infinite-dimensional space, and it remains unclear how to reliably obtain the RKHS norm of an unknown function. In this work, we propose a safe BO algorithm capable of estimating the RKHS norm from data. We provide statistical guarantees on the RKHS norm estimation, integrate the estimated RKHS norm into existing confidence intervals and show that we retain theoretical guarantees, and prove safety of the resulting safe BO algorithm. We apply our algorithm to safely optimize reinforcement learning policies on physics simulators and on a real inverted pendulum, demonstrating improved performance, safety, and scalability compared to the state-of-the-art.





## Neuroplastic Expansion in Deep Reinforcement Learning
- **Url**: http://arxiv.org/abs/2410.07994v2
- **Authors**: ['Jiashun Liu', 'Johan Obando-Ceron', 'Aaron Courville', 'Ling Pan']
- **Abstrat**: The loss of plasticity in learning agents, analogous to the solidification of neural pathways in biological brains, significantly impedes learning and adaptation in reinforcement learning due to its non-stationary nature. To address this fundamental challenge, we propose a novel approach, {\it Neuroplastic Expansion} (NE), inspired by cortical expansion in cognitive science. NE maintains learnability and adaptability throughout the entire training process by dynamically growing the network from a smaller initial size to its full dimension. Our method is designed with three key components: (\textit{1}) elastic topology generation based on potential gradients, (\textit{2}) dormant neuron pruning to optimize network expressivity, and (\textit{3}) neuron consolidation via experience review to strike a balance in the plasticity-stability dilemma. Extensive experiments demonstrate that NE effectively mitigates plasticity loss and outperforms state-of-the-art methods across various tasks in MuJoCo and DeepMind Control Suite environments. NE enables more adaptive learning in complex, dynamic environments, which represents a crucial step towards transitioning deep reinforcement learning from static, one-time training paradigms to more flexible, continually adapting models.





# TD3
# Prioritized Experience Replay
# path planning
## CoSTA$\ast$: Cost-Sensitive Toolpath Agent for Multi-turn Image Editing
- **Url**: http://arxiv.org/abs/2503.10613v1
- **Authors**: ['Advait Gupta', 'NandaKiran Velaga', 'Dang Nguyen', 'Tianyi Zhou']
- **Abstrat**: Text-to-image models like stable diffusion and DALLE-3 still struggle with multi-turn image editing. We decompose such a task as an agentic workflow (path) of tool use that addresses a sequence of subtasks by AI tools of varying costs. Conventional search algorithms require expensive exploration to find tool paths. While large language models (LLMs) possess prior knowledge of subtask planning, they may lack accurate estimations of capabilities and costs of tools to determine which to apply in each subtask. Can we combine the strengths of both LLMs and graph search to find cost-efficient tool paths? We propose a three-stage approach "CoSTA*" that leverages LLMs to create a subtask tree, which helps prune a graph of AI tools for the given task, and then conducts A* search on the small subgraph to find a tool path. To better balance the total cost and quality, CoSTA* combines both metrics of each tool on every subtask to guide the A* search. Each subtask's output is then evaluated by a vision-language model (VLM), where a failure will trigger an update of the tool's cost and quality on the subtask. Hence, the A* search can recover from failures quickly to explore other paths. Moreover, CoSTA* can automatically switch between modalities across subtasks for a better cost-quality trade-off. We build a novel benchmark of challenging multi-turn image editing, on which CoSTA* outperforms state-of-the-art image-editing models or agents in terms of both cost and quality, and performs versatile trade-offs upon user preference.





## On-the-Fly Path Planning for the Design of Compositional Gradients in High Dimensions
- **Url**: http://arxiv.org/abs/2501.00012v2
- **Authors**: ['Samuel Price', 'Zhaoxi Cao', 'Ian McCue']
- **Abstrat**: Functional gradients have recently experienced an explosion in activity due to advances in manufacturing, where compositions can now be spatially varied on-the-fly during fabrication. In addition, modern computational thermodynamics has reached sufficient maturity -- with respect to property databases and the availability of commercial software -- that gradients can be designed with specific sets of properties. Despite these successes, there are practical limitations on the calculation speeds of these thermodynamic tools that make it intractable to model every element in an alloy. As a result, most path planning is carried out via surrogate models on simplified systems (e.g., approximating Inconel 718 as Ni$_{59}$Cr$_{23}$Fe$_{18}$ instead of Ni$_{53}$Cr$_{23}$Fe$_{18}$Nb$_{3}$Mo$_{2}$Ti$_{1}$). In this work, we demonstrate that this limitation can be overcome using a combination of on-the-fly sampling and a conjectured corollary of the lever rule for transformations of isothermal paths in arbitrary compositional dimensions. We quantitatively benchmark the effectiveness of this new method and find that it can be as much as 106 times more efficient than surrogate modeling.





## World Modeling Makes a Better Planner: Dual Preference Optimization for Embodied Task Planning
- **Url**: http://arxiv.org/abs/2503.10480v1
- **Authors**: ['Siyin Wang', 'Zhaoye Fei', 'Qinyuan Cheng', 'Shiduo Zhang', 'Panpan Cai', 'Jinlan Fu', 'Xipeng Qiu']
- **Abstrat**: Recent advances in large vision-language models (LVLMs) have shown promise for embodied task planning, yet they struggle with fundamental challenges like dependency constraints and efficiency. Existing approaches either solely optimize action selection or leverage world models during inference, overlooking the benefits of learning to model the world as a way to enhance planning capabilities. We propose Dual Preference Optimization (D$^2$PO), a new learning framework that jointly optimizes state prediction and action selection through preference learning, enabling LVLMs to understand environment dynamics for better planning. To automatically collect trajectories and stepwise preference data without human annotation, we introduce a tree search mechanism for extensive exploration via trial-and-error. Extensive experiments on VoTa-Bench demonstrate that our D$^2$PO-based method significantly outperforms existing methods and GPT-4o when applied to Qwen2-VL (7B), LLaVA-1.6 (7B), and LLaMA-3.2 (11B), achieving superior task success rates with more efficient execution paths.





## Stratified Topological Autonomy for Long-Range Coordination (STALC)
- **Url**: http://arxiv.org/abs/2503.10475v1
- **Authors**: ['Cora A. Dimmig', 'Adam Goertz', 'Adam Polevoy', 'Mark Gonzales', 'Kevin C. Wolfe', 'Bradley Woosley', 'John Rogers', 'Joseph Moore']
- **Abstrat**: Achieving unified multi-robot coordination and motion planning in complex environments is a challenging problem. In this paper, we present a hierarchical approach to long-range coordination, which we call Stratified Topological Autonomy for Long-Range Coordination (STALC). In particular, we look at the problem of minimizing visibility to observers and maximizing safety with a multi-robot team navigating through a hazardous environment. At its core, our approach relies on the notion of a dynamic topological graph, where the edge weights vary dynamically based on the locations of the robots in the graph. To create this dynamic topological graph, we evaluate the visibility of the robot team from a discrete set of observer locations (both adversarial and friendly), and construct a topological graph whose edge weights depend on both adversary position and robot team configuration. We then impose temporal constraints on the evolution of those edge weights based on robot team state and use Mixed-Integer Programming (MIP) to generate optimal multirobot plans through the graph. The visibility information also informs the lower layers of the autonomy stack to plan minimal visibility paths through the environment for the team of robots. Our approach presents methods to reduce the computational complexity for a team of robots that interact and coordinate across the team to accomplish a common goal. We demonstrate our approach in simulated and hardware experiments in forested and urban environments.




