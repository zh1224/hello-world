# reinforcement learning
## Mobile-TeleVision: Predictive Motion Priors for Humanoid Whole-Body Control
- **Url**: http://arxiv.org/abs/2412.07773v1
- **Authors**: ['Chenhao Lu', 'Xuxin Cheng', 'Jialong Li', 'Shiqi Yang', 'Mazeyu Ji', 'Chengjing Yuan', 'Ge Yang', 'Sha Yi', 'Xiaolong Wang']
- **Abstrat**: Humanoid robots require both robust lower-body locomotion and precise upper-body manipulation. While recent Reinforcement Learning (RL) approaches provide whole-body loco-manipulation policies, they lack precise manipulation with high DoF arms. In this paper, we propose decoupling upper-body control from locomotion, using inverse kinematics (IK) and motion retargeting for precise manipulation, while RL focuses on robust lower-body locomotion. We introduce PMP (Predictive Motion Priors), trained with Conditional Variational Autoencoder (CVAE) to effectively represent upper-body motions. The locomotion policy is trained conditioned on this upper-body motion representation, ensuring that the system remains robust with both manipulation and locomotion. We show that CVAE features are crucial for stability and robustness, and significantly outperforms RL-based whole-body control in precise manipulation. With precise upper-body motion and robust lower-body locomotion control, operators can remotely control the humanoid to walk around and explore different environments, while performing diverse manipulation tasks.





## Efficient Online Reinforcement Learning Fine-Tuning Need Not Retain Offline Data
- **Url**: http://arxiv.org/abs/2412.07762v1
- **Authors**: ['Zhiyuan Zhou', 'Andy Peng', 'Qiyang Li', 'Sergey Levine', 'Aviral Kumar']
- **Abstrat**: The modern paradigm in machine learning involves pre-training on diverse data, followed by task-specific fine-tuning. In reinforcement learning (RL), this translates to learning via offline RL on a diverse historical dataset, followed by rapid online RL fine-tuning using interaction data. Most RL fine-tuning methods require continued training on offline data for stability and performance. However, this is undesirable because training on diverse offline data is slow and expensive for large datasets, and in principle, also limit the performance improvement possible because of constraints or pessimism on offline data. In this paper, we show that retaining offline data is unnecessary as long as we use a properly-designed online RL approach for fine-tuning offline RL initializations. To build this approach, we start by analyzing the role of retaining offline data in online fine-tuning. We find that continued training on offline data is mostly useful for preventing a sudden divergence in the value function at the onset of fine-tuning, caused by a distribution mismatch between the offline data and online rollouts. This divergence typically results in unlearning and forgetting the benefits of offline pre-training. Our approach, Warm-start RL (WSRL), mitigates the catastrophic forgetting of pre-trained initializations using a very simple idea. WSRL employs a warmup phase that seeds the online RL run with a very small number of rollouts from the pre-trained policy to do fast online RL. The data collected during warmup helps ``recalibrate'' the offline Q-function to the online distribution, allowing us to completely discard offline data without destabilizing the online RL fine-tuning. We show that WSRL is able to fine-tune without retaining any offline data, and is able to learn faster and attains higher performance than existing algorithms irrespective of whether they retain offline data or not.





## Optimizing Sensor Redundancy in Sequential Decision-Making Problems
- **Url**: http://arxiv.org/abs/2412.07686v1
- **Authors**: ['Jonas Nüßlein', 'Maximilian Zorn', 'Fabian Ritz', 'Jonas Stein', 'Gerhard Stenzel', 'Julian Schönberger', 'Thomas Gabor', 'Claudia Linnhoff-Popien']
- **Abstrat**: Reinforcement Learning (RL) policies are designed to predict actions based on current observations to maximize cumulative future rewards. In real-world applications (i.e., non-simulated environments), sensors are essential for measuring the current state and providing the observations on which RL policies rely to make decisions. A significant challenge in deploying RL policies in real-world scenarios is handling sensor dropouts, which can result from hardware malfunctions, physical damage, or environmental factors like dust on a camera lens. A common strategy to mitigate this issue is the use of backup sensors, though this comes with added costs. This paper explores the optimization of backup sensor configurations to maximize expected returns while keeping costs below a specified threshold, C. Our approach uses a second-order approximation of expected returns and includes penalties for exceeding cost constraints. We then optimize this quadratic program using Tabu Search, a meta-heuristic algorithm. The approach is evaluated across eight OpenAI Gym environments and a custom Unity-based robotic environment (RobotArmGrasping). Empirical results demonstrate that our quadratic program effectively approximates real expected returns, facilitating the identification of optimal sensor configurations.





## AdaSociety: An Adaptive Environment with Social Structures for Multi-Agent Decision-Making
- **Url**: http://arxiv.org/abs/2411.03865v2
- **Authors**: ['Yizhe Huang', 'Xingbo Wang', 'Hao Liu', 'Fanqi Kong', 'Aoyang Qin', 'Min Tang', 'Xiaoxi Wang', 'Song-Chun Zhu', 'Mingjie Bi', 'Siyuan Qi', 'Xue Feng']
- **Abstrat**: Traditional interactive environments limit agents' intelligence growth with fixed tasks. Recently, single-agent environments address this by generating new tasks based on agent actions, enhancing task diversity. We consider the decision-making problem in multi-agent settings, where tasks are further influenced by social connections, affecting rewards and information access. However, existing multi-agent environments lack a combination of adaptive physical surroundings and social connections, hindering the learning of intelligent behaviors. To address this, we introduce AdaSociety, a customizable multi-agent environment featuring expanding state and action spaces, alongside explicit and alterable social structures. As agents progress, the environment adaptively generates new tasks with social structures for agents to undertake. In AdaSociety, we develop three mini-games showcasing distinct social structures and tasks. Initial results demonstrate that specific social structures can promote both individual and collective benefits, though current reinforcement learning and LLM-based algorithms show limited effectiveness in leveraging social structures to enhance performance. Overall, AdaSociety serves as a valuable research platform for exploring intelligence in diverse physical and social settings. The code is available at https://github.com/bigai-ai/AdaSociety.





## Offline Multi-Agent Reinforcement Learning via In-Sample Sequential Policy Optimization
- **Url**: http://arxiv.org/abs/2412.07639v1
- **Authors**: ['Zongkai Liu', 'Qian Lin', 'Chao Yu', 'Xiawei Wu', 'Yile Liang', 'Donghui Li', 'Xuetao Ding']
- **Abstrat**: Offline Multi-Agent Reinforcement Learning (MARL) is an emerging field that aims to learn optimal multi-agent policies from pre-collected datasets. Compared to single-agent case, multi-agent setting involves a large joint state-action space and coupled behaviors of multiple agents, which bring extra complexity to offline policy optimization. In this work, we revisit the existing offline MARL methods and show that in certain scenarios they can be problematic, leading to uncoordinated behaviors and out-of-distribution (OOD) joint actions. To address these issues, we propose a new offline MARL algorithm, named In-Sample Sequential Policy Optimization (InSPO). InSPO sequentially updates each agent's policy in an in-sample manner, which not only avoids selecting OOD joint actions but also carefully considers teammates' updated policies to enhance coordination. Additionally, by thoroughly exploring low-probability actions in the behavior policy, InSPO can well address the issue of premature convergence to sub-optimal solutions. Theoretically, we prove InSPO guarantees monotonic policy improvement and converges to quantal response equilibrium (QRE). Experimental results demonstrate the effectiveness of our method compared to current state-of-the-art offline MARL methods.





## Swarm Behavior Cloning
- **Url**: http://arxiv.org/abs/2412.07617v1
- **Authors**: ['Jonas Nüßlein', 'Maximilian Zorn', 'Philipp Altmann', 'Claudia Linnhoff-Popien']
- **Abstrat**: In sequential decision-making environments, the primary approaches for training agents are Reinforcement Learning (RL) and Imitation Learning (IL). Unlike RL, which relies on modeling a reward function, IL leverages expert demonstrations, where an expert policy $\pi_e$ (e.g., a human) provides the desired behavior. Formally, a dataset $D$ of state-action pairs is provided: $D = {(s, a = \pi_e(s))}$. A common technique within IL is Behavior Cloning (BC), where a policy $\pi(s) = a$ is learned through supervised learning on $D$. Further improvements can be achieved by using an ensemble of $N$ individually trained BC policies, denoted as $E = {\pi_i(s)}{1 \leq i \leq N}$. The ensemble's action $a$ for a given state $s$ is the aggregated output of the $N$ actions: $a = \frac{1}{N} \sum{i} \pi_i(s)$. This paper addresses the issue of increasing action differences -- the observation that discrepancies between the $N$ predicted actions grow in states that are underrepresented in the training data. Large action differences can result in suboptimal aggregated actions. To address this, we propose a method that fosters greater alignment among the policies while preserving the diversity of their computations. This approach reduces action differences and ensures that the ensemble retains its inherent strengths, such as robustness and varied decision-making. We evaluate our approach across eight diverse environments, demonstrating a notable decrease in action differences and significant improvements in overall performance, as measured by mean episode returns.





## Energy-efficient flow control via optimized synthetic jet placement using deep reinforcement learning
- **Url**: http://arxiv.org/abs/2410.00424v3
- **Authors**: ['Wang Jia', 'Hang Xu']
- **Abstrat**: This study utilizes deep reinforcement learning (DRL) to develop flow control strategies for circular and square cylinders, enhancing energy efficiency and minimizing energy consumption while addressing the limitations of traditional methods.We find that the optimal jet placement for both square and circular cylinders is at the main flow separation point, achieving the best balance between energy efficiency and control effectiveness.For the circular cylinder, positioning the jet at approximately 105{\deg} from the stagnation point requires only 1% of the inlet flow rate and achieves an 8% reduction in drag, with energy consumption one-third of that at other positions. For the square cylinder, placing the jet near the rear corner requires only 2% of the inlet flow rate, achieving a maximum drag reduction of 14.4%, whereas energy consumption near the front corner is 27 times higher, resulting in only 12% drag reduction.In multi-action control, the convergence speed and stability are lower compared to single-action control, but activating multiple jets significantly reduces initial energy consumption and improves energy efficiency. Physically, the interaction of the synthetic jet with the flow generates new vortices that modify the local flow structure, significantly enhancing the cylinder's aerodynamic performance.Our control strategy achieves a superior balance between energy efficiency and control performance compared to previous studies, underscoring its significant potential to advance sustainable and effective flow control.





## ConfigX: Modular Configuration for Evolutionary Algorithms via Multitask Reinforcement Learning
- **Url**: http://arxiv.org/abs/2412.07507v1
- **Authors**: ['Hongshu Guo', 'Zeyuan Ma', 'Jiacheng Chen', 'Yining Ma', 'Zhiguang Cao', 'Xinglin Zhang', 'Yue-Jiao Gong']
- **Abstrat**: Recent advances in Meta-learning for Black-Box Optimization (MetaBBO) have shown the potential of using neural networks to dynamically configure evolutionary algorithms (EAs), enhancing their performance and adaptability across various BBO instances. However, they are often tailored to a specific EA, which limits their generalizability and necessitates retraining or redesigns for different EAs and optimization problems. To address this limitation, we introduce ConfigX, a new paradigm of the MetaBBO framework that is capable of learning a universal configuration agent (model) for boosting diverse EAs. To achieve so, our ConfigX first leverages a novel modularization system that enables the flexible combination of various optimization sub-modules to generate diverse EAs during training. Additionally, we propose a Transformer-based neural network to meta-learn a universal configuration policy through multitask reinforcement learning across a designed joint optimization task space. Extensive experiments verify that, our ConfigX, after large-scale pre-training, achieves robust zero-shot generalization to unseen tasks and outperforms state-of-the-art baselines. Moreover, ConfigX exhibits strong lifelong learning capabilities, allowing efficient adaptation to new tasks through fine-tuning. Our proposed ConfigX represents a significant step toward an automatic, all-purpose configuration agent for EAs.





## Optimizing pulsed blowing parameters for active separation control in a one-sided diffuser using reinforcement learning
- **Url**: http://arxiv.org/abs/2412.07480v1
- **Authors**: ['Alexandra Müller', 'Tobias Schesny', 'Ben Steinfurth', 'Julien Weiss']
- **Abstrat**: Reinforcement learning is employed to optimize the periodic forcing signal of a pulsed blowing system that controls flow separation in a fully-turbulent $Re_\theta = 1000$ diffuser flow. Based on the state of the wind tunnel experiment that is determined with wall shear-stress measurements, Proximal Policy Optimization is used to iteratively adjust the forcing signal. Out of the reward functions investigated in this study, the incremental reduction of flow reversal per action is shown to be the most sample efficient. Less than 100 episodes are required to find the parameter combination that ensures the highest control authority for a fixed mass flow consumption. Fully consistent with recent studies, the algorithm suggests that the mass flow is used most efficiently when the actuation signal is characterized by a low duty cycle where the pulse duration is small compared to the pulsation period. The results presented in this paper promote the application of reinforcement learning for optimization tasks based on turbulent, experimental data.





## Progressive-Resolution Policy Distillation: Leveraging Coarse-Resolution Simulation for Time-Efficient Fine-Resolution Policy Learning
- **Url**: http://arxiv.org/abs/2412.07477v1
- **Authors**: ['Yuki Kadokawa', 'Hirotaka Tahara', 'Takamitsu Matsubara']
- **Abstrat**: In earthwork and construction, excavators often encounter large rocks mixed with various soil conditions, requiring skilled operators. This paper presents a framework for achieving autonomous excavation using reinforcement learning (RL) through a rock excavation simulator. In the simulation, resolution can be defined by the particle size/number in the whole soil space. Fine-resolution simulations closely mimic real-world behavior but demand significant calculation time and challenging sample collection, while coarse-resolution simulations enable faster sample collection but deviate from real-world behavior. To combine the advantages of both resolutions, we explore using policies developed in coarse-resolution simulations for pre-training in fine-resolution simulations. To this end, we propose a novel policy learning framework called Progressive-Resolution Policy Distillation (PRPD), which progressively transfers policies through some middle-resolution simulations with conservative policy transfer to avoid domain gaps that could lead to policy transfer failure. Validation in a rock excavation simulator and nine real-world rock environments demonstrated that PRPD reduced sampling time to less than 1/7 while maintaining task success rates comparable to those achieved through policy learning in a fine-resolution simulation.





## RLT4Rec: Reinforcement Learning Transformer for User Cold Start and Item Recommendation
- **Url**: http://arxiv.org/abs/2412.07403v1
- **Authors**: ['Dilina Chandika Rajapakse', 'Douglas Leith']
- **Abstrat**: We introduce a new sequential transformer reinforcement learning architecture RLT4Rec and demonstrate that it achieves excellent performance in a range of item recommendation tasks. RLT4Rec uses a relatively simple transformer architecture that takes as input the user's (item,rating) history and outputs the next item to present to the user. Unlike existing RL approaches, there is no need to input a state observation or estimate. RLT4Rec handles new users and established users within the same consistent framework and automatically balances the "exploration" needed to discover the preferences of a new user with the "exploitation" that is more appropriate for established users. Training of RLT4Rec is robust and fast and is insensitive to the choice of training data, learning to generate "good" personalised sequences that the user tends to rate highly even when trained on "bad" data.





## Non-Progressive Influence Maximization in Dynamic Social Networks
- **Url**: http://arxiv.org/abs/2412.07402v1
- **Authors**: ['Yunming Hui', 'Shihan Wang', 'Melisachew Wudage Chekol', 'Stevan Rudinac', 'Inez Maria Zwetsloot']
- **Abstrat**: The influence maximization (IM) problem involves identifying a set of key individuals in a social network who can maximize the spread of influence through their network connections. With the advent of geometric deep learning on graphs, great progress has been made towards better solutions for the IM problem. In this paper, we focus on the dynamic non-progressive IM problem, which considers the dynamic nature of real-world social networks and the special case where the influence diffusion is non-progressive, i.e., nodes can be activated multiple times. We first extend an existing diffusion model to capture the non-progressive influence propagation in dynamic social networks. We then propose the method, DNIMRL, which employs deep reinforcement learning and dynamic graph embedding to solve the dynamic non-progressive IM problem. In particular, we propose a novel algorithm that effectively leverages graph embedding to capture the temporal changes of dynamic networks and seamlessly integrates with deep reinforcement learning. The experiments, on different types of real-world social network datasets, demonstrate that our method outperforms state-of-the-art baselines.





## Precise large-scale chemical transformations on surfaces: deep learning meets scanning probe microscopy with interpretability
- **Url**: http://arxiv.org/abs/2409.20014v2
- **Authors**: ['Nian Wu', 'Markus Aapro', 'Joakim S. Jestilä', 'Robert Drost', 'Miguel Martınez Garcıa', 'Tomas Torres', 'Feifei Xiang', 'Nan Cao', 'Zhijie He', 'Giovanni Bottari', 'Peter Liljeroth', 'Adam S. Foster']
- **Abstrat**: Scanning Probe Microscopy (SPM) techniques have shown great potential in fabricating nanoscale structures endowed with exotic quantum properties achieved through various manipulations of atoms and molecules. However, precise control requires extensive domain knowledge, which is not necessarily transferable to new systems and cannot be readily extended to large-scale operations. Therefore, efficient and autonomous SPM techniques are needed to learn optimal strategies for new systems, in particular for the challenge of controlling chemical reactions and hence offering a route to precise atomic and molecular construction. In this paper, we developed a software infrastructure named AutoOSS (\textbf{Auto}nomous \textbf{O}n-\textbf{S}urface \textbf{S}ynthesis) to automate bromine removal from hundreds of Zn(II)-5,15-bis(4-bromo-2,6-dimethylphenyl)porphyrin (\ch{ZnBr2Me4DPP}) on Au(111), using neural network models to interpret STM outputs and deep reinforcement learning models to optimize manipulation parameters. This is further supported by Bayesian Optimization Structure Search (BOSS) and Density Functional Theory (DFT) computations to explore 3D structures and reaction mechanisms based on STM images.





## Robust Markov Decision Processes: A Place Where AI and Formal Methods Meet
- **Url**: http://arxiv.org/abs/2411.11451v2
- **Authors**: ['Marnix Suilen', 'Thom Badings', 'Eline M. Bovy', 'David Parker', 'Nils Jansen']
- **Abstrat**: Markov decision processes (MDPs) are a standard model for sequential decision-making problems and are widely used across many scientific areas, including formal methods and artificial intelligence (AI). MDPs do, however, come with the restrictive assumption that the transition probabilities need to be precisely known. Robust MDPs (RMDPs) overcome this assumption by instead defining the transition probabilities to belong to some uncertainty set. We present a gentle survey on RMDPs, providing a tutorial covering their fundamentals. In particular, we discuss RMDP semantics and how to solve them by extending standard MDP methods such as value iteration and policy iteration. We also discuss how RMDPs relate to other models and how they are used in several contexts, including reinforcement learning and abstraction techniques. We conclude with some challenges for future work on RMDPs.





## Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision
- **Url**: http://arxiv.org/abs/2403.09472v2
- **Authors**: ['Zhiqing Sun', 'Longhui Yu', 'Yikang Shen', 'Weiyang Liu', 'Yiming Yang', 'Sean Welleck', 'Chuang Gan']
- **Abstrat**: Current AI alignment methodologies rely on human-provided demonstrations or judgments, and the learned capabilities of AI systems would be upper-bounded by human capabilities as a result. This raises a challenging research question: How can we keep improving the systems when their capabilities have surpassed the levels of humans? This paper answers this question in the context of tackling hard reasoning tasks (e.g., level 4-5 MATH problems) via learning from human annotations on easier tasks (e.g., level 1-3 MATH problems), which we term as easy-to-hard generalization. Our key insight is that an evaluator (reward model) trained on supervisions for easier tasks can be effectively used for scoring candidate solutions of harder tasks and hence facilitating easy-to-hard generalization over different levels of tasks. Based on this insight, we propose a novel approach to scalable alignment, which firstly trains the (process-supervised) reward models on easy problems (e.g., level 1-3), and then uses them to evaluate the performance of policy models on hard problems. We show that such easy-to-hard generalization from evaluators can enable easy-to-hard generalizations in generators either through re-ranking or reinforcement learning (RL). Notably, our process-supervised 7b RL model and 34b model (reranking@1024) achieves an accuracy of 34.0% and 52.5% on MATH500, respectively, despite only using human supervision on easy problems. Our approach suggests a promising path toward AI systems that advance beyond the frontier of human supervision.





## Ensuring Safety in Target Pursuit Control: A CBF-Safe Reinforcement Learning Approach
- **Url**: http://arxiv.org/abs/2411.17552v2
- **Authors**: ['Yaosheng Deng', 'Junjie Gao', 'Jiaping Xiao', 'Mir Feroskhan']
- **Abstrat**: This paper addresses the target-pursuit problem, aiming to ensure each pursuer's safety regarding collision avoidance, sensing range, and input saturation. An input-constrained CBF is proposed to dynamically regulate the pursuer's control, ensuring effective target pursuit even when the target performs evasive maneuvers. To further ensure safety, two sets of CBF constraints are designed to regulate the pursuer's position, enabling it to keep the target within the sensing range while avoiding collision in complex environments with external disturbances. These three CBFs collectively form our safety filter, which filters unsafe outputs from RL by solving a Quadratic Program (QP). Finally, the safety filter, combined with a switch strategy that enhances the feasibility of solving its QP, constitutes the Control Barrier Function (CBF)-Safe Reinforcement Learning (CSRL) algorithm, whose solutions are proven to satisfy the Karush-Kuhn-Tucker (KKT) conditions for all safety constraints. Simulation results validate the effectiveness of the CSRL algorithm, demonstrating its ability to handle complex pursuit scenarios while maintaining safety and improving control performance.





# TD3
# Prioritized Experience Replay
# path planning