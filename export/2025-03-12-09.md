# reinforcement learning
## Proto Successor Measure: Representing the Behavior Space of an RL Agent
- **Url**: http://arxiv.org/abs/2411.19418v2
- **Authors**: ['Siddhant Agarwal', 'Harshit Sikchi', 'Peter Stone', 'Amy Zhang']
- **Abstrat**: Having explored an environment, intelligent agents should be able to transfer their knowledge to most downstream tasks within that environment without additional interactions. Referred to as "zero-shot learning", this ability remains elusive for general-purpose reinforcement learning algorithms. While recent works have attempted to produce zero-shot RL agents, they make assumptions about the nature of the tasks or the structure of the MDP. We present Proto Successor Measure: the basis set for all possible behaviors of a Reinforcement Learning Agent in a dynamical system. We prove that any possible behavior (represented using visitation distributions) can be represented using an affine combination of these policy-independent basis functions. Given a reward function at test time, we simply need to find the right set of linear weights to combine these bases corresponding to the optimal policy. We derive a practical algorithm to learn these basis functions using reward-free interaction data from the environment and show that our approach can produce the optimal policy at test time for any given reward function without additional environmental interactions. Project page: https://agarwalsiddhant10.github.io/projects/psm.html.





## Curriculum Direct Preference Optimization for Diffusion and Consistency Models
- **Url**: http://arxiv.org/abs/2405.13637v3
- **Authors**: ['Florinel-Alin Croitoru', 'Vlad Hondru', 'Radu Tudor Ionescu', 'Nicu Sebe', 'Mubarak Shah']
- **Abstrat**: Direct Preference Optimization (DPO) has been proposed as an effective and efficient alternative to reinforcement learning from human feedback (RLHF). In this paper, we propose a novel and enhanced version of DPO based on curriculum learning for text-to-image generation. Our method is divided into two training stages. First, a ranking of the examples generated for each prompt is obtained by employing a reward model. Then, increasingly difficult pairs of examples are sampled and provided to a text-to-image generative (diffusion or consistency) model. Generated samples that are far apart in the ranking are considered to form easy pairs, while those that are close in the ranking form hard pairs. In other words, we use the rank difference between samples as a measure of difficulty. The sampled pairs are split into batches according to their difficulty levels, which are gradually used to train the generative model. Our approach, Curriculum DPO, is compared against state-of-the-art fine-tuning approaches on nine benchmarks, outperforming the competing methods in terms of text alignment, aesthetics and human preference. Our code is available at https://github.com/CroitoruAlin/Curriculum-DPO.





## MoE-Loco: Mixture of Experts for Multitask Locomotion
- **Url**: http://arxiv.org/abs/2503.08564v1
- **Authors**: ['Runhan Huang', 'Shaoting Zhu', 'Yilun Du', 'Hang Zhao']
- **Abstrat**: We present MoE-Loco, a Mixture of Experts (MoE) framework for multitask locomotion for legged robots. Our method enables a single policy to handle diverse terrains, including bars, pits, stairs, slopes, and baffles, while supporting quadrupedal and bipedal gaits. Using MoE, we mitigate the gradient conflicts that typically arise in multitask reinforcement learning, improving both training efficiency and performance. Our experiments demonstrate that different experts naturally specialize in distinct locomotion behaviors, which can be leveraged for task migration and skill composition. We further validate our approach in both simulation and real-world deployment, showcasing its robustness and adaptability.





## Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning
- **Url**: http://arxiv.org/abs/2411.18203v4
- **Authors**: ['Di Zhang', 'Junxian Li', 'Jingdi Lei', 'Xunzhi Wang', 'Yujie Liu', 'Zonglin Yang', 'Jiatong Li', 'Weida Wang', 'Suorong Yang', 'Jianbo Wu', 'Peng Ye', 'Wanli Ouyang', 'Dongzhan Zhou']
- **Abstrat**: Vision-language models (VLMs) have shown remarkable advancements in multimodal reasoning tasks. However, they still often generate inaccurate or irrelevant responses due to issues like hallucinated image understandings or unrefined reasoning paths. To address these challenges, we introduce Critic-V, a novel framework inspired by the Actor-Critic paradigm to boost the reasoning capability of VLMs. This framework decouples the reasoning process and critic process by integrating two independent components: the Reasoner, which generates reasoning paths based on visual and textual inputs, and the Critic, which provides constructive critique to refine these paths. In this approach, the Reasoner generates reasoning responses according to text prompts, which can evolve iteratively as a policy based on feedback from the Critic. This interaction process was theoretically driven by a reinforcement learning framework where the Critic offers natural language critiques instead of scalar rewards, enabling more nuanced feedback to boost the Reasoner's capability on complex reasoning tasks. The Critic model is trained using Direct Preference Optimization (DPO), leveraging a preference dataset of critiques ranked by Rule-based Reward~(RBR) to enhance its critic capabilities. Evaluation results show that the Critic-V framework significantly outperforms existing methods, including GPT-4V, on 5 out of 8 benchmarks, especially regarding reasoning accuracy and efficiency. Combining a dynamic text-based policy for the Reasoner and constructive feedback from the preference-optimized Critic enables a more reliable and context-sensitive multimodal reasoning process. Our approach provides a promising solution to enhance the reliability of VLMs, improving their performance in real-world reasoning-heavy multimodal applications such as autonomous driving and embodied intelligence.





## From Classification to Optimization: Slicing and Resource Management with TRACTOR
- **Url**: http://arxiv.org/abs/2312.07896v2
- **Authors**: ['Joshua Groen', 'Zixian Yang', 'Divyadharshini Muruganandham', 'Mauro Belgiovine', 'Lei Ying', 'Kaushik Chowdhury']
- **Abstrat**: 5G and beyond networks promise advancements in bandwidth, latency, and connectivity. The Open Radio Access Network (O-RAN) framework enhances flexibility through network slicing and closed-loop RAN control. Central to this evolution is integrating machine learning (ML) for dynamic network control. This paper presents a framework to optimize O-RAN operation. First, we build and share a robust O-RAN dataset from real-world traffic captured across diverse locations and mobility scenarios, replicated within a full-stack srsRAN-based O-RAN system using the Colosseum RF emulator. This dataset supports ML training and deployment. We then introduce a traffic classification approach leveraging various ML models, demonstrating rapid training, testing, and refinement to improve accuracy. With up to 99% offline accuracy and 92% online accuracy for specific slices, our framework adapts efficiently to different models and network conditions. Finally, we present a physical resource block (PRB) assignment optimization strategy using reinforcement learning to refine resource allocation. Our learned policy achieves a mean performance score (0.631), surpassing a manually configured expert policy (0.609) and a random baseline (0.588), demonstrating improved PRB utilization. More importantly, our approach exhibits lower variability, with the Coefficient of Variation (CV) reduced by up to an order of magnitude in three out of four cases, ensuring more consistent performance. Our contributions, including open-source tools and datasets, accelerate O-RAN and ML-driven network control research.





## GTR: Guided Thought Reinforcement Prevents Thought Collapse in RL-based VLM Agent Training
- **Url**: http://arxiv.org/abs/2503.08525v1
- **Authors**: ['Tong Wei', 'Yijun Yang', 'Junliang Xing', 'Yuanchun Shi', 'Zongqing Lu', 'Deheng Ye']
- **Abstrat**: Reinforcement learning with verifiable outcome rewards (RLVR) has effectively scaled up chain-of-thought (CoT) reasoning in large language models (LLMs). Yet, its efficacy in training vision-language model (VLM) agents for goal-directed action reasoning in visual environments is less established. This work investigates this problem through extensive experiments on complex card games, such as 24 points, and embodied tasks from ALFWorld. We find that when rewards are based solely on action outcomes, RL fails to incentivize CoT reasoning in VLMs, instead leading to a phenomenon we termed thought collapse, characterized by a rapid loss of diversity in the agent's thoughts, state-irrelevant and incomplete reasoning, and subsequent invalid actions, resulting in negative rewards. To counteract thought collapse, we highlight the necessity of process guidance and propose an automated corrector that evaluates and refines the agent's reasoning at each RL step. This simple and scalable GTR (Guided Thought Reinforcement) framework trains reasoning and action simultaneously without the need for dense, per-step human labeling. Our experiments demonstrate that GTR significantly enhances the performance and generalization of the LLaVA-7b model across various visual environments, achieving 3-5 times higher task success rates compared to SoTA models with notably smaller model sizes.





## PCA-Featured Transformer for Jamming Detection in 5G UAV Networks
- **Url**: http://arxiv.org/abs/2412.15312v2
- **Authors**: ['Joseanne Viana', 'Hamed Farkhari', 'Pedro Sebastiao', 'Victor P Gil Jimenez']
- **Abstrat**: Unmanned Aerial Vehicles (UAVs) face significant security risks from jamming attacks, which can compromise network functionality. Traditional detection methods often fall short when confronting AI-powered jamming that dynamically modifies its behavior, while contemporary machine learning approaches frequently demand substantial feature engineering and struggle with temporal patterns in attack signatures. The vulnerability extends to 5G networks employing Time Division Duplex (TDD) or Frequency Division Duplex (FDD), where service quality may deteriorate due to deliberate interference. We introduce a novel U-shaped transformer architecture that leverages Principal Component Analysis (PCA) to refine feature representations for improved wireless security. The training process is regularized by incorporating the output entropy uncertainty into the loss function, a mechanism inspired by the Soft Actor-Critic (SAC) algorithm in Reinforcement Learning (RL) to enable robust jamming detection techniques. The architecture features a modified transformer encoder specially designed to process critical wireless signal features, including Received Signal Strength Indicator (RSSI) and Signal-to-Interference-plus-Noise Ratio (SINR) measurements. We complement this with a custom positional encoding mechanism that specifically accounts for the inherent periodicity of wireless signals,enabling a more accurate representation of temporal signal patterns. In addition, we propose a batch size scheduler and implement chunking techniques to optimize convergence for time series data. These advancements contribute to up to a ten times improvement in training speed within the advanced U-shaped encoder-decoder transformer model introduced in this study. Experimental evaluations demonstrate the effectiveness of our entropy-based approach, achieving detection rates of 85.06% in NLoS scenarios.





## Hierarchical Multi Agent DRL for Soft Handovers Between Edge Clouds in Open RAN
- **Url**: http://arxiv.org/abs/2503.08493v1
- **Authors**: ['F. Giarrè', 'I. A. Meer', 'M. Masoudi', 'M. Ozger', 'C. Cavdar']
- **Abstrat**: Multi-connectivity (MC) for aerial users via a set of ground access points offers the potential for highly reliable communication. Within an open radio access network (O-RAN) architecture, edge clouds (ECs) enable MC with low latency for users within their coverage area. However, ensuring seamless service continuity for transitional users-those moving between the coverage areas of neighboring ECs-poses challenges due to centralized processing demands. To address this, we formulate a problem facilitating soft handovers between ECs, ensuring seamless transitions while maintaining service continuity for all users. We propose a hierarchical multi-agent reinforcement learning (HMARL) algorithm to dynamically determine the optimal functional split configuration for transitional and non-transitional users. Simulation results show that the proposed approach outperforms the conventional functional split in terms of the percentage of users maintaining service continuity, with at most 4% optimality gap. Additionally, HMARL achieves better scalability compared to the static baselines.





## Hybrid Deep Reinforcement Learning for Radio Tracer Localisation in Robotic-assisted Radioguided Surgery
- **Url**: http://arxiv.org/abs/2503.08492v1
- **Authors**: ['Hanyi Zhang', 'Kaizhong Deng', 'Zhaoyang Jacopo Hu', 'Baoru Huang', 'Daniel S. Elson']
- **Abstrat**: Radioguided surgery, such as sentinel lymph node biopsy, relies on the precise localization of radioactive targets by non-imaging gamma/beta detectors. Manual radioactive target detection based on visual display or audible indication of gamma level is highly dependent on the ability of the surgeon to track and interpret the spatial information. This paper presents a learning-based method to realize the autonomous radiotracer detection in robot-assisted surgeries by navigating the probe to the radioactive target. We proposed novel hybrid approach that combines deep reinforcement learning (DRL) with adaptive robotic scanning. The adaptive grid-based scanning could provide initial direction estimation while the DRL-based agent could efficiently navigate to the target utilising historical data. Simulation experiments demonstrate a 95% success rate, and improved efficiency and robustness compared to conventional techniques. Real-world evaluation on the da Vinci Research Kit (dVRK) further confirms the feasibility of the approach, achieving an 80% success rate in radiotracer detection. This method has the potential to enhance consistency, reduce operator dependency, and improve procedural accuracy in radioguided surgeries.





## An autonomous rl agent methodology for dynamic Web ui testing in a bdd framework
- **Url**: http://arxiv.org/abs/2503.08464v1
- **Authors**: ['Ali Hassaan Mughal']
- **Abstrat**: Modern software applications demand efficient and reliable testing methodologies to ensure robust   user interface functionality. This paper introduces an autonomous reinforcement learning (RL) agent   integrated within a Behavior-Driven Development (BDD) framework to enhance UI testing. By   leveraging the adaptive decision-making capabilities of RL, the proposed approach dynamically   generates and refines test scenarios aligned with specific business expectations and actual user   behavior. A novel system architecture is presented, detailing the state representation, action space,   and reward mechanisms that guide the autonomous exploration of UI states. Experimental evaluations   on open-source web applications demonstrate significant improvements in defect detection, test   coverage, and a reduction in manual testing efforts. This study establishes a foundation for integrating   advanced RL techniques with BDD practices, aiming to transform software quality assurance and   streamline continuous testing processes.





## V-Max: Making RL practical for Autonomous Driving
- **Url**: http://arxiv.org/abs/2503.08388v1
- **Authors**: ['Valentin Charraut', 'Thomas Tournaire', 'Waël Doulazmi', 'Thibault Buhet']
- **Abstrat**: Learning-based decision-making has the potential to enable generalizable Autonomous Driving (AD) policies, reducing the engineering overhead of rule-based approaches. Imitation Learning (IL) remains the dominant paradigm, benefiting from large-scale human demonstration datasets, but it suffers from inherent limitations such as distribution shift and imitation gaps. Reinforcement Learning (RL) presents a promising alternative, yet its adoption in AD remains limited due to the lack of standardized and efficient research frameworks. To this end, we introduce V-Max, an open research framework providing all the necessary tools to make RL practical for AD. V-Max is built on Waymax, a hardware-accelerated AD simulator designed for large-scale experimentation. We extend it using ScenarioNet's approach, enabling the fast simulation of diverse AD datasets. V-Max integrates a set of observation and reward functions, transformer-based encoders, and training pipelines. Additionally, it includes adversarial evaluation settings and an extensive set of evaluation metrics. Through a large-scale benchmark, we analyze how network architectures, observation functions, training data, and reward shaping impact RL performance.





## Synthesizing Programmatic Reinforcement Learning Policies with Large Language Model Guided Search
- **Url**: http://arxiv.org/abs/2405.16450v3
- **Authors**: ['Max Liu', 'Chan-Hung Yu', 'Wei-Hsu Lee', 'Cheng-Wei Hung', 'Yen-Chun Chen', 'Shao-Hua Sun']
- **Abstrat**: Programmatic reinforcement learning (PRL) has been explored for representing policies through programs as a means to achieve interpretability and generalization. Despite promising outcomes, current state-of-the-art PRL methods are hindered by sample inefficiency, necessitating tens of millions of program-environment interactions. To tackle this challenge, we introduce a novel LLM-guided search framework (LLM-GS). Our key insight is to leverage the programming expertise and common sense reasoning of LLMs to enhance the efficiency of assumption-free, random-guessing search methods. We address the challenge of LLMs' inability to generate precise and grammatically correct programs in domain-specific languages (DSLs) by proposing a Pythonic-DSL strategy - an LLM is instructed to initially generate Python codes and then convert them into DSL programs. To further optimize the LLM-generated programs, we develop a search algorithm named Scheduled Hill Climbing, designed to efficiently explore the programmatic search space to improve the programs consistently. Experimental results in the Karel domain demonstrate our LLM-GS framework's superior effectiveness and efficiency. Extensive ablation studies further verify the critical role of our Pythonic-DSL strategy and Scheduled Hill Climbing algorithm. Moreover, we conduct experiments with two novel tasks, showing that LLM-GS enables users without programming skills and knowledge of the domain or DSL to describe the tasks in natural language to obtain performant programs.





## Leveraging Symmetry in RL-based Legged Locomotion Control
- **Url**: http://arxiv.org/abs/2403.17320v3
- **Authors**: ['Zhi Su', 'Xiaoyu Huang', 'Daniel Ordoñez-Apraez', 'Yunfei Li', 'Zhongyu Li', 'Qiayuan Liao', 'Giulio Turrisi', 'Massimiliano Pontil', 'Claudio Semini', 'Yi Wu', 'Koushil Sreenath']
- **Abstrat**: Model-free reinforcement learning is a promising approach for autonomously solving challenging robotics control problems, but faces exploration difficulty without information of the robot's kinematics and dynamics morphology. The under-exploration of multiple modalities with symmetric states leads to behaviors that are often unnatural and sub-optimal. This issue becomes particularly pronounced in the context of robotic systems with morphological symmetries, such as legged robots for which the resulting asymmetric and aperiodic behaviors compromise performance, robustness, and transferability to real hardware. To mitigate this challenge, we can leverage symmetry to guide and improve the exploration in policy learning via equivariance/invariance constraints. In this paper, we investigate the efficacy of two approaches to incorporate symmetry: modifying the network architectures to be strictly equivariant/invariant, and leveraging data augmentation to approximate equivariant/invariant actor-critics. We implement the methods on challenging loco-manipulation and bipedal locomotion tasks and compare with an unconstrained baseline. We find that the strictly equivariant policy consistently outperforms other methods in sample efficiency and task performance in simulation. In addition, symmetry-incorporated approaches exhibit better gait quality, higher robustness and can be deployed zero-shot in real-world experiments.





## Gait in Eight: Efficient On-Robot Learning for Omnidirectional Quadruped Locomotion
- **Url**: http://arxiv.org/abs/2503.08375v1
- **Authors**: ['Nico Bohlinger', 'Jonathan Kinzel', 'Daniel Palenicek', 'Lukasz Antczak', 'Jan Peters']
- **Abstrat**: On-robot Reinforcement Learning is a promising approach to train embodiment-aware policies for legged robots. However, the computational constraints of real-time learning on robots pose a significant challenge. We present a framework for efficiently learning quadruped locomotion in just 8 minutes of raw real-time training utilizing the sample efficiency and minimal computational overhead of the new off-policy algorithm CrossQ. We investigate two control architectures: Predicting joint target positions for agile, high-speed locomotion and Central Pattern Generators for stable, natural gaits. While prior work focused on learning simple forward gaits, our framework extends on-robot learning to omnidirectional locomotion. We demonstrate the robustness of our approach in different indoor and outdoor environments.





## LiPS: Large-Scale Humanoid Robot Reinforcement Learning with Parallel-Series Structures
- **Url**: http://arxiv.org/abs/2503.08349v1
- **Authors**: ['Qiang Zhang', 'Gang Han', 'Jingkai Sun', 'Wen Zhao', 'Jiahang Cao', 'Jiaxu Wang', 'Hao Cheng', 'Lingfeng Zhang', 'Yijie Guo', 'Renjing Xu']
- **Abstrat**: In recent years, research on humanoid robots has garnered significant attention, particularly in reinforcement learning based control algorithms, which have achieved major breakthroughs. Compared to traditional model-based control algorithms, reinforcement learning based algorithms demonstrate substantial advantages in handling complex tasks. Leveraging the large-scale parallel computing capabilities of GPUs, contemporary humanoid robots can undergo extensive parallel training in simulated environments. A physical simulation platform capable of large-scale parallel training is crucial for the development of humanoid robots. As one of the most complex robot forms, humanoid robots typically possess intricate mechanical structures, encompassing numerous series and parallel mechanisms. However, many reinforcement learning based humanoid robot control algorithms currently employ open-loop topologies during training, deferring the conversion to series-parallel structures until the sim2real phase. This approach is primarily due to the limitations of physics engines, as current GPU-based physics engines often only support open-loop topologies or have limited capabilities in simulating multi-rigid-body closed-loop topologies. For enabling reinforcement learning-based humanoid robot control algorithms to train in large-scale parallel environments, we propose a novel training method LiPS. By incorporating multi-rigid-body dynamics modeling in the simulation environment, we significantly reduce the sim2real gap and the difficulty of converting to parallel structures during model deployment, thereby robustly supporting large-scale reinforcement learning for humanoid robots.





## Trinity: A Modular Humanoid Robot AI System
- **Url**: http://arxiv.org/abs/2503.08338v1
- **Authors**: ['Jingkai Sun', 'Qiang Zhang', 'Gang Han', 'Wen Zhao', 'Zhe Yong', 'Yan He', 'Jiaxu Wang', 'Jiahang Cao', 'Yijie Guo', 'Renjing Xu']
- **Abstrat**: In recent years, research on humanoid robots has garnered increasing attention. With breakthroughs in various types of artificial intelligence algorithms, embodied intelligence, exemplified by humanoid robots, has been highly anticipated. The advancements in reinforcement learning (RL) algorithms have significantly improved the motion control and generalization capabilities of humanoid robots. Simultaneously, the groundbreaking progress in large language models (LLM) and visual language models (VLM) has brought more possibilities and imagination to humanoid robots. LLM enables humanoid robots to understand complex tasks from language instructions and perform long-term task planning, while VLM greatly enhances the robots' understanding and interaction with their environment. This paper introduces \textcolor{magenta}{Trinity}, a novel AI system for humanoid robots that integrates RL, LLM, and VLM. By combining these technologies, Trinity enables efficient control of humanoid robots in complex environments. This innovative approach not only enhances the capabilities but also opens new avenues for future research and applications of humanoid robotics.





## Evaluating Interpretable Reinforcement Learning by Distilling Policies into Programs
- **Url**: http://arxiv.org/abs/2503.08322v1
- **Authors**: ['Hector Kohler', 'Quentin Delfosse', 'Waris Radji', 'Riad Akrour', 'Philippe Preux']
- **Abstrat**: There exist applications of reinforcement learning like medicine where policies need to be ''interpretable'' by humans. User studies have shown that some policy classes might be more interpretable than others. However, it is costly to conduct human studies of policy interpretability. Furthermore, there is no clear definition of policy interpretabiliy, i.e., no clear metrics for interpretability and thus claims depend on the chosen definition. We tackle the problem of empirically evaluating policies interpretability without humans. Despite this lack of clear definition, researchers agree on the notions of ''simulatability'': policy interpretability should relate to how humans understand policy actions given states. To advance research in interpretable reinforcement learning, we contribute a new methodology to evaluate policy interpretability. This new methodology relies on proxies for simulatability that we use to conduct a large-scale empirical evaluation of policy interpretability. We use imitation learning to compute baseline policies by distilling expert neural networks into small programs. We then show that using our methodology to evaluate the baselines interpretability leads to similar conclusions as user studies. We show that increasing interpretability does not necessarily reduce performances and can sometimes increase them. We also show that there is no policy class that better trades off interpretability and performance across tasks making it necessary for researcher to have methodologies for comparing policies interpretability.





## Reinforcement learning assisted non-reciprocal optomechanical gyroscope
- **Url**: http://arxiv.org/abs/2503.08319v1
- **Authors**: ['Qing-Shou Tan', 'Ya-Feng Jiao', 'Yunlan Zuo', 'Lan Xu', 'Jie-Qiao Liao', 'Le-Man Kuang']
- **Abstrat**: We propose a novel optomechanical gyroscope architecture based on a spinning cavity optomechanical resonator (COM) evanescently coupled to a tapered optical fiber without relying on costly quantum light sources. Our study reveals a striking dependence of the gyroscope's sensitivity on the propagation direction of the driving optical field, manifesting robust quantum non-reciprocal behavior. This non-reciprocity significantly enhances the precision of angular velocity estimation, offering a unique advantage over conventional gyroscopic systems. Furthermore, we demonstrate that the operational range of this non-reciprocal gyroscope is fundamentally governed by the frequency of the pumping optical field, enabling localized sensitivity to angular velocity. Leveraging the adaptive capabilities of reinforcement learning (RL), we optimize the gyroscope's sensitivity within a targeted angular velocity range, achieving unprecedented levels of precision. These results highlight the transformative potential of RL in advancing high-resolution, miniaturized optomechanical gyroscopes, opening new avenues for next-generation inertial sensing technologies.





# TD3
## Value Improved Actor Critic Algorithms
- **Url**: http://arxiv.org/abs/2406.01423v2
- **Authors**: ['Yaniv Oren', 'Moritz A. Zanger', 'Pascal R. van der Vaart', 'Mustafa Mert Celikok', 'Matthijs T. J. Spaan', 'Wendelin Bohmer']
- **Abstrat**: To learn approximately optimal acting policies for decision problems, modern Actor Critic algorithms rely on deep Neural Networks (DNNs) to parameterize the acting policy and greedification operators to iteratively improve it. The reliance on DNNs suggests an improvement that is gradient based, which is per step much less greedy than the improvement possible by greedier operators such as the greedy update used by Q-learning algorithms. On the other hand, slow and steady changes to the policy can also be beneficial for the stability of the learning process, resulting in a tradeoff between greedification and stability. To address this tradeoff, we propose to extend the standard framework of actor critic algorithms with value-improvement: a second greedification operator applied only when updating the policy's value estimate. In this framework the agent can evaluate non-parameterized policies and perform much greedier updates while maintaining the steady gradient-based improvement to the parameterized acting policy. We prove that this approach converges in the popular analysis scheme of Generalized Policy Iteration in the finite-horizon domain. Empirically, incorporating value-improvement into the popular off-policy actor-critic algorithms TD3 and SAC significantly improves or matches performance over their respective baselines, across different environments from the DeepMind continuous control domain, with negligible compute and implementation cost.





# Prioritized Experience Replay
# path planning
## Force Aware Branch Manipulation To Assist Agricultural Tasks
- **Url**: http://arxiv.org/abs/2503.07497v2
- **Authors**: ['Madhav Rijal', 'Rashik Shrestha', 'Trevor Smith', 'Yu Gu']
- **Abstrat**: This study presents a methodology to safely manipulate branches to aid various agricultural tasks. Humans in a real agricultural environment often manipulate branches to perform agricultural tasks effectively, but current agricultural robots lack this capability. This proposed strategy to manipulate branches can aid in different precision agriculture tasks, such as fruit picking in dense foliage, pollinating flowers under occlusion, and moving overhanging vines and branches for navigation. The proposed method modifies RRT* to plan a path that satisfies the branch geometric constraints and obeys branch deformable characteristics. Re-planning is done to obtain a path that helps the robot exert force within a desired range so that branches are not damaged during manipulation. Experimentally, this method achieved a success rate of 78% across 50 trials, successfully moving a branch from different starting points to a target region.





## KiteRunner: Language-Driven Cooperative Local-Global Navigation Policy with UAV Mapping in Outdoor Environments
- **Url**: http://arxiv.org/abs/2503.08330v1
- **Authors**: ['Shibo Huang', 'Chenfan Shi', 'Jian Yang', 'Hanlin Dong', 'Jinpeng Mi', 'Ke Li', 'Jianfeng Zhang', 'Miao Ding', 'Peidong Liang', 'Xiong You', 'Xian Wei']
- **Abstrat**: Autonomous navigation in open-world outdoor environments faces challenges in integrating dynamic conditions, long-distance spatial reasoning, and semantic understanding. Traditional methods struggle to balance local planning, global planning, and semantic task execution, while existing large language models (LLMs) enhance semantic comprehension but lack spatial reasoning capabilities. Although diffusion models excel in local optimization, they fall short in large-scale long-distance navigation. To address these gaps, this paper proposes KiteRunner, a language-driven cooperative local-global navigation strategy that combines UAV orthophoto-based global planning with diffusion model-driven local path generation for long-distance navigation in open-world scenarios. Our method innovatively leverages real-time UAV orthophotography to construct a global probability map, providing traversability guidance for the local planner, while integrating large models like CLIP and GPT to interpret natural language instructions. Experiments demonstrate that KiteRunner achieves 5.6% and 12.8% improvements in path efficiency over state-of-the-art methods in structured and unstructured environments, respectively, with significant reductions in human interventions and execution time.





## Interaction-Aware Multi-Robot Kinodynamic Motion Planning
- **Url**: http://arxiv.org/abs/2309.16445v3
- **Authors**: ['Akmaral Moldagalieva', 'Joaquim Ortiz-Haro', 'Wolfgang Hönig']
- **Abstrat**: Kinodynamic motion planning for a multi-robot system with different dynamics and actuation limits is a challenging problem. The difficulty increases with the presence of an aerodynamic interaction force that occur in aerial robots flying in close-proximity. Due to these complexities, existing planners either rely on simplified assumption like ignoring robot dynamics, interaction forces or produce highly suboptimal solutions. This paper presents a kinodynamic motion planner for a heterogeneous team of robots that respects robot dynamics and directly reasons about interaction forces between aerial robots operating in close-proximity. Our method, db-ECBS, generalizes the multi-agent path finding method Enhanced Conflict-Based Search (ECBS) to the continuous domain by using the single-robot kinodynamic motion planner discontinuity-bounded A*. Db-ECBS operates on three levels. Initially, individual robot trajectories are computed using a graph search that allows bounded discontinuities between precomputed motion primitives. The second level identifies inter-robot collisions, interaction force violations and resolves them by imposing constraints on the first level. The third and final level uses the resulting solution with discontinuities as an initial guess for a joint space trajectory optimization. The procedure is repeated with a reduced discontinuity bound resulting in a anytime, probabilistically complete, and asymptotically bounded suboptimal planner. We provide a benchmark of 65 problems with six different dynamics. We demonstrate that db-ECBS produces trajectories that are less than half the cost of existing planners. We show that the interaction-awareness is in particular important for very dense scenarios.




