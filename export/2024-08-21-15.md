# reinforcement learning
## Accelerating Goal-Conditioned RL Algorithms and Research
- **Url**: http://arxiv.org/abs/2408.11052v1
- **Authors**: ['Michał Bortkiewicz', 'Władek Pałucki', 'Vivek Myers', 'Tadeusz Dziarmaga', 'Tomasz Arczewski', 'Łukasz Kuciński', 'Benjamin Eysenbach']
- **Abstrat**: Self-supervision has the potential to transform reinforcement learning (RL), paralleling the breakthroughs it has enabled in other areas of machine learning. While self-supervised learning in other domains aims to find patterns in a fixed dataset, self-supervised goal-conditioned reinforcement learning (GCRL) agents discover new behaviors by learning from the goals achieved during unstructured interaction with the environment. However, these methods have failed to see similar success, both due to a lack of data from slow environments as well as a lack of stable algorithms. We take a step toward addressing both of these issues by releasing a high-performance codebase and benchmark JaxGCRL for self-supervised GCRL, enabling researchers to train agents for millions of environment steps in minutes on a single GPU. The key to this performance is a combination of GPU-accelerated environments and a stable, batched version of the contrastive reinforcement learning algorithm, based on an infoNCE objective, that effectively makes use of this increased data throughput. With this approach, we provide a foundation for future research in self-supervised GCRL, enabling researchers to quickly iterate on new ideas and evaluate them in a diverse set of challenging environments. Website + Code: https://github.com/MichalBortkiewicz/JaxGCRL





## RP1M: A Large-Scale Motion Dataset for Piano Playing with Bi-Manual Dexterous Robot Hands
- **Url**: http://arxiv.org/abs/2408.11048v1
- **Authors**: ['Yi Zhao', 'Le Chen', 'Jan Schneider', 'Quankai Gao', 'Juho Kannala', 'Bernhard Schölkopf', 'Joni Pajarinen', 'Dieter Büchler']
- **Abstrat**: It has been a long-standing research goal to endow robot hands with human-level dexterity. Bi-manual robot piano playing constitutes a task that combines challenges from dynamic tasks, such as generating fast while precise motions, with slower but contact-rich manipulation problems. Although reinforcement learning based approaches have shown promising results in single-task performance, these methods struggle in a multi-song setting. Our work aims to close this gap and, thereby, enable imitation learning approaches for robot piano playing at scale. To this end, we introduce the Robot Piano 1 Million (RP1M) dataset, containing bi-manual robot piano playing motion data of more than one million trajectories. We formulate finger placements as an optimal transport problem, thus, enabling automatic annotation of vast amounts of unlabeled songs. Benchmarking existing imitation learning approaches shows that such approaches reach state-of-the-art robot piano playing performance by leveraging RP1M.





## Quantum Machine Learning Algorithms for Anomaly Detection: a Survey
- **Url**: http://arxiv.org/abs/2408.11047v1
- **Authors**: ['Sebastiano Corli', 'Lorenzo Moro', 'Daniele Dragoni', 'Massimiliano Dispenza', 'Enrico Prati']
- **Abstrat**: The advent of quantum computers has justified the development of quantum machine learning algorithms , based on the adaptation of the principles of machine learning to the formalism of qubits. Among such quantum algorithms, anomaly detection represents an important problem crossing several disciplines from cybersecurity, to fraud detection to particle physics. We summarize the key concepts involved in quantum computing, introducing the formal concept of quantum speed up. The survey provides a structured map of anomaly detection based on quantum machine learning. We have grouped existing algorithms according to the different learning methods, namely quantum supervised, quantum unsupervised and quantum reinforcement learning, respectively. We provide an estimate of the hardware resources to provide sufficient computational power in the future. The survey provides a systematic and compact understanding of the techniques belonging to each category. We eventually provide a discussion on the computational complexity of the learning methods in real application domains.





## The Evolution of Reinforcement Learning in Quantitative Finance
- **Url**: http://arxiv.org/abs/2408.10932v1
- **Authors**: ['Nikolaos Pippas', 'Cagatay Turkay', 'Elliot A. Ludvig']
- **Abstrat**: Reinforcement Learning (RL) has experienced significant advancement over the past decade, prompting a growing interest in applications within finance. This survey critically evaluates 167 publications, exploring diverse RL applications and frameworks in finance. Financial markets, marked by their complexity, multi-agent nature, information asymmetry, and inherent randomness, serve as an intriguing test-bed for RL. Traditional finance offers certain solutions, and RL advances these with a more dynamic approach, incorporating machine learning methods, including transfer learning, meta-learning, and multi-agent solutions. This survey dissects key RL components through the lens of Quantitative Finance. We uncover emerging themes, propose areas for future research, and critique the strengths and weaknesses of existing methods.





## SUBER: An RL Environment with Simulated Human Behavior for Recommender Systems
- **Url**: http://arxiv.org/abs/2406.01631v2
- **Authors**: ['Nathan Corecco', 'Giorgio Piatti', 'Luca A. Lanzendörfer', 'Flint Xiaofeng Fan', 'Roger Wattenhofer']
- **Abstrat**: Reinforcement learning (RL) has gained popularity in the realm of recommender systems due to its ability to optimize long-term rewards and guide users in discovering relevant content. However, the successful implementation of RL in recommender systems is challenging because of several factors, including the limited availability of online data for training on-policy methods. This scarcity requires expensive human interaction for online model training. Furthermore, the development of effective evaluation frameworks that accurately reflect the quality of models remains a fundamental challenge in recommender systems. To address these challenges, we propose a comprehensive framework for synthetic environments that simulate human behavior by harnessing the capabilities of large language models (LLMs). We complement our framework with in-depth ablation studies and demonstrate its effectiveness with experiments on movie and book recommendations. Using LLMs as synthetic users, this work introduces a modular and novel framework to train RL-based recommender systems. The software, including the RL environment, is publicly available on GitHub.





## Knowledge Sharing and Transfer via Centralized Reward Agent for Multi-Task Reinforcement Learning
- **Url**: http://arxiv.org/abs/2408.10858v1
- **Authors**: ['Haozhe Ma', 'Zhengding Luo', 'Thanh Vinh Vo', 'Kuankuan Sima', 'Tze-Yun Leong']
- **Abstrat**: Reward shaping is effective in addressing the sparse-reward challenge in reinforcement learning by providing immediate feedback through auxiliary informative rewards. Based on the reward shaping strategy, we propose a novel multi-task reinforcement learning framework, that integrates a centralized reward agent (CRA) and multiple distributed policy agents. The CRA functions as a knowledge pool, which aims to distill knowledge from various tasks and distribute it to individual policy agents to improve learning efficiency. Specifically, the shaped rewards serve as a straightforward metric to encode knowledge. This framework not only enhances knowledge sharing across established tasks but also adapts to new tasks by transferring valuable reward signals. We validate the proposed method on both discrete and continuous domains, demonstrating its robustness in multi-task sparse-reward settings and its effective transferability to unseen tasks.





# TD3
# Prioritized Experience Replay
# path planning
## Affordances-Oriented Planning using Foundation Models for Continuous Vision-Language Navigation
- **Url**: http://arxiv.org/abs/2407.05890v2
- **Authors**: ['Jiaqi Chen', 'Bingqian Lin', 'Xinmin Liu', 'Lin Ma', 'Xiaodan Liang', 'Kwan-Yee K. Wong']
- **Abstrat**: LLM-based agents have demonstrated impressive zero-shot performance in vision-language navigation (VLN) task. However, existing LLM-based methods often focus only on solving high-level task planning by selecting nodes in predefined navigation graphs for movements, overlooking low-level control in navigation scenarios. To bridge this gap, we propose AO-Planner, a novel Affordances-Oriented Planner for continuous VLN task. Our AO-Planner integrates various foundation models to achieve affordances-oriented low-level motion planning and high-level decision-making, both performed in a zero-shot setting. Specifically, we employ a Visual Affordances Prompting (VAP) approach, where the visible ground is segmented by SAM to provide navigational affordances, based on which the LLM selects potential candidate waypoints and plans low-level paths towards selected waypoints. We further propose a high-level PathAgent which marks planned paths into the image input and reasons the most probable path by comprehending all environmental information. Finally, we convert the selected path into 3D coordinates using camera intrinsic parameters and depth information, avoiding challenging 3D predictions for LLMs. Experiments on the challenging R2R-CE and RxR-CE datasets show that AO-Planner achieves state-of-the-art zero-shot performance (8.8% improvement on SPL). Our method can also serve as a data annotator to obtain pseudo-labels, distilling its waypoint prediction ability into a learning-based predictor. This new predictor does not require any waypoint data from the simulator and achieves 47% SR competing with supervised methods. We establish an effective connection between LLM and 3D world, presenting novel prospects for employing foundation models in low-level motion control.




