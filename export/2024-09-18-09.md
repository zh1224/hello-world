# reinforcement learning
## UniLCD: Unified Local-Cloud Decision-Making via Reinforcement Learning
- **Url**: http://arxiv.org/abs/2409.11403v1
- **Authors**: ['Kathakoli Sengupta', 'Zhongkai Shagguan', 'Sandesh Bharadwaj', 'Sanjay Arora', 'Eshed Ohn-Bar', 'Renato Mancuso']
- **Abstrat**: Embodied vision-based real-world systems, such as mobile robots, require a careful balance between energy consumption, compute latency, and safety constraints to optimize operation across dynamic tasks and contexts. As local computation tends to be restricted, offloading the computation, ie, to a remote server, can save local resources while providing access to high-quality predictions from powerful and large models. However, the resulting communication and latency overhead has led to limited usability of cloud models in dynamic, safety-critical, real-time settings. To effectively address this trade-off, we introduce UniLCD, a novel hybrid inference framework for enabling flexible local-cloud collaboration. By efficiently optimizing a flexible routing module via reinforcement learning and a suitable multi-task objective, UniLCD is specifically designed to support the multiple constraints of safety-critical end-to-end mobile systems. We validate the proposed approach using a challenging, crowded navigation task requiring frequent and timely switching between local and cloud operations. UniLCD demonstrates improved overall performance and efficiency, by over 35% compared to state-of-the-art baselines based on various split computing and early exit strategies.





## Robo-GS: A Physics Consistent Spatial-Temporal Model for Robotic Arm with Hybrid Representation
- **Url**: http://arxiv.org/abs/2408.14873v2
- **Authors**: ['Haozhe Lou', 'Yurong Liu', 'Yike Pan', 'Yiran Geng', 'Jianteng Chen', 'Wenlong Ma', 'Chenglong Li', 'Lin Wang', 'Hengzhen Feng', 'Lu Shi', 'Liyi Luo', 'Yongliang Shi']
- **Abstrat**: Real2Sim2Real plays a critical role in robotic arm control and reinforcement learning, yet bridging this gap remains a significant challenge due to the complex physical properties of robots and the objects they manipulate. Existing methods lack a comprehensive solution to accurately reconstruct real-world objects with spatial representations and their associated physics attributes.   We propose a Real2Sim pipeline with a hybrid representation model that integrates mesh geometry, 3D Gaussian kernels, and physics attributes to enhance the digital asset representation of robotic arms.   This hybrid representation is implemented through a Gaussian-Mesh-Pixel binding technique, which establishes an isomorphic mapping between mesh vertices and Gaussian models. This enables a fully differentiable rendering pipeline that can be optimized through numerical solvers, achieves high-fidelity rendering via Gaussian Splatting, and facilitates physically plausible simulation of the robotic arm's interaction with its environment using mesh-based methods.   The code,full presentation and datasets will be made publicly available at our website https://robostudioapp.com





## Integrating Reinforcement Learning and Model Predictive Control with Applications to Microgrids
- **Url**: http://arxiv.org/abs/2409.11267v1
- **Authors**: ['Caio Fabio Oliveira da Silva', 'Azita Dabiri', 'Bart De Schutter']
- **Abstrat**: This work proposes an approach that integrates reinforcement learning and model predictive control (MPC) to efficiently solve finite-horizon optimal control problems in mixed-logical dynamical systems. Optimization-based control of such systems with discrete and continuous decision variables entails the online solution of mixed-integer quadratic or linear programs, which suffer from the curse of dimensionality. Our approach aims at mitigating this issue by effectively decoupling the decision on the discrete variables and the decision on the continuous variables. Moreover, to mitigate the combinatorial growth in the number of possible actions due to the prediction horizon, we conceive the definition of decoupled Q-functions to make the learning problem more tractable. The use of reinforcement learning reduces the online optimization problem of the MPC controller from a mixed-integer linear (quadratic) program to a linear (quadratic) program, greatly reducing the computational time. Simulation experiments for a microgrid, based on real-world data, demonstrate that the proposed method significantly reduces the online computation time of the MPC approach and that it generates policies with small optimality gaps and high feasibility rates.





## Attacking Slicing Network via Side-channel Reinforcement Learning Attack
- **Url**: http://arxiv.org/abs/2409.11258v1
- **Authors**: ['Wei Shao', 'Chandra Thapa', 'Rayne Holland', 'Sarah Ali Siddiqui', 'Seyit Camtepe']
- **Abstrat**: Network slicing in 5G and the future 6G networks will enable the creation of multiple virtualized networks on a shared physical infrastructure. This innovative approach enables the provision of tailored networks to accommodate specific business types or industry users, thus delivering more customized and efficient services. However, the shared memory and cache in network slicing introduce security vulnerabilities that have yet to be fully addressed. In this paper, we introduce a reinforcement learning-based side-channel cache attack framework specifically designed for network slicing environments. Unlike traditional cache attack methods, our framework leverages reinforcement learning to dynamically identify and exploit cache locations storing sensitive information, such as authentication keys and user registration data. We assume that one slice network is compromised and demonstrate how the attacker can induce another shared slice to send registration requests, thereby estimating the cache locations of critical data. By formulating the cache timing channel attack as a reinforcement learning-driven guessing game between the attack slice and the victim slice, our model efficiently explores possible actions to pinpoint memory blocks containing sensitive information. Experimental results showcase the superiority of our approach, achieving a success rate of approximately 95\% to 98\% in accurately identifying the storage locations of sensitive data. This high level of accuracy underscores the potential risks in shared network slicing environments and highlights the need for robust security measures to safeguard against such advanced side-channel attacks.





## Design Optimization of NOMA Aided Multi-STAR-RIS for Indoor Environments: A Convex Approximation Imitated Reinforcement Learning Approach
- **Url**: http://arxiv.org/abs/2406.13280v2
- **Authors**: ['Yu Min Park', 'Sheikh Salman Hassan', 'Yan Kyaw Tun', 'Eui-Nam Huh', 'Walid Saad', 'Choong Seon Hong']
- **Abstrat**: Non-orthogonal multiple access (NOMA) enables multiple users to share the same frequency band, and simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS) provides 360-degree full-space coverage, optimizing both transmission and reflection for improved network performance and dynamic control of the indoor environment. However, deploying STAR-RIS indoors presents challenges in interference mitigation, power consumption, and real-time configuration. In this work, a novel network architecture utilizing multiple access points (APs), STAR-RISs, and NOMA is proposed for indoor communication. To address these, we formulate an optimization problem involving user assignment, access point (AP) beamforming, and STAR-RIS phase control. A decomposition approach is used to solve the complex problem efficiently, employing a many-to-one matching algorithm for user-AP assignment and K-means clustering for resource management. Additionally, multi-agent deep reinforcement learning (MADRL) is leveraged to optimize the control of the STAR-RIS. Within the proposed MADRL framework, a novel approach is introduced in which each decision variable acts as an independent agent, enabling collaborative learning and decision making. The MADRL framework is enhanced by incorporating convex approximation (CA), which accelerates policy learning through suboptimal solutions from successive convex approximation (SCA), leading to faster adaptation and convergence. Simulations demonstrate significant improvements in network utility compared to baseline approaches.





## Fault Detection for agents on power grid topology optimization: A Comprehensive analysis
- **Url**: http://arxiv.org/abs/2406.16426v3
- **Authors**: ['Malte Lehna', 'Mohamed Hassouna', 'Dmitry Degtyar', 'Sven Tomforde', 'Christoph Scholz']
- **Abstrat**: Optimizing the topology of transmission networks using Deep Reinforcement Learning (DRL) has increasingly come into focus. Various DRL agents have been proposed, which are mostly benchmarked on the Grid2Op environment from the Learning to Run a Power Network (L2RPN) challenges. The environments have many advantages with their realistic grid scenarios and underlying power flow backends. However, the interpretation of agent survival or failure is not always clear, as there are a variety of potential causes. In this work, we focus on the failures of the power grid simulation to identify patterns and detect them in advance. We collect the failed scenarios of three different agents on the WCCI 2022 L2RPN environment, totaling about 40k data points. By clustering, we are able to detect five distinct clusters, identifying common failure types. Further, we propose a multi-class prediction approach to detect failures beforehand and evaluate five different prediction models. Here, the Light Gradient-Boosting Machine (LightGBM) shows the best failure prediction performance, with an accuracy of 82%. It also accurately classifies whether a the grid survives or fails in 87% of cases. Finally, we provide a detailed feature importance analysis that identifies critical features and regions in the grid.





## LLM-as-a-Judge & Reward Model: What They Can and Cannot Do
- **Url**: http://arxiv.org/abs/2409.11239v1
- **Authors**: ['Guijin Son', 'Hyunwoo Ko', 'Hoyoung Lee', 'Yewon Kim', 'Seunghyeok Hong']
- **Abstrat**: LLM-as-a-Judge and reward models are widely used alternatives of multiple-choice questions or human annotators for large language model (LLM) evaluation. Their efficacy shines in evaluating long-form responses, serving a critical role as evaluators of leaderboards and as proxies to align LLMs via reinforcement learning. However, despite their popularity, their effectiveness outside of English remains largely unexplored. In this paper, we conduct a comprehensive analysis on automated evaluators, reporting key findings on their behavior in a non-English environment. First, we discover that English evaluation capabilities significantly influence language-specific capabilities, often more than the language proficiency itself, enabling evaluators trained in English to easily transfer their skills to other languages. Second, we identify critical shortcomings, where LLMs fail to detect and penalize errors, such as factual inaccuracies, cultural misrepresentations, and the presence of unwanted language. Finally, we release Kudge, the first non-English meta-evaluation dataset containing 5,012 human annotations in Korean.





## Leveraging Symmetry to Accelerate Learning of Trajectory Tracking Controllers for Free-Flying Robotic Systems
- **Url**: http://arxiv.org/abs/2409.11238v1
- **Authors**: ['Jake Welde', 'Nishanth Rao', 'Pratik Kunapuli', 'Dinesh Jayaraman', 'Vijay Kumar']
- **Abstrat**: Tracking controllers enable robotic systems to accurately follow planned reference trajectories. In particular, reinforcement learning (RL) has shown promise in the synthesis of controllers for systems with complex dynamics and modest online compute budgets. However, the poor sample efficiency of RL and the challenges of reward design make training slow and sometimes unstable, especially for high-dimensional systems. In this work, we leverage the inherent Lie group symmetries of robotic systems with a floating base to mitigate these challenges when learning tracking controllers. We model a general tracking problem as a Markov decision process (MDP) that captures the evolution of both the physical and reference states. Next, we prove that symmetry in the underlying dynamics and running costs leads to an MDP homomorphism, a mapping that allows a policy trained on a lower-dimensional "quotient" MDP to be lifted to an optimal tracking controller for the original system. We compare this symmetry-informed approach to an unstructured baseline, using Proximal Policy Optimization (PPO) to learn tracking controllers for three systems: the Particle (a forced point mass), the Astrobee (a fullyactuated space robot), and the Quadrotor (an underactuated system). Results show that a symmetry-aware approach both accelerates training and reduces tracking error after the same number of training steps.





## Linear Jamming Bandits: Learning to Jam 5G-based Coded Communications Systems
- **Url**: http://arxiv.org/abs/2409.11191v1
- **Authors**: ['Zachary Schutz', 'Daniel J. Jakubisin', 'Charles E. Thornton', 'R. Michael Buehrer']
- **Abstrat**: We study jamming of an OFDM-modulated signal which employs forward error correction coding. We extend this to leverage reinforcement learning with a contextual bandit to jam a 5G-based system implementing some aspects of the 5G protocol. This model introduces unreliable reward feedback in the form of ACK/NACK observations to the jammer to understand the effect of how imperfect observations of errors can affect the jammer's ability to learn. We gain insights into the convergence time of the jammer and its ability to jam a victim 5G waveform, as well as insights into the vulnerabilities of wireless communications for reinforcement learning-based jamming.





## Preventing Unconstrained CBF Safety Filters Caused by Invalid Relative Degree Assumptions
- **Url**: http://arxiv.org/abs/2409.11171v1
- **Authors**: ['Lukas Brunke', 'Siqi Zhou', 'Angela P. Schoellig']
- **Abstrat**: Control barrier function (CBF)-based safety filters are used to certify and modify potentially unsafe control inputs to a system such as those provided by a reinforcement learning agent or a non-expert user. In this context, safety is defined as the satisfaction of state constraints. Originally designed for continuous-time systems, CBF safety filters typically assume that the system's relative degree is well-defined and is constant across the domain; however, this assumption is restrictive and rarely verified -- even linear system dynamics with a quadratic CBF candidate may not satisfy this assumption. In real-world applications, continuous-time CBF safety filters are implemented in discrete time, exacerbating issues related to violating the condition on the relative degree. These violations can lead to the safety filter being unconstrained (any control input may be certified) for a finite time interval and result in chattering issues and constraint violations. We propose an alternative formulation to address these challenges. Specifically, we present a theoretically sound method that employs multiple CBFs to generate bounded control inputs at each state within the safe set, thereby preventing incorrect certification of arbitrary control inputs. Using this approach, we derive conditions on the maximum sampling time to ensure safety in discrete-time implementations. We demonstrate the effectiveness of our proposed method through simulations and real-world quadrotor experiments, successfully preventing chattering and constraint violations. Finally, we discuss the implications of violating the relative degree condition on CBF synthesis and learning-based CBF methods.





## Adaptive Reinforcement Learning for Robot Control
- **Url**: http://arxiv.org/abs/2404.18713v2
- **Authors**: ['Yu Tang Liu', 'Nilaksh Singh', 'Aamir Ahmad']
- **Abstrat**: Deep reinforcement learning (DRL) has shown remarkable success in simulation domains, yet its application in designing robot controllers remains limited, due to its single-task orientation and insufficient adaptability to environmental changes. To overcome these limitations, we present a novel adaptive agent that leverages transfer learning techniques to dynamically adapt policy in response to different tasks and environmental conditions. The approach is validated through the blimp control challenge, where multitasking capabilities and environmental adaptability are essential. The agent is trained using a custom, highly parallelized simulator built on IsaacGym. We perform zero-shot transfer to fly the blimp in the real world to solve various tasks. We share our code at \url{https://github.com/robot-perception-group/adaptive\_agent/}.





## Co-Designing Tools and Control Policies for Robust Manipulation
- **Url**: http://arxiv.org/abs/2409.11113v1
- **Authors**: ['Yifei Dong', 'Shaohang Han', 'Xianyi Cheng', 'Werner Friedl', 'Rafael I. Cabral Muchacho', 'Máximo A. Roa', 'Jana Tumova', 'Florian T. Pokorny']
- **Abstrat**: Inherent robustness in manipulation is prevalent in biological systems and critical for robotic manipulation systems due to real-world uncertainties and disturbances. This robustness relies not only on robust control policies but also on the design characteristics of the end-effectors. This paper introduces a bi-level optimization approach to co-designing tools and control policies to achieve robust manipulation. The approach employs reinforcement learning for lower-level control policy learning and multi-task Bayesian optimization for upper-level design optimization. Diverging from prior approaches, we incorporate caging-based robustness metrics into both levels, ensuring manipulation robustness against disturbances and environmental variations. Our method is evaluated in four non-prehensile manipulation environments, demonstrating improvements in task success rate under disturbances and environment changes. A real-world experiment is also conducted to validate the framework's practical effectiveness.





## Reactive Environments for Active Inference Agents with RxEnvironments.jl
- **Url**: http://arxiv.org/abs/2409.11087v1
- **Authors**: ['Wouter W. L. Nuijten', 'Bert de Vries']
- **Abstrat**: Active Inference is a framework that emphasizes the interaction between agents and their environment. While the framework has seen significant advancements in the development of agents, the environmental models are often borrowed from reinforcement learning problems, which may not fully capture the complexity of multi-agent interactions or allow complex, conditional communication. This paper introduces Reactive Environments, a comprehensive paradigm that facilitates complex multi-agent communication. In this paradigm, both agents and environments are defined as entities encapsulated by boundaries with interfaces. This setup facilitates a robust framework for communication in nonequilibrium-Steady-State systems, allowing for complex interactions and information exchange. We present a Julia package RxEnvironments.jl, which is a specific implementation of Reactive Environments, where we utilize a Reactive Programming style for efficient implementation. The flexibility of this paradigm is demonstrated through its application to several complex, multi-agent environments. These case studies highlight the potential of Reactive Environments in modeling sophisticated systems of interacting agents.





## A Reinforcement Learning Environment for Automatic Code Optimization in the MLIR Compiler
- **Url**: http://arxiv.org/abs/2409.11068v1
- **Authors**: ['Nazim Bendib', 'Iheb Nassim Aouadj', 'Riyadh Baghdadi']
- **Abstrat**: Code optimization is a crucial task aimed at enhancing code performance. However, this process is often tedious and complex, highlighting the necessity for automatic code optimization techniques. Reinforcement Learning (RL), a machine learning technique, has emerged as a promising approach for tackling such complex optimization problems. In this project, we introduce the first RL environment for the MLIR compiler, dedicated to facilitating MLIR compiler research, and enabling automatic code optimization using Multi-Action Reinforcement Learning. We also propose a novel formulation of the action space as a Cartesian product of simpler action subspaces, enabling more efficient and effective optimizations. Experimental results demonstrate that our proposed environment allows for an effective optimization of MLIR operations, and yields comparable performance to TensorFlow, surpassing it in multiple cases, highlighting the potential of RL-based optimization in compiler frameworks.





## On-policy Actor-Critic Reinforcement Learning for Multi-UAV Exploration
- **Url**: http://arxiv.org/abs/2409.11058v1
- **Authors**: ['Ali Moltajaei Farid', 'Jafar Roshanian', 'Malek Mouhoub']
- **Abstrat**: Unmanned aerial vehicles (UAVs) have become increasingly popular in various fields, including precision agriculture, search and rescue, and remote sensing. However, exploring unknown environments remains a significant challenge. This study aims to address this challenge by utilizing on-policy Reinforcement Learning (RL) with Proximal Policy Optimization (PPO) to explore the {two dimensional} area of interest with multiple UAVs. The UAVs will avoid collision with obstacles and each other and do the exploration in a distributed manner. The proposed solution includes actor-critic networks using deep convolutional neural networks {(CNN)} and long short-term memory (LSTM) for identifying the UAVs and areas that have already been covered. Compared to other RL techniques, such as policy gradient (PG) and asynchronous advantage actor-critic (A3C), the simulation results demonstrate the superiority of the proposed PPO approach. Also, the results show that combining LSTM with CNN in critic can improve exploration. Since the proposed exploration has to work in unknown environments, the results showed that the proposed setup can complete the coverage when we have new maps that differ from the trained maps. Finally, we showed how tuning hyper parameters may affect the overall performance.





## A Systematization of the Wagner Framework: Graph Theory Conjectures and Reinforcement Learning
- **Url**: http://arxiv.org/abs/2406.12667v2
- **Authors**: ['Flora Angileri', 'Giulia Lombardi', 'Andrea Fois', 'Renato Faraone', 'Carlo Metta', 'Michele Salvi', 'Luigi Amedeo Bianchi', 'Marco Fantozzi', 'Silvia Giulia Galfrè', 'Daniele Pavesi', 'Maurizio Parton', 'Francesco Morandin']
- **Abstrat**: In 2021, Adam Zsolt Wagner proposed an approach to disprove conjectures in graph theory using Reinforcement Learning (RL). Wagner's idea can be framed as follows: consider a conjecture, such as a certain quantity f(G) < 0 for every graph G; one can then play a single-player graph-building game, where at each turn the player decides whether to add an edge or not. The game ends when all edges have been considered, resulting in a certain graph G_T, and f(G_T) is the final score of the game; RL is then used to maximize this score. This brilliant idea is as simple as innovative, and it lends itself to systematic generalization. Several different single-player graph-building games can be employed, along with various RL algorithms. Moreover, RL maximizes the cumulative reward, allowing for step-by-step rewards instead of a single final score, provided the final cumulative reward represents the quantity of interest f(G_T). In this paper, we discuss these and various other choices that can be significant in Wagner's framework. As a contribution to this systematization, we present four distinct single-player graph-building games. Each game employs both a step-by-step reward system and a single final score. We also propose a principled approach to select the most suitable neural network architecture for any given conjecture, and introduce a new dataset of graphs labeled with their Laplacian spectra. Furthermore, we provide a counterexample for a conjecture regarding the sum of the matching number and the spectral radius, which is simpler than the example provided in Wagner's original paper.   The games have been implemented as environments in the Gymnasium framework, and along with the dataset, are available as open-source supplementary materials.





## Model-Based Epistemic Variance of Values for Risk-Aware Policy Optimization
- **Url**: http://arxiv.org/abs/2312.04386v3
- **Authors**: ['Carlos E. Luis', 'Alessandro G. Bottero', 'Julia Vinogradska', 'Felix Berkenkamp', 'Jan Peters']
- **Abstrat**: We consider the problem of quantifying uncertainty over expected cumulative rewards in model-based reinforcement learning. In particular, we focus on characterizing the variance over values induced by a distribution over Markov decision processes (MDPs). Previous work upper bounds the posterior variance over values by solving a so-called uncertainty Bellman equation (UBE), but the over-approximation may result in inefficient exploration. We propose a new UBE whose solution converges to the true posterior variance over values and leads to lower regret in tabular exploration problems. We identify challenges to apply the UBE theory beyond tabular problems and propose a suitable approximation. Based on this approximation, we introduce a general-purpose policy optimization algorithm, Q-Uncertainty Soft Actor-Critic (QU-SAC), that can be applied for either risk-seeking or risk-averse policy optimization with minimal changes. Experiments in both online and offline RL demonstrate improved performance compared to other uncertainty estimation methods.





## PIP-Loco: A Proprioceptive Infinite Horizon Planning Framework for Quadrupedal Robot Locomotion
- **Url**: http://arxiv.org/abs/2409.09441v2
- **Authors**: ['Aditya Shirwatkar', 'Naman Saxena', 'Kishore Chandra', 'Shishir Kolathaya']
- **Abstrat**: A core strength of Model Predictive Control (MPC) for quadrupedal locomotion has been its ability to enforce constraints and provide interpretability of the sequence of commands over the horizon. However, despite being able to plan, MPC struggles to scale with task complexity, often failing to achieve robust behavior on rapidly changing surfaces. On the other hand, model-free Reinforcement Learning (RL) methods have outperformed MPC on multiple terrains, showing emergent motions but inherently lack any ability to handle constraints or perform planning. To address these limitations, we propose a framework that integrates proprioceptive planning with RL, allowing for agile and safe locomotion behaviors through the horizon. Inspired by MPC, we incorporate an internal model that includes a velocity estimator and a Dreamer module. During training, the framework learns an expert policy and an internal model that are co-dependent, facilitating exploration for improved locomotion behaviors. During deployment, the Dreamer module solves an infinite-horizon MPC problem, adapting actions and velocity commands to respect the constraints. We validate the robustness of our training framework through ablation studies on internal model components and demonstrate improved robustness to training noise. Finally, we evaluate our approach across multi-terrain scenarios in both simulation and hardware.





# TD3
# Prioritized Experience Replay
# path planning
## Autonomous Navigation in Ice-Covered Waters with Learned Predictions on Ship-Ice Interactions
- **Url**: http://arxiv.org/abs/2409.11326v1
- **Authors**: ['Ninghan Zhong', 'Alessandro Potenza', 'Stephen L. Smith']
- **Abstrat**: Autonomous navigation in ice-covered waters poses significant challenges due to the frequent lack of viable collision-free trajectories. When complete obstacle avoidance is infeasible, it becomes imperative for the navigation strategy to minimize collisions. Additionally, the dynamic nature of ice, which moves in response to ship maneuvers, complicates the path planning process. To address these challenges, we propose a novel deep learning model to estimate the coarse dynamics of ice movements triggered by ship actions through occupancy estimation. To ensure real-time applicability, we propose a novel approach that caches intermediate prediction results and seamlessly integrates the predictive model into a graph search planner. We evaluate the proposed planner both in simulation and in a physical testbed against existing approaches and show that our planner significantly reduces collisions with ice when compared to the state-of-the-art. Codes and demos of this work are available at https://github.com/IvanIZ/predictive-asv-planner.





## Semformer: Transformer Language Models with Semantic Planning
- **Url**: http://arxiv.org/abs/2409.11143v1
- **Authors**: ['Yongjing Yin', 'Junran Ding', 'Kai Song', 'Yue Zhang']
- **Abstrat**: Next-token prediction serves as the dominant component in current neural language models. During the training phase, the model employs teacher forcing, which predicts tokens based on all preceding ground truth tokens. However, this approach has been found to create shortcuts, utilizing the revealed prefix to spuriously fit future tokens, potentially compromising the accuracy of the next-token predictor. In this paper, we introduce Semformer, a novel method of training a Transformer language model that explicitly models the semantic planning of response. Specifically, we incorporate a sequence of planning tokens into the prefix, guiding the planning token representations to predict the latent semantic representations of the response, which are induced by an autoencoder. In a minimal planning task (i.e., graph path-finding), our model exhibits near-perfect performance and effectively mitigates shortcut learning, a feat that standard training methods and baseline models have been unable to accomplish. Furthermore, we pretrain Semformer from scratch with 125M parameters, demonstrating its efficacy through measures of perplexity, in-context learning, and fine-tuning on summarization tasks.




