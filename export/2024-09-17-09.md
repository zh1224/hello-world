# reinforcement learning
## SARO: Space-Aware Robot System for Terrain Crossing via Vision-Language Model
- **Url**: http://arxiv.org/abs/2407.16412v2
- **Authors**: ['Shaoting Zhu', 'Derun Li', 'Linzhan Mou', 'Yong Liu', 'Ningyi Xu', 'Hang Zhao']
- **Abstrat**: The application of vision-language models (VLMs) has achieved impressive success in various robotics tasks. However, there are few explorations for these foundation models used in quadruped robot navigation through terrains in 3D environments. In this work, we introduce SARO (Space Aware Robot System for Terrain Crossing), an innovative system composed of a high-level reasoning module, a closed-loop sub-task execution module, and a low-level control policy. It enables the robot to navigate across 3D terrains and reach the goal position. For high-level reasoning and execution, we propose a novel algorithmic system taking advantage of a VLM, with a design of task decomposition and a closed-loop sub-task execution mechanism. For low-level locomotion control, we utilize the Probability Annealing Selection (PAS) method to effectively train a control policy by reinforcement learning. Numerous experiments show that our whole system can accurately and robustly navigate across several 3D terrains, and its generalization ability ensures the applications in diverse indoor and outdoor scenarios and terrains. Project page: https://saro-vlm.github.io/





## Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes
- **Url**: http://arxiv.org/abs/2408.03539v3
- **Authors**: ['Chen Tang', 'Ben Abbatematteo', 'Jiaheng Hu', 'Rohan Chandra', 'Roberto Martín-Martín', 'Peter Stone']
- **Abstrat**: Reinforcement learning (RL), particularly its combination with deep neural networks referred to as deep RL (DRL), has shown tremendous promise across a wide range of applications, suggesting its potential for enabling the development of sophisticated robotic behaviors. Robotics problems, however, pose fundamental difficulties for the application of RL, stemming from the complexity and cost of interacting with the physical world. This article provides a modern survey of DRL for robotics, with a particular focus on evaluating the real-world successes achieved with DRL in realizing several key robotic competencies. Our analysis aims to identify the key factors underlying those exciting successes, reveal underexplored areas, and provide an overall characterization of the status of DRL in robotics. We highlight several important avenues for future work, emphasizing the need for stable and sample-efficient real-world RL paradigms, holistic approaches for discovering and integrating various competencies to tackle complex long-horizon, open-world tasks, and principled development and evaluation procedures. This survey is designed to offer insights for both RL practitioners and roboticists toward harnessing RL's power to create generally capable real-world robotic systems.





## Deep Reinforcement Learning for Autonomous Cyber Operations: A Survey
- **Url**: http://arxiv.org/abs/2310.07745v2
- **Authors**: ['Gregory Palmer', 'Chris Parry', 'Daniel J. B. Harrold', 'Chris Willis']
- **Abstrat**: The rapid increase in the number of cyber-attacks in recent years raises the need for principled methods for defending networks against malicious actors. Deep reinforcement learning (DRL) has emerged as a promising approach for mitigating these attacks. However, while DRL has shown much potential for cyber defence, numerous challenges must be overcome before DRL can be applied to autonomous cyber operations (ACO) at scale. Principled methods are required for environments that confront learners with very high-dimensional state spaces, large multi-discrete action spaces, and adversarial learning. Recent works have reported success in solving these problems individually. There have also been impressive engineering efforts towards solving all three for real-time strategy games. However, applying DRL to the full ACO problem remains an open challenge. Here, we survey the relevant DRL literature and conceptualize an idealised ACO-DRL agent. We provide: i.) A summary of the domain properties that define the ACO problem; ii.) A comprehensive comparison of current ACO environments used for benchmarking DRL approaches; iii.) An overview of state-of-the-art approaches for scaling DRL to domains that confront learners with the curse of dimensionality, and; iv.) A survey and critique of current methods for limiting the exploitability of agents within adversarial settings from the perspective of ACO. We conclude with open research questions that we hope will motivate future directions for researchers and practitioners working on ACO.





## Instigating Cooperation among LLM Agents Using Adaptive Information Modulation
- **Url**: http://arxiv.org/abs/2409.10372v1
- **Authors**: ['Qiliang Chen', 'Alireza', 'Ilami', 'Nunzio Lore', 'Babak Heydari']
- **Abstrat**: This paper introduces a novel framework combining LLM agents as proxies for human strategic behavior with reinforcement learning (RL) to engage these agents in evolving strategic interactions within team environments. Our approach extends traditional agent-based simulations by using strategic LLM agents (SLA) and introducing dynamic and adaptive governance through a pro-social promoting RL agent (PPA) that modulates information access across agents in a network, optimizing social welfare and promoting pro-social behavior. Through validation in iterative games, including the prisoner dilemma, we demonstrate that SLA agents exhibit nuanced strategic adaptations. The PPA agent effectively learns to adjust information transparency, resulting in enhanced cooperation rates. This framework offers significant insights into AI-mediated social dynamics, contributing to the deployment of AI in real-world team settings.





## A Scalable and Parallelizable Digital Twin Framework for Sustainable Sim2Real Transition of Multi-Agent Reinforcement Learning Systems
- **Url**: http://arxiv.org/abs/2403.10996v2
- **Authors**: ['Chinmay Vilas Samak', 'Tanmay Vilas Samak', 'Venkat Krovi']
- **Abstrat**: Multi-agent reinforcement learning (MARL) systems usually require significantly long training times due to their inherent complexity. Furthermore, deploying them in the real world demands a feature-rich environment along with multiple embodied agents, which may not be feasible due to budget or space limitations, not to mention energy consumption and safety issues. This work tries to address these pain points by presenting a sustainable digital twin framework capable of accelerating MARL training by selectively scaling parallelized workloads on-demand, and transferring the trained policies from simulation to reality using minimal hardware resources. The applicability of the proposed digital twin framework is highlighted through two representative use cases, which cover cooperative as well as competitive classes of MARL problems. We study the effect of agent and environment parallelization on training time and that of systematic domain randomization on zero-shot sim2real transfer across both the case studies. Results indicate up to 76.3% reduction in training time with the proposed parallelization scheme and as low as 2.9% sim2real gap using the suggested deployment method.





## Catch It! Learning to Catch in Flight with Mobile Dexterous Hands
- **Url**: http://arxiv.org/abs/2409.10319v1
- **Authors**: ['Yuanhang Zhang', 'Tianhai Liang', 'Zhenyang Chen', 'Yanjie Ze', 'Huazhe Xu']
- **Abstrat**: Catching objects in flight (i.e., thrown objects) is a common daily skill for humans, yet it presents a significant challenge for robots. This task requires a robot with agile and accurate motion, a large spatial workspace, and the ability to interact with diverse objects. In this paper, we build a mobile manipulator composed of a mobile base, a 6-DoF arm, and a 12-DoF dexterous hand to tackle such a challenging task. We propose a two-stage reinforcement learning framework to efficiently train a whole-body-control catching policy for this high-DoF system in simulation. The objects' throwing configurations, shapes, and sizes are randomized during training to enhance policy adaptivity to various trajectories and object characteristics in flight. The results show that our trained policy catches diverse objects with randomly thrown trajectories, at a high success rate of about 80\% in simulation, with a significant improvement over the baselines. The policy trained in simulation can be directly deployed in the real world with onboard sensing and computation, which achieves catching sandbags in various shapes, randomly thrown by humans. Our project page is available at https://mobile-dex-catch.github.io/.





## ReflectDiffu: Reflect between Emotion-intent Contagion and Mimicry for Empathetic Response Generation via a RL-Diffusion Framework
- **Url**: http://arxiv.org/abs/2409.10289v1
- **Authors**: ['Jiahao Yuan', 'Zixiang Di', 'Zhiqing Cui', 'Guisong Yang', 'Usman Naseem']
- **Abstrat**: Empathetic response generation necessitates the integration of emotional and intentional dynamics to foster meaningful interactions. Existing research either neglects the intricate interplay between emotion and intent, leading to suboptimal controllability of empathy, or resorts to large language models (LLMs), which incur significant computational overhead. In this paper, we introduce ReflectDiffu, a lightweight and comprehensive framework for empathetic response generation. This framework incorporates emotion contagion to augment emotional expressiveness and employs an emotion-reasoning mask to pinpoint critical emotional elements. Additionally, it integrates intent mimicry within reinforcement learning for refinement during diffusion. By harnessing an intent twice reflect the mechanism of Exploring-Sampling-Correcting, ReflectDiffu adeptly translates emotional decision-making into precise intent actions, thereby addressing empathetic response misalignments stemming from emotional misrecognition. Through reflection, the framework maps emotional states to intents, markedly enhancing both response empathy and flexibility. Comprehensive experiments reveal that ReflectDiffu outperforms existing models regarding relevance, controllability, and informativeness, achieving state-of-the-art results in both automatic and human evaluations.





## The Role of Deep Learning Regularizations on Actors in Offline RL
- **Url**: http://arxiv.org/abs/2409.07606v2
- **Authors**: ['Denis Tarasov', 'Anja Surina', 'Caglar Gulcehre']
- **Abstrat**: Deep learning regularization techniques, such as dropout, layer normalization, or weight decay, are widely adopted in the construction of modern artificial neural networks, often resulting in more robust training processes and improved generalization capabilities. However, in the domain of Reinforcement Learning (RL), the application of these techniques has been limited, usually applied to value function estimators, and may result in detrimental effects. This issue is even more pronounced in offline RL settings, which bear greater similarity to supervised learning but have received less attention. Recent work in continuous offline RL has demonstrated that while we can build sufficiently powerful critic networks, the generalization of actor networks remains a bottleneck. In this study, we empirically show that applying standard regularization techniques to actor networks in offline RL actor-critic algorithms yields improvements of 6% on average across two algorithms and three different continuous D4RL domains.





## Safety-Oriented Pruning and Interpretation of Reinforcement Learning Policies
- **Url**: http://arxiv.org/abs/2409.10218v1
- **Authors**: ['Dennis Gross', 'Helge Spieker']
- **Abstrat**: Pruning neural networks (NNs) can streamline them but risks removing vital parameters from safe reinforcement learning (RL) policies. We introduce an interpretable RL method called VERINTER, which combines NN pruning with model checking to ensure interpretable RL safety. VERINTER exactly quantifies the effects of pruning and the impact of neural connections on complex safety properties by analyzing changes in safety measurements. This method maintains safety in pruned RL policies and enhances understanding of their safety dynamics, which has proven effective in multiple RL settings.





## Enhancing RL Safety with Counterfactual LLM Reasoning
- **Url**: http://arxiv.org/abs/2409.10188v1
- **Authors**: ['Dennis Gross', 'Helge Spieker']
- **Abstrat**: Reinforcement learning (RL) policies may exhibit unsafe behavior and are hard to explain. We use counterfactual large language model reasoning to enhance RL policy safety post-training. We show that our approach improves and helps to explain the RL policy safety.





## Safe and Stable Closed-Loop Learning for Neural-Network-Supported Model Predictive Control
- **Url**: http://arxiv.org/abs/2409.10171v1
- **Authors**: ['Sebastian Hirt', 'Maik Pfefferkorn', 'Rolf Findeisen']
- **Abstrat**: Safe learning of control policies remains challenging, both in optimal control and reinforcement learning. In this article, we consider safe learning of parametrized predictive controllers that operate with incomplete information about the underlying process. To this end, we employ Bayesian optimization for learning the best parameters from closed-loop data. Our method focuses on the system's overall long-term performance in closed-loop while keeping it safe and stable. Specifically, we parametrize the stage cost function of an MPC using a feedforward neural network. This allows for a high degree of flexibility, enabling the system to achieve a better closed-loop performance with respect to a superordinate measure. However, this flexibility also necessitates safety measures, especially with respect to closed-loop stability. To this end, we explicitly incorporated stability information in the Bayesian-optimization-based learning procedure, thereby achieving rigorous probabilistic safety guarantees. The proposed approach is illustrated using a numeric example.





## Quantile Regression for Distributional Reward Models in RLHF
- **Url**: http://arxiv.org/abs/2409.10164v1
- **Authors**: ['Nicolai Dorka']
- **Abstrat**: Reinforcement learning from human feedback (RLHF) has become a key method for aligning large language models (LLMs) with human preferences through the use of reward models. However, traditional reward models typically generate point estimates, which oversimplify the diversity and complexity of human values and preferences. In this paper, we introduce Quantile Reward Models (QRMs), a novel approach to reward modeling that learns a distribution over rewards instead of a single scalar value. Our method uses quantile regression to estimate a full, potentially multimodal distribution over preferences, providing a more powerful and nuanced representation of preferences. This distributional approach can better capture the diversity of human values, addresses label noise, and accommodates conflicting preferences by modeling them as distinct modes in the distribution. Our experimental results show that QRM outperforms comparable traditional point-estimate models on RewardBench. Furthermore, we demonstrate that the additional information provided by the distributional estimates can be utilized in downstream applications, such as risk-aware reinforcement learning, resulting in LLM policies that generate fewer extremely negative responses. Our code and model are released at https://github.com/Nicolinho/QRM.





## Universal Plans: One Action Sequence to Solve Them All!
- **Url**: http://arxiv.org/abs/2407.02090v2
- **Authors**: ['Kalle G. Timperi', 'Alexander J. LaValle', 'Steven M. LaValle']
- **Abstrat**: This paper introduces the notion of a universal plan, which when executed, is guaranteed to solve all planning problems in a category, regardless of the obstacles, initial state, and goal set. Such plans are specified as a deterministic sequence of actions that are blindly applied without any sensor feedback. Thus, they can be considered as pure exploration in a reinforcement learning context, and we show that with basic memory requirements, they even yield optimal plans. Building upon results in number theory and theory of automata, we provide universal plans both for discrete and continuous (motion) planning and prove their (semi)completeness. The concepts are applied and illustrated through simulation studies, and several directions for future research are sketched.





## DRIFT: Deep Reinforcement Learning for Intelligent Floating Platforms Trajectories
- **Url**: http://arxiv.org/abs/2310.04266v2
- **Authors**: ['Matteo El-Hariry', 'Antoine Richard', 'Vivek Muralidharan', 'Matthieu Geist', 'Miguel Olivares-Mendez']
- **Abstrat**: This investigation introduces a novel deep reinforcement learning-based suite to control floating platforms in both simulated and real-world environments. Floating platforms serve as versatile test-beds to emulate micro-gravity environments on Earth, useful to test autonomous navigation systems for space applications. Our approach addresses the system and environmental uncertainties in controlling such platforms by training policies capable of precise maneuvers amid dynamic and unpredictable conditions. Leveraging Deep Reinforcement Learning (DRL) techniques, our suite achieves robustness, adaptability, and good transferability from simulation to reality. Our deep reinforcement learning framework provides advantages such as fast training times, large-scale testing capabilities, rich visualization options, and ROS bindings for integration with real-world robotic systems. Being open access, our suite serves as a comprehensive platform for practitioners who want to replicate similar research in their own simulated environments and labs.





## Robust Reinforcement Learning with Dynamic Distortion Risk Measures
- **Url**: http://arxiv.org/abs/2409.10096v1
- **Authors**: ['Anthony Coache', 'Sebastian Jaimungal']
- **Abstrat**: In a reinforcement learning (RL) setting, the agent's optimal strategy heavily depends on her risk preferences and the underlying model dynamics of the training environment. These two aspects influence the agent's ability to make well-informed and time-consistent decisions when facing testing environments. In this work, we devise a framework to solve robust risk-aware RL problems where we simultaneously account for environmental uncertainty and risk with a class of dynamic robust distortion risk measures. Robustness is introduced by considering all models within a Wasserstein ball around a reference model. We estimate such dynamic robust risk measures using neural networks by making use of strictly consistent scoring functions, derive policy gradient formulae using the quantile representation of distortion risk measures, and construct an actor-critic algorithm to solve this class of robust risk-aware RL problems. We demonstrate the performance of our algorithm on a portfolio allocation example.





# TD3
# Prioritized Experience Replay
# path planning
## Optimal Geodesic Curvature Constrained Dubins' Path on Sphere with Free Terminal Orientation
- **Url**: http://arxiv.org/abs/2409.10363v1
- **Authors**: ['Deepak Prakash Kumar', 'Swaroop Darbha', 'Satyanarayana Gupta Manyam', 'Dzung Tran', 'David W. Casbeer']
- **Abstrat**: In this paper, motion planning for a vehicle moving on a unit sphere with unit speed is considered, wherein the desired terminal location is fixed, but the terminal orientation is free. The motion of the vehicle is modeled to be constrained by a maximum geodesic curvature $U_{max},$ which controls the rate of change of heading of the vehicle such that the maximum heading change occurs when the vehicle travels on a tight circular arc of radius $r = \frac{1}{\sqrt{1 + U_{max}^2}}$. Using Pontryagin's Minimum Principle, the main result of this paper shows that for $r \leq \frac{1}{2}$, the optimal path connecting a given initial configuration and a final location on the sphere belongs to a set of at most seven paths. The candidate paths are of type $CG, CC,$ and degenerate paths of the same, where $C \in \{L, R\}$ denotes a tight left or right turn, respectively, and $G$ denotes a great circular arc.





## Safety-critical Locomotion of Biped Robots in Infeasible Paths: Overcoming Obstacles during Navigation toward Destination
- **Url**: http://arxiv.org/abs/2409.10274v1
- **Authors**: ['Jaemin Lee', 'Min Dai', 'Jeeseop Kim', 'Aaron D. Ames']
- **Abstrat**: This paper proposes a safety-critical locomotion control framework employed for legged robots exploring through infeasible path in obstacle-rich environments. Our research focus is on achieving safe and robust locomotion where robots confront unavoidable obstacles en route to their designated destination. Through the utilization of outcomes from physical interactions with unknown objects, we establish a hierarchy among the safety-critical conditions avoiding the obstacles. This hierarchy enables the generation of a safe reference trajectory that adeptly mitigates conflicts among safety conditions and reduce the risk while controlling the robot toward its destination without additional motion planning methods. In addition, robust bipedal locomotion is achieved by utilizing the Hybrid Linear Inverted Pendulum model, coupled with a disturbance observer addressing a disturbance from the physical interaction.





## RaceMOP: Mapless Online Path Planning for Multi-Agent Autonomous Racing using Residual Policy Learning
- **Url**: http://arxiv.org/abs/2403.07129v2
- **Authors**: ['Raphael Trumpp', 'Ehsan Javanmardi', 'Jin Nakazato', 'Manabu Tsukada', 'Marco Caccamo']
- **Abstrat**: The interactive decision-making in multi-agent autonomous racing offers insights valuable beyond the domain of self-driving cars. Mapless online path planning is particularly of practical appeal but poses a challenge for safely overtaking opponents due to the limited planning horizon. To address this, we introduce RaceMOP, a novel method for mapless online path planning designed for multi-agent racing of F1TENTH cars. Unlike classical planners that rely on predefined racing lines, RaceMOP operates without a map, utilizing only local observations to execute high-speed overtaking maneuvers. Our approach combines an artificial potential field method as a base policy with residual policy learning to enable long-horizon planning. We advance the field by introducing a novel approach for policy fusion with the residual policy directly in probability space. Extensive experiments on twelve simulated racetracks validate that RaceMOP is capable of long-horizon decision-making with robust collision avoidance during overtaking maneuvers. RaceMOP demonstrates superior handling over existing mapless planners and generalizes to unknown racetracks, affirming its potential for broader applications in robotics. Our code is available at http://github.com/raphajaner/racemop.





## NEUSIS: A Compositional Neuro-Symbolic Framework for Autonomous Perception, Reasoning, and Planning in Complex UAV Search Missions
- **Url**: http://arxiv.org/abs/2409.10196v1
- **Authors**: ['Zhixi Cai', 'Cristian Rojas Cardenas', 'Kevin Leo', 'Chenyuan Zhang', 'Kal Backman', 'Hanbing Li', 'Boying Li', 'Mahsa Ghorbanali', 'Stavya Datta', 'Lizhen Qu', 'Julian Gutierrez Santiago', 'Alexey Ignatiev', 'Yuan-Fang Li', 'Mor Vered', 'Peter J Stuckey', 'Maria Garcia de la Banda', 'Hamid Rezatofighi']
- **Abstrat**: This paper addresses the problem of autonomous UAV search missions, where a UAV must locate specific Entities of Interest (EOIs) within a time limit, based on brief descriptions in large, hazard-prone environments with keep-out zones. The UAV must perceive, reason, and make decisions with limited and uncertain information. We propose NEUSIS, a compositional neuro-symbolic system designed for interpretable UAV search and navigation in realistic scenarios. NEUSIS integrates neuro-symbolic visual perception, reasoning, and grounding (GRiD) to process raw sensory inputs, maintains a probabilistic world model for environment representation, and uses a hierarchical planning component (SNaC) for efficient path planning. Experimental results from simulated urban search missions using AirSim and Unreal Engine show that NEUSIS outperforms a state-of-the-art (SOTA) vision-language model and a SOTA search planning model in success rate, search efficiency, and 3D localization. These results demonstrate the effectiveness of our compositional neuro-symbolic approach in handling complex, real-world scenarios, making it a promising solution for autonomous UAV systems in search missions.




