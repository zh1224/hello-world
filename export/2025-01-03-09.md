# reinforcement learning
## In-Trajectory Inverse Reinforcement Learning: Learn Incrementally Before An Ongoing Trajectory Terminates
- **Url**: http://arxiv.org/abs/2410.15612v3
- **Authors**: ['Shicheng Liu', 'Minghui Zhu']
- **Abstrat**: Inverse reinforcement learning (IRL) aims to learn a reward function and a corresponding policy that best fit the demonstrated trajectories of an expert. However, current IRL works cannot learn incrementally from an ongoing trajectory because they have to wait to collect at least one complete trajectory to learn. To bridge the gap, this paper considers the problem of learning a reward function and a corresponding policy while observing the initial state-action pair of an ongoing trajectory and keeping updating the learned reward and policy when new state-action pairs of the ongoing trajectory are observed. We formulate this problem as an online bi-level optimization problem where the upper level dynamically adjusts the learned reward according to the newly observed state-action pairs with the help of a meta-regularization term, and the lower level learns the corresponding policy. We propose a novel algorithm to solve this problem and guarantee that the algorithm achieves sub-linear local regret $O(\sqrt{T}+\log T+\sqrt{T}\log T)$. If the reward function is linear, we prove that the proposed algorithm achieves sub-linear regret $O(\log T)$. Experiments are used to validate the proposed algorithm.





## AutoSAM: Towards Automatic Sampling of User Behaviors for Sequential Recommender Systems
- **Url**: http://arxiv.org/abs/2311.00388v3
- **Authors**: ['Hao Zhang', 'Mingyue Cheng', 'Qi Liu', 'Zhiding Liu', 'Junzhe Jiang', 'Enhong Chen']
- **Abstrat**: Sequential recommender systems (SRS) have gained widespread popularity in recommendation due to their ability to effectively capture dynamic user preferences. One default setting in the current SRS is to uniformly consider each historical behavior as a positive interaction. Actually, this setting has the potential to yield sub-optimal performance, as each item makes a distinct contribution to the user's interest. For example, purchased items should be given more importance than clicked ones. Hence, we propose a general automatic sampling framework, named AutoSAM, to non-uniformly treat historical behaviors. Specifically, AutoSAM augments the standard sequential recommendation architecture with an additional sampler layer to adaptively learn the skew distribution of the raw input, and then sample informative sub-sets to build more generalizable SRS. To overcome the challenges of non-differentiable sampling actions and also introduce multiple decision factors for sampling, we further introduce a novel reinforcement learning based method to guide the training of the sampler. We theoretically design multi-objective sampling rewards including Future Prediction and Sequence Perplexity, and then optimize the whole framework in an end-to-end manner by combining the policy gradient. We conduct extensive experiments on benchmark recommender models and four real-world datasets. The experimental results demonstrate the effectiveness of the proposed approach. We will make our code publicly available after the acceptance.





## Relativistic VQE calculations of molecular electric dipole moments on trapped ion quantum hardware
- **Url**: http://arxiv.org/abs/2406.04992v3
- **Authors**: ['Palak Chawla', 'Shweta', 'K. R. Swain', 'Tushti Patel', 'Renu Bala', 'Disha Shetty', 'Kenji Sugisaki', 'Sudhindu Bikash Mandal', 'Jordi Riu', 'Jan Nogue', 'V. S. Prasannaa', 'B. P. Das']
- **Abstrat**: The quantum-classical hybrid variational quantum eigensolver (VQE) algorithm is among the most actively studied topics in atomic and molecular calculations on quantum computers, yet few studies address properties other than energies or account for relativistic effects. This work presents high-precision 18-qubit relativistic VQE simulations for calculating the permanent electric dipole moments (PDMs) of BeH to RaH molecules on traditional computers, and 6- and 12-qubit PDM computations for SrH on IonQ quantum devices. To achieve high precision on current noisy intermediate scale era quantum hardware, we apply various resource reduction methods, including Reinforcement Learning and causal flow preserving ZX-Calculus routines, along with error mitigation and post-selection techniques. Our approach reduces the two-qubit gate count in our 12-qubit circuit by 99.71%, with only a 2.35% trade-off in precision for PDM when evaluated classically within a suitably chosen active space. On the current generation IonQ Forte-I hardware, the error in PDM is -1.17% relative to classical calculations and only 1.21% compared to the unoptimized circuit.





## Multi-Agent Quantum Reinforcement Learning using Evolutionary Optimization
- **Url**: http://arxiv.org/abs/2311.05546v4
- **Authors**: ['Michael Kölle', 'Felix Topp', 'Thomy Phan', 'Philipp Altmann', 'Jonas Nüßlein', 'Claudia Linnhoff-Popien']
- **Abstrat**: Multi-Agent Reinforcement Learning is becoming increasingly more important in times of autonomous driving and other smart industrial applications. Simultaneously a promising new approach to Reinforcement Learning arises using the inherent properties of quantum mechanics, reducing the trainable parameters of a model significantly. However, gradient-based Multi-Agent Quantum Reinforcement Learning methods often have to struggle with barren plateaus, holding them back from matching the performance of classical approaches. While gradient free Quantum Reinforcement Learning methods may alleviate some of these challenges, they too are not immune to the difficulties posed by barren plateaus. We build upon an existing approach for gradient free Quantum Reinforcement Learning and propose three genetic variations with Variational Quantum Circuits for Multi-Agent Reinforcement Learning using evolutionary optimization. We evaluate our genetic variations in the Coin Game environment and also compare them to classical approaches. We showed that our Variational Quantum Circuit approaches perform significantly better compared to a neural network with a similar amount of trainable parameters. Compared to the larger neural network, our approaches archive similar results using $97.88\%$ less parameters.





## Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents
- **Url**: http://arxiv.org/abs/2407.01887v3
- **Authors**: ['Fanzeng Xia', 'Hao Liu', 'Yisong Yue', 'Tongxin Li']
- **Abstrat**: In-context reinforcement learning (ICRL) is a frontier paradigm for solving reinforcement learning problems in the foundation model era. While ICRL capabilities have been demonstrated in transformers through task-specific training, the potential of Large Language Models (LLMs) out-of-the-box remains largely unexplored. Recent findings highlight that LLMs often face challenges when dealing with numerical contexts, and limited attention has been paid to evaluating their performance through preference feedback generated by the environment. This paper is the first to investigate LLMs as in-context decision-makers under the problem of Dueling Bandits (DB), a stateless preference-based reinforcement learning setting that extends the classic Multi-Armed Bandit (MAB) model by querying for preference feedback. We compare GPT-3.5 Turbo, GPT-4, GPT-4 Turbo, Llama 3.1, and o1-Preview against nine well-established DB algorithms. Our results reveal that our top-performing LLM, GPT-4 Turbo, has the zero-shot relative decision-making ability to achieve surprisingly low weak regret across all the DB environment instances by quickly including the best arm in duels. However, an optimality gap exists between LLMs and classic DB algorithms in terms of strong regret. LLMs struggle to converge and consistently exploit even when explicitly prompted to do so, and are sensitive to prompt variations. To bridge this gap, we propose an agentic flow framework: LLM with Enhanced Algorithmic Dueling (LEAD), which integrates off-the-shelf DB algorithms with LLM agents through fine-grained adaptive interplay. We show that LEAD has theoretical guarantees inherited from classic DB algorithms on both weak and strong regret. We validate its efficacy and robustness even with noisy and adversarial prompts. The design of our framework sheds light on how to enhance the trustworthiness of LLMs used for in-context decision-making.





## Optimality theory of stigmergic collective information processing by chemotactic cells
- **Url**: http://arxiv.org/abs/2407.15298v2
- **Authors**: ['Masaki Kato', 'Tetsuya J. Kobayashi']
- **Abstrat**: Collective information processing is fundamental in various biological systems, where the cooperation of multiple cells results in complex functions beyond individual capabilities. A distinctive example is collective exploration where chemotactic cells not only sense the gradient of guiding exogeneous cues originating from targets but also generate and modulate endogenous cues to coordinate their collective behaviors. While the optimality of gradient sensing has been studied extensively in the context of single-cell information processing, the optimality of collective information processing that includes both gradient sensing and gradient generation remains underexplored. In this study, we formulate the collective exploration problem as a reinforcement learning (RL) by a population. Based on RL theory, we derive the optimal exploration dynamics of agents and identify their structural correspondence with the Keller-Segel model, the established phenomenological model of collective cellular dynamics. Our theory identifies an optimal coupling relation between gradient sensing and gradient generation and demonstrates that the optimal way to generate a gradient qualitatively differs depending on whether the gradient sensing is logarithmic or linear. The underlying RL structure is leveraged to compare the derived collective dynamics with single-agent searching dynamics, showing that distributed information processing by population enables a fraction of agents to reach the target robustly. Our formulation provides a foundation for understanding the collective information processing mediated by dynamic sensing and modulation of cues.





## Baichuan4-Finance Technical Report
- **Url**: http://arxiv.org/abs/2412.15270v2
- **Authors**: ['Hanyu Zhang', 'Boyu Qiu', 'Yuhao Feng', 'Shuqi Li', 'Qian Ma', 'Xiyuan Zhang', 'Qiang Ju', 'Dong Yan', 'Jian Xie']
- **Abstrat**: Large language models (LLMs) have demonstrated strong capabilities in language understanding, generation, and reasoning, yet their potential in finance remains underexplored due to the complexity and specialization of financial knowledge. In this work, we report the development of the Baichuan4-Finance series, including a comprehensive suite of foundational Baichuan4-Finance-Base and an aligned language model Baichuan4-Finance, which are built upon Baichuan4-Turbo base model and tailored for finance domain. Firstly, we have dedicated significant effort to building a detailed pipeline for improving data quality. Moreover, in the continual pre-training phase, we propose a novel domain self-constraint training strategy, which enables Baichuan4-Finance-Base to acquire financial knowledge without losing general capabilities. After Supervised Fine-tuning and Reinforcement Learning from Human Feedback and AI Feedback, the chat model Baichuan4-Finance is able to tackle various financial certification questions and real-world scenario applications. We evaluate Baichuan4-Finance on many widely used general datasets and two holistic financial benchmarks. The evaluation results show that Baichuan4-Finance-Base surpasses almost all competitive baselines on financial tasks by significant margins without sacrificing performance on general LLM benchmarks. At the same time, Baichuan4-Finance demonstrates even more impressive performance on financial application scenarios, showcasing its potential to foster community innovation in the financial LLM field.





## FALCON: Feedback-driven Adaptive Long/short-term memory reinforced Coding Optimization system
- **Url**: http://arxiv.org/abs/2410.21349v3
- **Authors**: ['Zeyuan Li', 'Yangfan He', 'Lewei He', 'Jianhui Wang', 'Tianyu Shi', 'Bin Lei', 'Yuchen Li', 'Qiuwu Chen']
- **Abstrat**: Recently, large language models (LLMs) have achieved significant progress in automated code generation. Despite their strong instruction-following capabilities, these models frequently struggled to align with user intent in coding scenarios. In particular, they were hampered by datasets that lacked diversity and failed to address specialized tasks or edge cases. Furthermore, challenges in supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) led to failures in generating precise, human-intent-aligned code. To tackle these challenges and improve the code generation performance for automated programming systems, we propose Feedback-driven Adaptive Long/short-term memory reinforced Coding Optimization (i.e., FALCON). FALCON is structured into two hierarchical levels. From the global level, long-term memory improves code quality by retaining and applying learned knowledge. At the local level, short-term memory allows for the incorporation of immediate feedback from compilers and AI systems. Additionally, we introduce meta-reinforcement learning with feedback rewards to solve the global-local bi-level optimization problem and enhance the model's adaptability across diverse code generation tasks. Extensive experiments demonstrate that our technique achieves state-of-the-art performance, leading other reinforcement learning methods by more than 4.5 percentage points on the MBPP benchmark and 6.1 percentage points on the Humaneval benchmark. The open-sourced code is publicly available at https://github.com/titurte/FALCON.





## Discounted-Sum Automata with Multiple Discount Factors
- **Url**: http://arxiv.org/abs/2307.08780v2
- **Authors**: ['Udi Boker', 'Guy Hefetz']
- **Abstrat**: Discounting the influence of future events is a key paradigm in economics and it is widely used in computer-science models, such as games, Markov decision processes (MDPs), reinforcement learning, and automata. While a single game or MDP may allow for several different discount factors, discounted-sum automata (NDAs) were only studied with respect to a single discount factor. It is known that every class of NDAs with an integer as the discount factor has good computational properties: It is closed under determinization and under the algebraic operations min, max, addition, and subtraction, and there are algorithms for its basic decision problems, such as automata equivalence and containment. Extending the integer discount factor to an arbitrary rational number, loses most of these good properties.   We define and analyze nondeterministic discounted-sum automata in which each transition can have a different integral discount factor (integral NMDAs). We show that integral NMDAs with an arbitrary choice of discount factors are not closed under determinization and under algebraic operations and that their containment problem is undecidable. We then define and analyze a restricted class of integral NMDAs, which we call tidy NMDAs, in which the choice of discount factors depends on the prefix of the word read so far. Among their special cases are NMDAs that correlate discount factors to actions (alphabet letters) or to the elapsed time. We show that for every function $\theta$ that defines the choice of discount factors, the class of $\theta$-NMDAs enjoys all of the above good properties of NDAs with a single integral discount factor, as well as the same complexity of the required decision problems. Tidy NMDAs are also as expressive as deterministic integral NMDAs with an arbitrary choice of discount factors.





## Enhancing Code LLMs with Reinforcement Learning in Code Generation: A Survey
- **Url**: http://arxiv.org/abs/2412.20367v2
- **Authors**: ['Junqiao Wang', 'Zeng Zhang', 'Yangfan He', 'Yuyang Song', 'Tianyu Shi', 'Yuchen Li', 'Hengyuan Xu', 'Kunyu Wu', 'Guangwu Qian', 'Qiuwu Chen', 'Lewei He']
- **Abstrat**: With the rapid evolution of large language models (LLM), reinforcement learning (RL) has emerged as a pivotal technique for code generation and optimization in various domains. This paper presents a systematic survey of the application of RL in code optimization and generation, highlighting its role in enhancing compiler optimization, resource allocation, and the development of frameworks and tools. Subsequent sections first delve into the intricate processes of compiler optimization, where RL algorithms are leveraged to improve efficiency and resource utilization. The discussion then progresses to the function of RL in resource allocation, emphasizing register allocation and system optimization. We also explore the burgeoning role of frameworks and tools in code generation, examining how RL can be integrated to bolster their capabilities. This survey aims to serve as a comprehensive resource for researchers and practitioners interested in harnessing the power of RL to advance code generation and optimization techniques.





## A survey of Monte Carlo methods for noisy and costly densities with application to reinforcement learning and ABC
- **Url**: http://arxiv.org/abs/2108.00490v3
- **Authors**: ['F. Llorente', 'L. Martino', 'J. Read', 'D. Delgado']
- **Abstrat**: This survey gives an overview of Monte Carlo methodologies using surrogate models, for dealing with densities which are intractable, costly, and/or noisy. This type of problem can be found in numerous real-world scenarios, including stochastic optimization and reinforcement learning, where each evaluation of a density function may incur some computationally-expensive or even physical (real-world activity) cost, likely to give different results each time. The surrogate model does not incur this cost, but there are important trade-offs and considerations involved in the choice and design of such methodologies. We classify the different methodologies into three main classes and describe specific instances of algorithms under a unified notation. A modular scheme which encompasses the considered methods is also presented. A range of application scenarios is discussed, with special attention to the likelihood-free setting and reinforcement learning. Several numerical comparisons are also provided.





# TD3
# Prioritized Experience Replay
# path planning