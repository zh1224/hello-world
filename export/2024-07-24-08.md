# reinforcement
## A Simulation Benchmark for Autonomous Racing with Large-Scale Human Data
- **Url**: http://arxiv.org/abs/2407.16680v1
- **Authors**: ['Adrian Remonda', 'Nicklas Hansen', 'Ayoub Raji', 'Nicola Musiu', 'Marko Bertogna', 'Eduardo Veas', 'Xiaolong Wang']
- **Abstrat**: Despite the availability of international prize-money competitions, scaled vehicles, and simulation environments, research on autonomous racing and the control of sports cars operating close to the limit of handling has been limited by the high costs of vehicle acquisition and management, as well as the limited physics accuracy of open-source simulators. In this paper, we propose a racing simulation platform based on the simulator Assetto Corsa to test, validate, and benchmark autonomous driving algorithms, including reinforcement learning (RL) and classical Model Predictive Control (MPC), in realistic and challenging scenarios. Our contributions include the development of this simulation platform, several state-of-the-art algorithms tailored to the racing environment, and a comprehensive dataset collected from human drivers. Additionally, we evaluate algorithms in the offline RL setting. All the necessary code (including environment and benchmarks), working examples, datasets, and videos are publicly released and can be found at: \url{https://assetto-corsa-gym.github.io}.





## From Imitation to Refinement -- Residual RL for Precise Visual Assembly
- **Url**: http://arxiv.org/abs/2407.16677v1
- **Authors**: ['Lars Ankile', 'Anthony Simeonov', 'Idan Shenfeld', 'Marcel Torne', 'Pulkit Agrawal']
- **Abstrat**: Behavior cloning (BC) currently stands as a dominant paradigm for learning real-world visual manipulation. However, in tasks that require locally corrective behaviors like multi-part assembly, learning robust policies purely from human demonstrations remains challenging. Reinforcement learning (RL) can mitigate these limitations by allowing policies to acquire locally corrective behaviors through task reward supervision and exploration. This paper explores the use of RL fine-tuning to improve upon BC-trained policies in precise manipulation tasks. We analyze and overcome technical challenges associated with using RL to directly train policy networks that incorporate modern architectural components like diffusion models and action chunking. We propose training residual policies on top of frozen BC-trained diffusion models using standard policy gradient methods and sparse rewards, an approach we call ResiP (Residual for Precise manipulation). Our experimental results demonstrate that this residual learning framework can significantly improve success rates beyond the base BC-trained models in high-precision assembly tasks by learning corrective actions. We also show that by combining ResiP with teacher-student distillation and visual domain randomization, our method can enable learning real-world policies for robotic assembly directly from RGB images. Find videos and code at \url{https://residual-assembly.github.io}.





## Efficient Discovery of Actual Causality using Abstraction-Refinement
- **Url**: http://arxiv.org/abs/2407.16629v1
- **Authors**: ['Arshia Rafieioskouei', 'Borzoo Bonakdarpour']
- **Abstrat**: Causality is an influence by which one event contributes to the production of another event, where the cause is partly responsible for the effect, and the effect is partly dependent on the cause. In this paper, we propose a novel and effective method to formally reason about the causal effect of events in engineered systems, with application on finding the root-cause of safety violations in embedded and cyber-physical systems. We are motivated by the notion of actual causality by Halpern and Pearl, which focuses on the causal effect of particular events, rather than type-level causality, which attempts to make general statements about scientific and natural phenomena. Our first contribution is formulating discovery of actual causality in computing systems modeled by a transition systems as an SMT solving problem. Since datasets for causality analysis tend to be large, in order to tackle the scalability problem of automated formal reasoning, our second contribution is a novel technique based on abstraction-refinement that allows identifying actual causes within smaller abstract causal models. We demonstrate the effectiveness of our approach (by several orders of magnitude) using three case studies to find the actual cause of violations of safety in (1) a neural network controller for an mountain car, (2) a controller for a lunar lander obtained by reinforcement learning, and (3) an MPC controller for an F-16 autopilot simulator.





## Recurrent Action Transformer with Memory
- **Url**: http://arxiv.org/abs/2306.09459v4
- **Authors**: ['Egor Cherepanov', 'Alexey Staroverov', 'Dmitry Yudin', 'Alexey K. Kovalev', 'Aleksandr I. Panov']
- **Abstrat**: Recently, the use of transformers in offline reinforcement learning has become a rapidly developing area. This is due to their ability to treat the agent's trajectory in the environment as a sequence, thereby reducing the policy learning problem to sequence modeling. In environments where the agent's decisions depend on past events (POMDPs), capturing both the event itself and the decision point in the context of the model is essential. However, the quadratic complexity of the attention mechanism limits the potential for context expansion. One solution to this problem is to enhance transformers with memory mechanisms. This paper proposes a Recurrent Action Transformer with Memory (RATE), a novel model architecture incorporating a recurrent memory mechanism designed to regulate information retention. To evaluate our model, we conducted extensive experiments on memory-intensive environments (ViZDoom-Two-Colors, T-Maze, Memory Maze, Minigrid.Memory), classic Atari games and MuJoCo control environments. The results show that using memory can significantly improve performance in memory-intensive environments while maintaining or improving results in classic environments. We hope our findings will stimulate research on memory mechanisms for transformers applicable to offline reinforcement learning.





## Functional Acceleration for Policy Mirror Descent
- **Url**: http://arxiv.org/abs/2407.16602v1
- **Authors**: ['Veronica Chelu', 'Doina Precup']
- **Abstrat**: We apply functional acceleration to the Policy Mirror Descent (PMD) general family of algorithms, which cover a wide range of novel and fundamental methods in Reinforcement Learning (RL). Leveraging duality, we propose a momentum-based PMD update. By taking the functional route, our approach is independent of the policy parametrization and applicable to large-scale optimization, covering previous applications of momentum at the level of policy parameters as a special case. We theoretically analyze several properties of this approach and complement with a numerical ablation study, which serves to illustrate the policy optimization dynamics on the value polytope, relative to different algorithmic design choices in this space. We further characterize numerically several features of the problem setting relevant for functional acceleration, and lastly, we investigate the impact of approximation on their learning mechanics.





## Real-Time Interactions Between Human Controllers and Remote Devices in Metaverse
- **Url**: http://arxiv.org/abs/2407.16591v1
- **Authors**: ['Kan Chen', 'Zhen Meng', 'Xiangmin Xu', 'Changyang She', 'Philip G. Zhao']
- **Abstrat**: Supporting real-time interactions between human controllers and remote devices remains a challenging goal in the Metaverse due to the stringent requirements on computing workload, communication throughput, and round-trip latency. In this paper, we establish a novel framework for real-time interactions through the virtual models in the Metaverse. Specifically, we jointly predict the motion of the human controller for 1) proactive rendering in the Metaverse and 2) generating control commands to the real-world remote device in advance. The virtual model is decoupled into two components for rendering and control, respectively. To dynamically adjust the prediction horizons for rendering and control, we develop a two-step human-in-the-loop continuous reinforcement learning approach and use an expert policy to improve the training efficiency. An experimental prototype is built to verify our algorithm with different communication latencies. Compared with the baseline policy without prediction, our proposed method can reduce 1) the Motion-To-Photon (MTP) latency between human motion and rendering feedback and 2) the root mean squared error (RMSE) between human motion and real-world remote devices significantly.





## TLCR: Token-Level Continuous Reward for Fine-grained Reinforcement Learning from Human Feedback
- **Url**: http://arxiv.org/abs/2407.16574v1
- **Authors**: ['Eunseop Yoon', 'Hee Suk Yoon', 'SooHwan Eom', 'Gunsoo Han', 'Daniel Wontae Nam', 'Daejin Jo', 'Kyoung-Woon On', 'Mark A. Hasegawa-Johnson', 'Sungwoong Kim', 'Chang D. Yoo']
- **Abstrat**: Reinforcement Learning from Human Feedback (RLHF) leverages human preference data to train language models to align more closely with human essence. These human preference data, however, are labeled at the sequence level, creating a mismatch between sequence-level preference labels and tokens, which are autoregressively generated from the language model. Although several recent approaches have tried to provide token-level (i.e., dense) rewards for each individual token, these typically rely on predefined discrete reward values (e.g., positive: +1, negative: -1, neutral: 0), failing to account for varying degrees of preference inherent to each token. To address this limitation, we introduce TLCR (Token-Level Continuous Reward) for RLHF, which incorporates a discriminator trained to distinguish positive and negative tokens, and the confidence of the discriminator is used to assign continuous rewards to each token considering the context. Extensive experiments show that our proposed TLCR leads to consistent performance improvements over previous sequence-level or token-level discrete rewards on open-ended generation benchmarks.





## First-order ANIL provably learns representations despite overparametrization
- **Url**: http://arxiv.org/abs/2303.01335v3
- **Authors**: ['Oğuz Kaan Yüksel', 'Etienne Boursier', 'Nicolas Flammarion']
- **Abstrat**: Due to its empirical success in few-shot classification and reinforcement learning, meta-learning has recently received significant interest. Meta-learning methods leverage data from previous tasks to learn a new task in a sample-efficient manner. In particular, model-agnostic methods look for initialization points from which gradient descent quickly adapts to any new task. Although it has been empirically suggested that such methods perform well by learning shared representations during pretraining, there is limited theoretical evidence of such behavior. More importantly, it has not been shown that these methods still learn a shared structure, despite architectural misspecifications. In this direction, this work shows, in the limit of an infinite number of tasks, that first-order ANIL with a linear two-layer network architecture successfully learns linear shared representations. This result even holds with overparametrization; having a width larger than the dimension of the shared representations results in an asymptotically low-rank solution. The learned solution then yields a good adaptation performance on any new task after a single gradient step. Overall, this illustrates how well model-agnostic methods such as first-order ANIL can learn shared representations.





## Solving a Real-World Optimization Problem Using Proximal Policy Optimization with Curriculum Learning and Reward Engineering
- **Url**: http://arxiv.org/abs/2404.02577v2
- **Authors**: ['Abhijeet Pendyala', 'Asma Atamna', 'Tobias Glasmachers']
- **Abstrat**: We present a proximal policy optimization (PPO) agent trained through curriculum learning (CL) principles and meticulous reward engineering to optimize a real-world high-throughput waste sorting facility. Our work addresses the challenge of effectively balancing the competing objectives of operational safety, volume optimization, and minimizing resource usage. A vanilla agent trained from scratch on these multiple criteria fails to solve the problem due to its inherent complexities. This problem is particularly difficult due to the environment's extremely delayed rewards with long time horizons and class (or action) imbalance, with important actions being infrequent in the optimal policy. This forces the agent to anticipate long-term action consequences and prioritize rare but rewarding behaviours, creating a non-trivial reinforcement learning task. Our five-stage CL approach tackles these challenges by gradually increasing the complexity of the environmental dynamics during policy transfer while simultaneously refining the reward mechanism. This iterative and adaptable process enables the agent to learn a desired optimal policy. Results demonstrate that our approach significantly improves inference-time safety, achieving near-zero safety violations in addition to enhancing waste sorting plant efficiency.





## Cross Anything: General Quadruped Robot Navigation through Complex Terrains
- **Url**: http://arxiv.org/abs/2407.16412v1
- **Authors**: ['Shaoting Zhu', 'Derun Li', 'Yong Liu', 'Ningyi Xu', 'Hang Zhao']
- **Abstrat**: The application of vision-language models (VLMs) has achieved impressive success in various robotics tasks, but there are few explorations for foundation models used in quadruped robot navigation. We introduce Cross Anything System (CAS), an innovative system composed of a high-level reasoning module and a low-level control policy, enabling the robot to navigate across complex 3D terrains and reach the goal position. For high-level reasoning and motion planning, we propose a novel algorithmic system taking advantage of a VLM, with a design of task decomposition and a closed-loop sub-task execution mechanism. For low-level locomotion control, we utilize the Probability Annealing Selection (PAS) method to train a control policy by reinforcement learning. Numerous experiments show that our whole system can accurately and robustly navigate across complex 3D terrains, and its strong generalization ability ensures the applications in diverse indoor and outdoor scenarios and terrains. Project page: https://cross-anything.github.io/





## Evaluating Uncertainties in Electricity Markets via Machine Learning and Quantum Computing
- **Url**: http://arxiv.org/abs/2407.16404v1
- **Authors**: ['Shuyang Zhu', 'Ziqing Zhu', 'Linghua Zhu', 'Yujian Ye', 'Siqi Bu', 'Sasa Z. Djokic']
- **Abstrat**: The analysis of decision-making process in electricity markets is crucial for understanding and resolving issues related to market manipulation and reduced social welfare. Traditional Multi-Agent Reinforcement Learning (MARL) method can model decision-making of generation companies (GENCOs), but faces challenges due to uncertainties in policy functions, reward functions, and inter-agent interactions. Quantum computing offers a promising solution to resolve these uncertainties, and this paper introduces the Quantum Multi-Agent Deep Q-Network (Q-MADQN) method, which integrates variational quantum circuits into the traditional MARL framework. The main contributions of the paper are: identifying the correspondence between market uncertainties and quantum properties, proposing the Q-MADQN algorithm for simulating electricity market bidding, and demonstrating that Q-MADQN allows for a more thorough exploration and simulates more potential bidding strategies of profit-oriented GENCOs, compared to conventional methods, without compromising computational efficiency. The proposed method is illustrated on IEEE 30-bus test network, confirming that it offers a more accurate model for simulating complex market dynamics.





## Reinforcement Learning-based Adaptive Mitigation of Uncorrected DRAM Errors in the Field
- **Url**: http://arxiv.org/abs/2407.16377v1
- **Authors**: ['Isaac Boixaderas', 'Sergi Moré', 'Javier Bartolome', 'David Vicente', 'Petar Radojković', 'Paul M. Carpenter', 'Eduard Ayguadé']
- **Abstrat**: Scaling to larger systems, with current levels of reliability, requires cost-effective methods to mitigate hardware failures. One of the main causes of hardware failure is an uncorrected error in memory, which terminates the current job and wastes all computation since the last checkpoint. This paper presents the first adaptive method for triggering uncorrected error mitigation. It uses a prediction approach that considers the likelihood of an uncorrected error and its current potential cost. The method is based on reinforcement learning, and the only user-defined parameters are the mitigation cost and whether the job can be restarted from a mitigation point. We evaluate our method using classical machine learning metrics together with a cost-benefit analysis, which compares the cost of mitigation actions with the benefits from mitigating some of the errors. On two years of production logs from the MareNostrum supercomputer, our method reduces lost compute time by 54% compared with no mitigation and is just 6% below the optimal Oracle method. All source code is open source.





## Arbitrary quantum states preparation aided by deep reinforcement learning
- **Url**: http://arxiv.org/abs/2407.16368v1
- **Authors**: ['Zhao-Wei Wang', 'Zhao-Ming Wang']
- **Abstrat**: The preparation of quantum states is essential in the realm of quantum information processing, and the development of efficient methodologies can significantly alleviate the strain on quantum resources. Within the framework of deep reinforcement learning (DRL), we integrate the initial and the target state information within the state preparation task together, so as to realize the control trajectory design between two arbitrary quantum states. Utilizing a semiconductor double quantum dots (DQDs) model, our results demonstrate that the resulting control trajectories can effectively achieve arbitrary quantum state preparation (AQSP) for both single-qubit and two-qubit systems, with average fidelities of 0.9868 and 0.9556 for the test sets, respectively. Furthermore, we consider the noise around the system and the control trajectories exhibit commendable robustness against charge and nuclear noise. Our study not only substantiates the efficacy of DRL in QSP, but also provides a new solution for quantum control tasks of multi-initial and multi-objective states, and is expected to be extended to a wider range of quantum control problems.





## MOMAland: A Set of Benchmarks for Multi-Objective Multi-Agent Reinforcement Learning
- **Url**: http://arxiv.org/abs/2407.16312v1
- **Authors**: ['Florian Felten', 'Umut Ucak', 'Hicham Azmani', 'Gao Peng', 'Willem Röpke', 'Hendrik Baier', 'Patrick Mannion', 'Diederik M. Roijers', 'Jordan K. Terry', 'El-Ghazali Talbi', 'Grégoire Danoy', 'Ann Nowé', 'Roxana Rădulescu']
- **Abstrat**: Many challenging tasks such as managing traffic systems, electricity grids, or supply chains involve complex decision-making processes that must balance multiple conflicting objectives and coordinate the actions of various independent decision-makers (DMs). One perspective for formalising and addressing such tasks is multi-objective multi-agent reinforcement learning (MOMARL). MOMARL broadens reinforcement learning (RL) to problems with multiple agents each needing to consider multiple objectives in their learning process. In reinforcement learning research, benchmarks are crucial in facilitating progress, evaluation, and reproducibility. The significance of benchmarks is underscored by the existence of numerous benchmark frameworks developed for various RL paradigms, including single-agent RL (e.g., Gymnasium), multi-agent RL (e.g., PettingZoo), and single-agent multi-objective RL (e.g., MO-Gymnasium). To support the advancement of the MOMARL field, we introduce MOMAland, the first collection of standardised environments for multi-objective multi-agent reinforcement learning. MOMAland addresses the need for comprehensive benchmarking in this emerging field, offering over 10 diverse environments that vary in the number of agents, state representations, reward structures, and utility considerations. To provide strong baselines for future research, MOMAland also includes algorithms capable of learning policies in such settings.





## Automated Security Response through Online Learning with Adaptive Conjectures
- **Url**: http://arxiv.org/abs/2402.12499v2
- **Authors**: ['Kim Hammar', 'Tao Li', 'Rolf Stadler', 'Quanyan Zhu']
- **Abstrat**: We study automated security response for an IT infrastructure and formulate the interaction between an attacker and a defender as a partially observed, non-stationary game. We relax the standard assumption that the game model is correctly specified and consider that each player has a probabilistic conjecture about the model, which may be misspecified in the sense that the true model has probability 0. This formulation allows us to capture uncertainty about the infrastructure and the intents of the players. To learn effective game strategies online, we design a novel method where a player iteratively adapts its conjecture using Bayesian learning and updates its strategy through rollout. We prove that the conjectures converge to best fits, and we provide a bound on the performance improvement that rollout enables with a conjectured model. To characterize the steady state of the game, we propose a variant of the Berk-Nash equilibrium. We present our method through an advanced persistent threat use case. Testbed evaluations show that our method produces effective security strategies that adapt to a changing environment. We also find that our method enables faster convergence than current reinforcement learning techniques.





## Negotiating Control: Neurosymbolic Variable Autonomy
- **Url**: http://arxiv.org/abs/2407.16254v1
- **Authors**: ['Georgios Bakirtzis', 'Manolis Chiou', 'Andreas Theodorou']
- **Abstrat**: Variable autonomy equips a system, such as a robot, with mixed initiatives such that it can adjust its independence level based on the task's complexity and the surrounding environment. Variable autonomy solves two main problems in robotic planning: the first is the problem of humans being unable to keep focus in monitoring and intervening during robotic tasks without appropriate human factor indicators, and the second is achieving mission success in unforeseen and uncertain environments in the face of static reward structures. An open problem in variable autonomy is developing robust methods to dynamically balance autonomy and human intervention in real-time, ensuring optimal performance and safety in unpredictable and evolving environments. We posit that addressing unpredictable and evolving environments through an addition of rule-based symbolic logic has the potential to make autonomy adjustments more contextually reliable and adding feedback to reinforcement learning through data from mixed-initiative control further increases efficacy and safety of autonomous behaviour.





## ODGR: Online Dynamic Goal Recognition
- **Url**: http://arxiv.org/abs/2407.16220v1
- **Authors**: ['Matan Shamir', 'Osher Elhadad', 'Matthew E. Taylor', 'Reuth Mirsky']
- **Abstrat**: Traditionally, Reinforcement Learning (RL) problems are aimed at optimization of the behavior of an agent. This paper proposes a novel take on RL, which is used to learn the policy of another agent, to allow real-time recognition of that agent's goals. Goal Recognition (GR) has traditionally been framed as a planning problem where one must recognize an agent's objectives based on its observed actions. Recent approaches have shown how reinforcement learning can be used as part of the GR pipeline, but are limited to recognizing predefined goals and lack scalability in domains with a large goal space. This paper formulates a novel problem, "Online Dynamic Goal Recognition" (ODGR), as a first step to address these limitations. Contributions include introducing the concept of dynamic goals into the standard GR problem definition, revisiting common approaches by reformulating them using ODGR, and demonstrating the feasibility of solving ODGR in a navigation domain using transfer learning. These novel formulations open the door for future extensions of existing transfer learning-based GR methods, which will be robust to changing and expansive real-time environments.




