# reinforcement
## A Simulation Benchmark for Autonomous Racing with Large-Scale Human Data
- **Url**: http://arxiv.org/abs/2407.16680v1
- **Authors**: ['Adrian Remonda', 'Nicklas Hansen', 'Ayoub Raji', 'Nicola Musiu', 'Marko Bertogna', 'Eduardo Veas', 'Xiaolong Wang']
- **Abstrat**: Despite the availability of international prize-money competitions, scaled vehicles, and simulation environments, research on autonomous racing and the control of sports cars operating close to the limit of handling has been limited by the high costs of vehicle acquisition and management, as well as the limited physics accuracy of open-source simulators. In this paper, we propose a racing simulation platform based on the simulator Assetto Corsa to test, validate, and benchmark autonomous driving algorithms, including reinforcement learning (RL) and classical Model Predictive Control (MPC), in realistic and challenging scenarios. Our contributions include the development of this simulation platform, several state-of-the-art algorithms tailored to the racing environment, and a comprehensive dataset collected from human drivers. Additionally, we evaluate algorithms in the offline RL setting. All the necessary code (including environment and benchmarks), working examples, datasets, and videos are publicly released and can be found at: \url{https://assetto-corsa-gym.github.io}.





## From Imitation to Refinement -- Residual RL for Precise Visual Assembly
- **Url**: http://arxiv.org/abs/2407.16677v1
- **Authors**: ['Lars Ankile', 'Anthony Simeonov', 'Idan Shenfeld', 'Marcel Torne', 'Pulkit Agrawal']
- **Abstrat**: Behavior cloning (BC) currently stands as a dominant paradigm for learning real-world visual manipulation. However, in tasks that require locally corrective behaviors like multi-part assembly, learning robust policies purely from human demonstrations remains challenging. Reinforcement learning (RL) can mitigate these limitations by allowing policies to acquire locally corrective behaviors through task reward supervision and exploration. This paper explores the use of RL fine-tuning to improve upon BC-trained policies in precise manipulation tasks. We analyze and overcome technical challenges associated with using RL to directly train policy networks that incorporate modern architectural components like diffusion models and action chunking. We propose training residual policies on top of frozen BC-trained diffusion models using standard policy gradient methods and sparse rewards, an approach we call ResiP (Residual for Precise manipulation). Our experimental results demonstrate that this residual learning framework can significantly improve success rates beyond the base BC-trained models in high-precision assembly tasks by learning corrective actions. We also show that by combining ResiP with teacher-student distillation and visual domain randomization, our method can enable learning real-world policies for robotic assembly directly from RGB images. Find videos and code at \url{https://residual-assembly.github.io}.





## Efficient Discovery of Actual Causality using Abstraction-Refinement
- **Url**: http://arxiv.org/abs/2407.16629v1
- **Authors**: ['Arshia Rafieioskouei', 'Borzoo Bonakdarpour']
- **Abstrat**: Causality is an influence by which one event contributes to the production of another event, where the cause is partly responsible for the effect, and the effect is partly dependent on the cause. In this paper, we propose a novel and effective method to formally reason about the causal effect of events in engineered systems, with application on finding the root-cause of safety violations in embedded and cyber-physical systems. We are motivated by the notion of actual causality by Halpern and Pearl, which focuses on the causal effect of particular events, rather than type-level causality, which attempts to make general statements about scientific and natural phenomena. Our first contribution is formulating discovery of actual causality in computing systems modeled by a transition systems as an SMT solving problem. Since datasets for causality analysis tend to be large, in order to tackle the scalability problem of automated formal reasoning, our second contribution is a novel technique based on abstraction-refinement that allows identifying actual causes within smaller abstract causal models. We demonstrate the effectiveness of our approach (by several orders of magnitude) using three case studies to find the actual cause of violations of safety in (1) a neural network controller for an mountain car, (2) a controller for a lunar lander obtained by reinforcement learning, and (3) an MPC controller for an F-16 autopilot simulator.





## Recurrent Action Transformer with Memory
- **Url**: http://arxiv.org/abs/2306.09459v4
- **Authors**: ['Egor Cherepanov', 'Alexey Staroverov', 'Dmitry Yudin', 'Alexey K. Kovalev', 'Aleksandr I. Panov']
- **Abstrat**: Recently, the use of transformers in offline reinforcement learning has become a rapidly developing area. This is due to their ability to treat the agent's trajectory in the environment as a sequence, thereby reducing the policy learning problem to sequence modeling. In environments where the agent's decisions depend on past events (POMDPs), capturing both the event itself and the decision point in the context of the model is essential. However, the quadratic complexity of the attention mechanism limits the potential for context expansion. One solution to this problem is to enhance transformers with memory mechanisms. This paper proposes a Recurrent Action Transformer with Memory (RATE), a novel model architecture incorporating a recurrent memory mechanism designed to regulate information retention. To evaluate our model, we conducted extensive experiments on memory-intensive environments (ViZDoom-Two-Colors, T-Maze, Memory Maze, Minigrid.Memory), classic Atari games and MuJoCo control environments. The results show that using memory can significantly improve performance in memory-intensive environments while maintaining or improving results in classic environments. We hope our findings will stimulate research on memory mechanisms for transformers applicable to offline reinforcement learning.





## Functional Acceleration for Policy Mirror Descent
- **Url**: http://arxiv.org/abs/2407.16602v1
- **Authors**: ['Veronica Chelu', 'Doina Precup']
- **Abstrat**: We apply functional acceleration to the Policy Mirror Descent (PMD) general family of algorithms, which cover a wide range of novel and fundamental methods in Reinforcement Learning (RL). Leveraging duality, we propose a momentum-based PMD update. By taking the functional route, our approach is independent of the policy parametrization and applicable to large-scale optimization, covering previous applications of momentum at the level of policy parameters as a special case. We theoretically analyze several properties of this approach and complement with a numerical ablation study, which serves to illustrate the policy optimization dynamics on the value polytope, relative to different algorithmic design choices in this space. We further characterize numerically several features of the problem setting relevant for functional acceleration, and lastly, we investigate the impact of approximation on their learning mechanics.





## Real-Time Interactions Between Human Controllers and Remote Devices in Metaverse
- **Url**: http://arxiv.org/abs/2407.16591v1
- **Authors**: ['Kan Chen', 'Zhen Meng', 'Xiangmin Xu', 'Changyang She', 'Philip G. Zhao']
- **Abstrat**: Supporting real-time interactions between human controllers and remote devices remains a challenging goal in the Metaverse due to the stringent requirements on computing workload, communication throughput, and round-trip latency. In this paper, we establish a novel framework for real-time interactions through the virtual models in the Metaverse. Specifically, we jointly predict the motion of the human controller for 1) proactive rendering in the Metaverse and 2) generating control commands to the real-world remote device in advance. The virtual model is decoupled into two components for rendering and control, respectively. To dynamically adjust the prediction horizons for rendering and control, we develop a two-step human-in-the-loop continuous reinforcement learning approach and use an expert policy to improve the training efficiency. An experimental prototype is built to verify our algorithm with different communication latencies. Compared with the baseline policy without prediction, our proposed method can reduce 1) the Motion-To-Photon (MTP) latency between human motion and rendering feedback and 2) the root mean squared error (RMSE) between human motion and real-world remote devices significantly.





## TLCR: Token-Level Continuous Reward for Fine-grained Reinforcement Learning from Human Feedback
- **Url**: http://arxiv.org/abs/2407.16574v1
- **Authors**: ['Eunseop Yoon', 'Hee Suk Yoon', 'SooHwan Eom', 'Gunsoo Han', 'Daniel Wontae Nam', 'Daejin Jo', 'Kyoung-Woon On', 'Mark A. Hasegawa-Johnson', 'Sungwoong Kim', 'Chang D. Yoo']
- **Abstrat**: Reinforcement Learning from Human Feedback (RLHF) leverages human preference data to train language models to align more closely with human essence. These human preference data, however, are labeled at the sequence level, creating a mismatch between sequence-level preference labels and tokens, which are autoregressively generated from the language model. Although several recent approaches have tried to provide token-level (i.e., dense) rewards for each individual token, these typically rely on predefined discrete reward values (e.g., positive: +1, negative: -1, neutral: 0), failing to account for varying degrees of preference inherent to each token. To address this limitation, we introduce TLCR (Token-Level Continuous Reward) for RLHF, which incorporates a discriminator trained to distinguish positive and negative tokens, and the confidence of the discriminator is used to assign continuous rewards to each token considering the context. Extensive experiments show that our proposed TLCR leads to consistent performance improvements over previous sequence-level or token-level discrete rewards on open-ended generation benchmarks.





## First-order ANIL provably learns representations despite overparametrization
- **Url**: http://arxiv.org/abs/2303.01335v3
- **Authors**: ['Oğuz Kaan Yüksel', 'Etienne Boursier', 'Nicolas Flammarion']
- **Abstrat**: Due to its empirical success in few-shot classification and reinforcement learning, meta-learning has recently received significant interest. Meta-learning methods leverage data from previous tasks to learn a new task in a sample-efficient manner. In particular, model-agnostic methods look for initialization points from which gradient descent quickly adapts to any new task. Although it has been empirically suggested that such methods perform well by learning shared representations during pretraining, there is limited theoretical evidence of such behavior. More importantly, it has not been shown that these methods still learn a shared structure, despite architectural misspecifications. In this direction, this work shows, in the limit of an infinite number of tasks, that first-order ANIL with a linear two-layer network architecture successfully learns linear shared representations. This result even holds with overparametrization; having a width larger than the dimension of the shared representations results in an asymptotically low-rank solution. The learned solution then yields a good adaptation performance on any new task after a single gradient step. Overall, this illustrates how well model-agnostic methods such as first-order ANIL can learn shared representations.





## Solving a Real-World Optimization Problem Using Proximal Policy Optimization with Curriculum Learning and Reward Engineering
- **Url**: http://arxiv.org/abs/2404.02577v2
- **Authors**: ['Abhijeet Pendyala', 'Asma Atamna', 'Tobias Glasmachers']
- **Abstrat**: We present a proximal policy optimization (PPO) agent trained through curriculum learning (CL) principles and meticulous reward engineering to optimize a real-world high-throughput waste sorting facility. Our work addresses the challenge of effectively balancing the competing objectives of operational safety, volume optimization, and minimizing resource usage. A vanilla agent trained from scratch on these multiple criteria fails to solve the problem due to its inherent complexities. This problem is particularly difficult due to the environment's extremely delayed rewards with long time horizons and class (or action) imbalance, with important actions being infrequent in the optimal policy. This forces the agent to anticipate long-term action consequences and prioritize rare but rewarding behaviours, creating a non-trivial reinforcement learning task. Our five-stage CL approach tackles these challenges by gradually increasing the complexity of the environmental dynamics during policy transfer while simultaneously refining the reward mechanism. This iterative and adaptable process enables the agent to learn a desired optimal policy. Results demonstrate that our approach significantly improves inference-time safety, achieving near-zero safety violations in addition to enhancing waste sorting plant efficiency.





## Cross Anything: General Quadruped Robot Navigation through Complex Terrains
- **Url**: http://arxiv.org/abs/2407.16412v1
- **Authors**: ['Shaoting Zhu', 'Derun Li', 'Yong Liu', 'Ningyi Xu', 'Hang Zhao']
- **Abstrat**: The application of vision-language models (VLMs) has achieved impressive success in various robotics tasks, but there are few explorations for foundation models used in quadruped robot navigation. We introduce Cross Anything System (CAS), an innovative system composed of a high-level reasoning module and a low-level control policy, enabling the robot to navigate across complex 3D terrains and reach the goal position. For high-level reasoning and motion planning, we propose a novel algorithmic system taking advantage of a VLM, with a design of task decomposition and a closed-loop sub-task execution mechanism. For low-level locomotion control, we utilize the Probability Annealing Selection (PAS) method to train a control policy by reinforcement learning. Numerous experiments show that our whole system can accurately and robustly navigate across complex 3D terrains, and its strong generalization ability ensures the applications in diverse indoor and outdoor scenarios and terrains. Project page: https://cross-anything.github.io/





## Evaluating Uncertainties in Electricity Markets via Machine Learning and Quantum Computing
- **Url**: http://arxiv.org/abs/2407.16404v1
- **Authors**: ['Shuyang Zhu', 'Ziqing Zhu', 'Linghua Zhu', 'Yujian Ye', 'Siqi Bu', 'Sasa Z. Djokic']
- **Abstrat**: The analysis of decision-making process in electricity markets is crucial for understanding and resolving issues related to market manipulation and reduced social welfare. Traditional Multi-Agent Reinforcement Learning (MARL) method can model decision-making of generation companies (GENCOs), but faces challenges due to uncertainties in policy functions, reward functions, and inter-agent interactions. Quantum computing offers a promising solution to resolve these uncertainties, and this paper introduces the Quantum Multi-Agent Deep Q-Network (Q-MADQN) method, which integrates variational quantum circuits into the traditional MARL framework. The main contributions of the paper are: identifying the correspondence between market uncertainties and quantum properties, proposing the Q-MADQN algorithm for simulating electricity market bidding, and demonstrating that Q-MADQN allows for a more thorough exploration and simulates more potential bidding strategies of profit-oriented GENCOs, compared to conventional methods, without compromising computational efficiency. The proposed method is illustrated on IEEE 30-bus test network, confirming that it offers a more accurate model for simulating complex market dynamics.





## Reinforcement Learning-based Adaptive Mitigation of Uncorrected DRAM Errors in the Field
- **Url**: http://arxiv.org/abs/2407.16377v1
- **Authors**: ['Isaac Boixaderas', 'Sergi Moré', 'Javier Bartolome', 'David Vicente', 'Petar Radojković', 'Paul M. Carpenter', 'Eduard Ayguadé']
- **Abstrat**: Scaling to larger systems, with current levels of reliability, requires cost-effective methods to mitigate hardware failures. One of the main causes of hardware failure is an uncorrected error in memory, which terminates the current job and wastes all computation since the last checkpoint. This paper presents the first adaptive method for triggering uncorrected error mitigation. It uses a prediction approach that considers the likelihood of an uncorrected error and its current potential cost. The method is based on reinforcement learning, and the only user-defined parameters are the mitigation cost and whether the job can be restarted from a mitigation point. We evaluate our method using classical machine learning metrics together with a cost-benefit analysis, which compares the cost of mitigation actions with the benefits from mitigating some of the errors. On two years of production logs from the MareNostrum supercomputer, our method reduces lost compute time by 54% compared with no mitigation and is just 6% below the optimal Oracle method. All source code is open source.





## Arbitrary quantum states preparation aided by deep reinforcement learning
- **Url**: http://arxiv.org/abs/2407.16368v1
- **Authors**: ['Zhao-Wei Wang', 'Zhao-Ming Wang']
- **Abstrat**: The preparation of quantum states is essential in the realm of quantum information processing, and the development of efficient methodologies can significantly alleviate the strain on quantum resources. Within the framework of deep reinforcement learning (DRL), we integrate the initial and the target state information within the state preparation task together, so as to realize the control trajectory design between two arbitrary quantum states. Utilizing a semiconductor double quantum dots (DQDs) model, our results demonstrate that the resulting control trajectories can effectively achieve arbitrary quantum state preparation (AQSP) for both single-qubit and two-qubit systems, with average fidelities of 0.9868 and 0.9556 for the test sets, respectively. Furthermore, we consider the noise around the system and the control trajectories exhibit commendable robustness against charge and nuclear noise. Our study not only substantiates the efficacy of DRL in QSP, but also provides a new solution for quantum control tasks of multi-initial and multi-objective states, and is expected to be extended to a wider range of quantum control problems.





## MOMAland: A Set of Benchmarks for Multi-Objective Multi-Agent Reinforcement Learning
- **Url**: http://arxiv.org/abs/2407.16312v1
- **Authors**: ['Florian Felten', 'Umut Ucak', 'Hicham Azmani', 'Gao Peng', 'Willem Röpke', 'Hendrik Baier', 'Patrick Mannion', 'Diederik M. Roijers', 'Jordan K. Terry', 'El-Ghazali Talbi', 'Grégoire Danoy', 'Ann Nowé', 'Roxana Rădulescu']
- **Abstrat**: Many challenging tasks such as managing traffic systems, electricity grids, or supply chains involve complex decision-making processes that must balance multiple conflicting objectives and coordinate the actions of various independent decision-makers (DMs). One perspective for formalising and addressing such tasks is multi-objective multi-agent reinforcement learning (MOMARL). MOMARL broadens reinforcement learning (RL) to problems with multiple agents each needing to consider multiple objectives in their learning process. In reinforcement learning research, benchmarks are crucial in facilitating progress, evaluation, and reproducibility. The significance of benchmarks is underscored by the existence of numerous benchmark frameworks developed for various RL paradigms, including single-agent RL (e.g., Gymnasium), multi-agent RL (e.g., PettingZoo), and single-agent multi-objective RL (e.g., MO-Gymnasium). To support the advancement of the MOMARL field, we introduce MOMAland, the first collection of standardised environments for multi-objective multi-agent reinforcement learning. MOMAland addresses the need for comprehensive benchmarking in this emerging field, offering over 10 diverse environments that vary in the number of agents, state representations, reward structures, and utility considerations. To provide strong baselines for future research, MOMAland also includes algorithms capable of learning policies in such settings.





## Automated Security Response through Online Learning with Adaptive Conjectures
- **Url**: http://arxiv.org/abs/2402.12499v2
- **Authors**: ['Kim Hammar', 'Tao Li', 'Rolf Stadler', 'Quanyan Zhu']
- **Abstrat**: We study automated security response for an IT infrastructure and formulate the interaction between an attacker and a defender as a partially observed, non-stationary game. We relax the standard assumption that the game model is correctly specified and consider that each player has a probabilistic conjecture about the model, which may be misspecified in the sense that the true model has probability 0. This formulation allows us to capture uncertainty about the infrastructure and the intents of the players. To learn effective game strategies online, we design a novel method where a player iteratively adapts its conjecture using Bayesian learning and updates its strategy through rollout. We prove that the conjectures converge to best fits, and we provide a bound on the performance improvement that rollout enables with a conjectured model. To characterize the steady state of the game, we propose a variant of the Berk-Nash equilibrium. We present our method through an advanced persistent threat use case. Testbed evaluations show that our method produces effective security strategies that adapt to a changing environment. We also find that our method enables faster convergence than current reinforcement learning techniques.





## Negotiating Control: Neurosymbolic Variable Autonomy
- **Url**: http://arxiv.org/abs/2407.16254v1
- **Authors**: ['Georgios Bakirtzis', 'Manolis Chiou', 'Andreas Theodorou']
- **Abstrat**: Variable autonomy equips a system, such as a robot, with mixed initiatives such that it can adjust its independence level based on the task's complexity and the surrounding environment. Variable autonomy solves two main problems in robotic planning: the first is the problem of humans being unable to keep focus in monitoring and intervening during robotic tasks without appropriate human factor indicators, and the second is achieving mission success in unforeseen and uncertain environments in the face of static reward structures. An open problem in variable autonomy is developing robust methods to dynamically balance autonomy and human intervention in real-time, ensuring optimal performance and safety in unpredictable and evolving environments. We posit that addressing unpredictable and evolving environments through an addition of rule-based symbolic logic has the potential to make autonomy adjustments more contextually reliable and adding feedback to reinforcement learning through data from mixed-initiative control further increases efficacy and safety of autonomous behaviour.





## ODGR: Online Dynamic Goal Recognition
- **Url**: http://arxiv.org/abs/2407.16220v1
- **Authors**: ['Matan Shamir', 'Osher Elhadad', 'Matthew E. Taylor', 'Reuth Mirsky']
- **Abstrat**: Traditionally, Reinforcement Learning (RL) problems are aimed at optimization of the behavior of an agent. This paper proposes a novel take on RL, which is used to learn the policy of another agent, to allow real-time recognition of that agent's goals. Goal Recognition (GR) has traditionally been framed as a planning problem where one must recognize an agent's objectives based on its observed actions. Recent approaches have shown how reinforcement learning can be used as part of the GR pipeline, but are limited to recognizing predefined goals and lack scalability in domains with a large goal space. This paper formulates a novel problem, "Online Dynamic Goal Recognition" (ODGR), as a first step to address these limitations. Contributions include introducing the concept of dynamic goals into the standard GR problem definition, revisiting common approaches by reformulating them using ODGR, and demonstrating the feasibility of solving ODGR in a navigation domain using transfer learning. These novel formulations open the door for future extensions of existing transfer learning-based GR methods, which will be robust to changing and expansive real-time environments.




# reinforcement learning
## A Simulation Benchmark for Autonomous Racing with Large-Scale Human Data
- **Url**: http://arxiv.org/abs/2407.16680v1
- **Authors**: ['Adrian Remonda', 'Nicklas Hansen', 'Ayoub Raji', 'Nicola Musiu', 'Marko Bertogna', 'Eduardo Veas', 'Xiaolong Wang']
- **Abstrat**: Despite the availability of international prize-money competitions, scaled vehicles, and simulation environments, research on autonomous racing and the control of sports cars operating close to the limit of handling has been limited by the high costs of vehicle acquisition and management, as well as the limited physics accuracy of open-source simulators. In this paper, we propose a racing simulation platform based on the simulator Assetto Corsa to test, validate, and benchmark autonomous driving algorithms, including reinforcement learning (RL) and classical Model Predictive Control (MPC), in realistic and challenging scenarios. Our contributions include the development of this simulation platform, several state-of-the-art algorithms tailored to the racing environment, and a comprehensive dataset collected from human drivers. Additionally, we evaluate algorithms in the offline RL setting. All the necessary code (including environment and benchmarks), working examples, datasets, and videos are publicly released and can be found at: \url{https://assetto-corsa-gym.github.io}.





## From Imitation to Refinement -- Residual RL for Precise Visual Assembly
- **Url**: http://arxiv.org/abs/2407.16677v1
- **Authors**: ['Lars Ankile', 'Anthony Simeonov', 'Idan Shenfeld', 'Marcel Torne', 'Pulkit Agrawal']
- **Abstrat**: Behavior cloning (BC) currently stands as a dominant paradigm for learning real-world visual manipulation. However, in tasks that require locally corrective behaviors like multi-part assembly, learning robust policies purely from human demonstrations remains challenging. Reinforcement learning (RL) can mitigate these limitations by allowing policies to acquire locally corrective behaviors through task reward supervision and exploration. This paper explores the use of RL fine-tuning to improve upon BC-trained policies in precise manipulation tasks. We analyze and overcome technical challenges associated with using RL to directly train policy networks that incorporate modern architectural components like diffusion models and action chunking. We propose training residual policies on top of frozen BC-trained diffusion models using standard policy gradient methods and sparse rewards, an approach we call ResiP (Residual for Precise manipulation). Our experimental results demonstrate that this residual learning framework can significantly improve success rates beyond the base BC-trained models in high-precision assembly tasks by learning corrective actions. We also show that by combining ResiP with teacher-student distillation and visual domain randomization, our method can enable learning real-world policies for robotic assembly directly from RGB images. Find videos and code at \url{https://residual-assembly.github.io}.





## Efficient Discovery of Actual Causality using Abstraction-Refinement
- **Url**: http://arxiv.org/abs/2407.16629v1
- **Authors**: ['Arshia Rafieioskouei', 'Borzoo Bonakdarpour']
- **Abstrat**: Causality is an influence by which one event contributes to the production of another event, where the cause is partly responsible for the effect, and the effect is partly dependent on the cause. In this paper, we propose a novel and effective method to formally reason about the causal effect of events in engineered systems, with application on finding the root-cause of safety violations in embedded and cyber-physical systems. We are motivated by the notion of actual causality by Halpern and Pearl, which focuses on the causal effect of particular events, rather than type-level causality, which attempts to make general statements about scientific and natural phenomena. Our first contribution is formulating discovery of actual causality in computing systems modeled by a transition systems as an SMT solving problem. Since datasets for causality analysis tend to be large, in order to tackle the scalability problem of automated formal reasoning, our second contribution is a novel technique based on abstraction-refinement that allows identifying actual causes within smaller abstract causal models. We demonstrate the effectiveness of our approach (by several orders of magnitude) using three case studies to find the actual cause of violations of safety in (1) a neural network controller for an mountain car, (2) a controller for a lunar lander obtained by reinforcement learning, and (3) an MPC controller for an F-16 autopilot simulator.





## Recurrent Action Transformer with Memory
- **Url**: http://arxiv.org/abs/2306.09459v4
- **Authors**: ['Egor Cherepanov', 'Alexey Staroverov', 'Dmitry Yudin', 'Alexey K. Kovalev', 'Aleksandr I. Panov']
- **Abstrat**: Recently, the use of transformers in offline reinforcement learning has become a rapidly developing area. This is due to their ability to treat the agent's trajectory in the environment as a sequence, thereby reducing the policy learning problem to sequence modeling. In environments where the agent's decisions depend on past events (POMDPs), capturing both the event itself and the decision point in the context of the model is essential. However, the quadratic complexity of the attention mechanism limits the potential for context expansion. One solution to this problem is to enhance transformers with memory mechanisms. This paper proposes a Recurrent Action Transformer with Memory (RATE), a novel model architecture incorporating a recurrent memory mechanism designed to regulate information retention. To evaluate our model, we conducted extensive experiments on memory-intensive environments (ViZDoom-Two-Colors, T-Maze, Memory Maze, Minigrid.Memory), classic Atari games and MuJoCo control environments. The results show that using memory can significantly improve performance in memory-intensive environments while maintaining or improving results in classic environments. We hope our findings will stimulate research on memory mechanisms for transformers applicable to offline reinforcement learning.





## Functional Acceleration for Policy Mirror Descent
- **Url**: http://arxiv.org/abs/2407.16602v1
- **Authors**: ['Veronica Chelu', 'Doina Precup']
- **Abstrat**: We apply functional acceleration to the Policy Mirror Descent (PMD) general family of algorithms, which cover a wide range of novel and fundamental methods in Reinforcement Learning (RL). Leveraging duality, we propose a momentum-based PMD update. By taking the functional route, our approach is independent of the policy parametrization and applicable to large-scale optimization, covering previous applications of momentum at the level of policy parameters as a special case. We theoretically analyze several properties of this approach and complement with a numerical ablation study, which serves to illustrate the policy optimization dynamics on the value polytope, relative to different algorithmic design choices in this space. We further characterize numerically several features of the problem setting relevant for functional acceleration, and lastly, we investigate the impact of approximation on their learning mechanics.





## Real-Time Interactions Between Human Controllers and Remote Devices in Metaverse
- **Url**: http://arxiv.org/abs/2407.16591v1
- **Authors**: ['Kan Chen', 'Zhen Meng', 'Xiangmin Xu', 'Changyang She', 'Philip G. Zhao']
- **Abstrat**: Supporting real-time interactions between human controllers and remote devices remains a challenging goal in the Metaverse due to the stringent requirements on computing workload, communication throughput, and round-trip latency. In this paper, we establish a novel framework for real-time interactions through the virtual models in the Metaverse. Specifically, we jointly predict the motion of the human controller for 1) proactive rendering in the Metaverse and 2) generating control commands to the real-world remote device in advance. The virtual model is decoupled into two components for rendering and control, respectively. To dynamically adjust the prediction horizons for rendering and control, we develop a two-step human-in-the-loop continuous reinforcement learning approach and use an expert policy to improve the training efficiency. An experimental prototype is built to verify our algorithm with different communication latencies. Compared with the baseline policy without prediction, our proposed method can reduce 1) the Motion-To-Photon (MTP) latency between human motion and rendering feedback and 2) the root mean squared error (RMSE) between human motion and real-world remote devices significantly.





## TLCR: Token-Level Continuous Reward for Fine-grained Reinforcement Learning from Human Feedback
- **Url**: http://arxiv.org/abs/2407.16574v1
- **Authors**: ['Eunseop Yoon', 'Hee Suk Yoon', 'SooHwan Eom', 'Gunsoo Han', 'Daniel Wontae Nam', 'Daejin Jo', 'Kyoung-Woon On', 'Mark A. Hasegawa-Johnson', 'Sungwoong Kim', 'Chang D. Yoo']
- **Abstrat**: Reinforcement Learning from Human Feedback (RLHF) leverages human preference data to train language models to align more closely with human essence. These human preference data, however, are labeled at the sequence level, creating a mismatch between sequence-level preference labels and tokens, which are autoregressively generated from the language model. Although several recent approaches have tried to provide token-level (i.e., dense) rewards for each individual token, these typically rely on predefined discrete reward values (e.g., positive: +1, negative: -1, neutral: 0), failing to account for varying degrees of preference inherent to each token. To address this limitation, we introduce TLCR (Token-Level Continuous Reward) for RLHF, which incorporates a discriminator trained to distinguish positive and negative tokens, and the confidence of the discriminator is used to assign continuous rewards to each token considering the context. Extensive experiments show that our proposed TLCR leads to consistent performance improvements over previous sequence-level or token-level discrete rewards on open-ended generation benchmarks.





## First-order ANIL provably learns representations despite overparametrization
- **Url**: http://arxiv.org/abs/2303.01335v3
- **Authors**: ['Oğuz Kaan Yüksel', 'Etienne Boursier', 'Nicolas Flammarion']
- **Abstrat**: Due to its empirical success in few-shot classification and reinforcement learning, meta-learning has recently received significant interest. Meta-learning methods leverage data from previous tasks to learn a new task in a sample-efficient manner. In particular, model-agnostic methods look for initialization points from which gradient descent quickly adapts to any new task. Although it has been empirically suggested that such methods perform well by learning shared representations during pretraining, there is limited theoretical evidence of such behavior. More importantly, it has not been shown that these methods still learn a shared structure, despite architectural misspecifications. In this direction, this work shows, in the limit of an infinite number of tasks, that first-order ANIL with a linear two-layer network architecture successfully learns linear shared representations. This result even holds with overparametrization; having a width larger than the dimension of the shared representations results in an asymptotically low-rank solution. The learned solution then yields a good adaptation performance on any new task after a single gradient step. Overall, this illustrates how well model-agnostic methods such as first-order ANIL can learn shared representations.





## Solving a Real-World Optimization Problem Using Proximal Policy Optimization with Curriculum Learning and Reward Engineering
- **Url**: http://arxiv.org/abs/2404.02577v2
- **Authors**: ['Abhijeet Pendyala', 'Asma Atamna', 'Tobias Glasmachers']
- **Abstrat**: We present a proximal policy optimization (PPO) agent trained through curriculum learning (CL) principles and meticulous reward engineering to optimize a real-world high-throughput waste sorting facility. Our work addresses the challenge of effectively balancing the competing objectives of operational safety, volume optimization, and minimizing resource usage. A vanilla agent trained from scratch on these multiple criteria fails to solve the problem due to its inherent complexities. This problem is particularly difficult due to the environment's extremely delayed rewards with long time horizons and class (or action) imbalance, with important actions being infrequent in the optimal policy. This forces the agent to anticipate long-term action consequences and prioritize rare but rewarding behaviours, creating a non-trivial reinforcement learning task. Our five-stage CL approach tackles these challenges by gradually increasing the complexity of the environmental dynamics during policy transfer while simultaneously refining the reward mechanism. This iterative and adaptable process enables the agent to learn a desired optimal policy. Results demonstrate that our approach significantly improves inference-time safety, achieving near-zero safety violations in addition to enhancing waste sorting plant efficiency.





## Cross Anything: General Quadruped Robot Navigation through Complex Terrains
- **Url**: http://arxiv.org/abs/2407.16412v1
- **Authors**: ['Shaoting Zhu', 'Derun Li', 'Yong Liu', 'Ningyi Xu', 'Hang Zhao']
- **Abstrat**: The application of vision-language models (VLMs) has achieved impressive success in various robotics tasks, but there are few explorations for foundation models used in quadruped robot navigation. We introduce Cross Anything System (CAS), an innovative system composed of a high-level reasoning module and a low-level control policy, enabling the robot to navigate across complex 3D terrains and reach the goal position. For high-level reasoning and motion planning, we propose a novel algorithmic system taking advantage of a VLM, with a design of task decomposition and a closed-loop sub-task execution mechanism. For low-level locomotion control, we utilize the Probability Annealing Selection (PAS) method to train a control policy by reinforcement learning. Numerous experiments show that our whole system can accurately and robustly navigate across complex 3D terrains, and its strong generalization ability ensures the applications in diverse indoor and outdoor scenarios and terrains. Project page: https://cross-anything.github.io/





## Evaluating Uncertainties in Electricity Markets via Machine Learning and Quantum Computing
- **Url**: http://arxiv.org/abs/2407.16404v1
- **Authors**: ['Shuyang Zhu', 'Ziqing Zhu', 'Linghua Zhu', 'Yujian Ye', 'Siqi Bu', 'Sasa Z. Djokic']
- **Abstrat**: The analysis of decision-making process in electricity markets is crucial for understanding and resolving issues related to market manipulation and reduced social welfare. Traditional Multi-Agent Reinforcement Learning (MARL) method can model decision-making of generation companies (GENCOs), but faces challenges due to uncertainties in policy functions, reward functions, and inter-agent interactions. Quantum computing offers a promising solution to resolve these uncertainties, and this paper introduces the Quantum Multi-Agent Deep Q-Network (Q-MADQN) method, which integrates variational quantum circuits into the traditional MARL framework. The main contributions of the paper are: identifying the correspondence between market uncertainties and quantum properties, proposing the Q-MADQN algorithm for simulating electricity market bidding, and demonstrating that Q-MADQN allows for a more thorough exploration and simulates more potential bidding strategies of profit-oriented GENCOs, compared to conventional methods, without compromising computational efficiency. The proposed method is illustrated on IEEE 30-bus test network, confirming that it offers a more accurate model for simulating complex market dynamics.





## Reinforcement Learning-based Adaptive Mitigation of Uncorrected DRAM Errors in the Field
- **Url**: http://arxiv.org/abs/2407.16377v1
- **Authors**: ['Isaac Boixaderas', 'Sergi Moré', 'Javier Bartolome', 'David Vicente', 'Petar Radojković', 'Paul M. Carpenter', 'Eduard Ayguadé']
- **Abstrat**: Scaling to larger systems, with current levels of reliability, requires cost-effective methods to mitigate hardware failures. One of the main causes of hardware failure is an uncorrected error in memory, which terminates the current job and wastes all computation since the last checkpoint. This paper presents the first adaptive method for triggering uncorrected error mitigation. It uses a prediction approach that considers the likelihood of an uncorrected error and its current potential cost. The method is based on reinforcement learning, and the only user-defined parameters are the mitigation cost and whether the job can be restarted from a mitigation point. We evaluate our method using classical machine learning metrics together with a cost-benefit analysis, which compares the cost of mitigation actions with the benefits from mitigating some of the errors. On two years of production logs from the MareNostrum supercomputer, our method reduces lost compute time by 54% compared with no mitigation and is just 6% below the optimal Oracle method. All source code is open source.





## Arbitrary quantum states preparation aided by deep reinforcement learning
- **Url**: http://arxiv.org/abs/2407.16368v1
- **Authors**: ['Zhao-Wei Wang', 'Zhao-Ming Wang']
- **Abstrat**: The preparation of quantum states is essential in the realm of quantum information processing, and the development of efficient methodologies can significantly alleviate the strain on quantum resources. Within the framework of deep reinforcement learning (DRL), we integrate the initial and the target state information within the state preparation task together, so as to realize the control trajectory design between two arbitrary quantum states. Utilizing a semiconductor double quantum dots (DQDs) model, our results demonstrate that the resulting control trajectories can effectively achieve arbitrary quantum state preparation (AQSP) for both single-qubit and two-qubit systems, with average fidelities of 0.9868 and 0.9556 for the test sets, respectively. Furthermore, we consider the noise around the system and the control trajectories exhibit commendable robustness against charge and nuclear noise. Our study not only substantiates the efficacy of DRL in QSP, but also provides a new solution for quantum control tasks of multi-initial and multi-objective states, and is expected to be extended to a wider range of quantum control problems.





## MOMAland: A Set of Benchmarks for Multi-Objective Multi-Agent Reinforcement Learning
- **Url**: http://arxiv.org/abs/2407.16312v1
- **Authors**: ['Florian Felten', 'Umut Ucak', 'Hicham Azmani', 'Gao Peng', 'Willem Röpke', 'Hendrik Baier', 'Patrick Mannion', 'Diederik M. Roijers', 'Jordan K. Terry', 'El-Ghazali Talbi', 'Grégoire Danoy', 'Ann Nowé', 'Roxana Rădulescu']
- **Abstrat**: Many challenging tasks such as managing traffic systems, electricity grids, or supply chains involve complex decision-making processes that must balance multiple conflicting objectives and coordinate the actions of various independent decision-makers (DMs). One perspective for formalising and addressing such tasks is multi-objective multi-agent reinforcement learning (MOMARL). MOMARL broadens reinforcement learning (RL) to problems with multiple agents each needing to consider multiple objectives in their learning process. In reinforcement learning research, benchmarks are crucial in facilitating progress, evaluation, and reproducibility. The significance of benchmarks is underscored by the existence of numerous benchmark frameworks developed for various RL paradigms, including single-agent RL (e.g., Gymnasium), multi-agent RL (e.g., PettingZoo), and single-agent multi-objective RL (e.g., MO-Gymnasium). To support the advancement of the MOMARL field, we introduce MOMAland, the first collection of standardised environments for multi-objective multi-agent reinforcement learning. MOMAland addresses the need for comprehensive benchmarking in this emerging field, offering over 10 diverse environments that vary in the number of agents, state representations, reward structures, and utility considerations. To provide strong baselines for future research, MOMAland also includes algorithms capable of learning policies in such settings.





## Automated Security Response through Online Learning with Adaptive Conjectures
- **Url**: http://arxiv.org/abs/2402.12499v2
- **Authors**: ['Kim Hammar', 'Tao Li', 'Rolf Stadler', 'Quanyan Zhu']
- **Abstrat**: We study automated security response for an IT infrastructure and formulate the interaction between an attacker and a defender as a partially observed, non-stationary game. We relax the standard assumption that the game model is correctly specified and consider that each player has a probabilistic conjecture about the model, which may be misspecified in the sense that the true model has probability 0. This formulation allows us to capture uncertainty about the infrastructure and the intents of the players. To learn effective game strategies online, we design a novel method where a player iteratively adapts its conjecture using Bayesian learning and updates its strategy through rollout. We prove that the conjectures converge to best fits, and we provide a bound on the performance improvement that rollout enables with a conjectured model. To characterize the steady state of the game, we propose a variant of the Berk-Nash equilibrium. We present our method through an advanced persistent threat use case. Testbed evaluations show that our method produces effective security strategies that adapt to a changing environment. We also find that our method enables faster convergence than current reinforcement learning techniques.





## Negotiating Control: Neurosymbolic Variable Autonomy
- **Url**: http://arxiv.org/abs/2407.16254v1
- **Authors**: ['Georgios Bakirtzis', 'Manolis Chiou', 'Andreas Theodorou']
- **Abstrat**: Variable autonomy equips a system, such as a robot, with mixed initiatives such that it can adjust its independence level based on the task's complexity and the surrounding environment. Variable autonomy solves two main problems in robotic planning: the first is the problem of humans being unable to keep focus in monitoring and intervening during robotic tasks without appropriate human factor indicators, and the second is achieving mission success in unforeseen and uncertain environments in the face of static reward structures. An open problem in variable autonomy is developing robust methods to dynamically balance autonomy and human intervention in real-time, ensuring optimal performance and safety in unpredictable and evolving environments. We posit that addressing unpredictable and evolving environments through an addition of rule-based symbolic logic has the potential to make autonomy adjustments more contextually reliable and adding feedback to reinforcement learning through data from mixed-initiative control further increases efficacy and safety of autonomous behaviour.





## ODGR: Online Dynamic Goal Recognition
- **Url**: http://arxiv.org/abs/2407.16220v1
- **Authors**: ['Matan Shamir', 'Osher Elhadad', 'Matthew E. Taylor', 'Reuth Mirsky']
- **Abstrat**: Traditionally, Reinforcement Learning (RL) problems are aimed at optimization of the behavior of an agent. This paper proposes a novel take on RL, which is used to learn the policy of another agent, to allow real-time recognition of that agent's goals. Goal Recognition (GR) has traditionally been framed as a planning problem where one must recognize an agent's objectives based on its observed actions. Recent approaches have shown how reinforcement learning can be used as part of the GR pipeline, but are limited to recognizing predefined goals and lack scalability in domains with a large goal space. This paper formulates a novel problem, "Online Dynamic Goal Recognition" (ODGR), as a first step to address these limitations. Contributions include introducing the concept of dynamic goals into the standard GR problem definition, revisiting common approaches by reformulating them using ODGR, and demonstrating the feasibility of solving ODGR in a navigation domain using transfer learning. These novel formulations open the door for future extensions of existing transfer learning-based GR methods, which will be robust to changing and expansive real-time environments.





# agent
## RedAgent: Red Teaming Large Language Models with Context-aware Autonomous Language Agent
- **Url**: http://arxiv.org/abs/2407.16667v1
- **Authors**: ['Huiyu Xu', 'Wenhui Zhang', 'Zhibo Wang', 'Feng Xiao', 'Rui Zheng', 'Yunhe Feng', 'Zhongjie Ba', 'Kui Ren']
- **Abstrat**: Recently, advanced Large Language Models (LLMs) such as GPT-4 have been integrated into many real-world applications like Code Copilot. These applications have significantly expanded the attack surface of LLMs, exposing them to a variety of threats. Among them, jailbreak attacks that induce toxic responses through jailbreak prompts have raised critical safety concerns. To identify these threats, a growing number of red teaming approaches simulate potential adversarial scenarios by crafting jailbreak prompts to test the target LLM. However, existing red teaming methods do not consider the unique vulnerabilities of LLM in different scenarios, making it difficult to adjust the jailbreak prompts to find context-specific vulnerabilities. Meanwhile, these methods are limited to refining jailbreak templates using a few mutation operations, lacking the automation and scalability to adapt to different scenarios. To enable context-aware and efficient red teaming, we abstract and model existing attacks into a coherent concept called "jailbreak strategy" and propose a multi-agent LLM system named RedAgent that leverages these strategies to generate context-aware jailbreak prompts. By self-reflecting on contextual feedback in an additional memory buffer, RedAgent continuously learns how to leverage these strategies to achieve effective jailbreaks in specific contexts. Extensive experiments demonstrate that our system can jailbreak most black-box LLMs in just five queries, improving the efficiency of existing red teaming methods by two times. Additionally, RedAgent can jailbreak customized LLM applications more efficiently. By generating context-aware jailbreak prompts towards applications on GPTs, we discover 60 severe vulnerabilities of these real-world applications with only two queries per vulnerability. We have reported all found issues and communicated with OpenAI and Meta for bug fixes.





## Dynamic Signals
- **Url**: http://arxiv.org/abs/2407.16648v1
- **Authors**: ['Mark Whitmeyer', 'Cole Williams']
- **Abstrat**: In this paper, we reveal that the signal representation of information introduced by Gentzkow and Kamenica (2017) can be applied profitably to dynamic decision problems. We use this to characterize when one dynamic information structure is more valuable to an agent than another, irrespective of what other dynamic sources of information the agent may possess. Notably, this robust dominance is equivalent to an intuitive dynamic version of Brooks, Frankel, and Kamenica (2022)'s reveal-or-refine condition.





## Velocity Driven Vision: Asynchronous Sensor Fusion Birds Eye View Models for Autonomous Vehicles
- **Url**: http://arxiv.org/abs/2407.16636v1
- **Authors**: ['Seamie Hayes', 'Sushil Sharma', 'Ciarán Eising']
- **Abstrat**: Fusing different sensor modalities can be a difficult task, particularly if they are asynchronous. Asynchronisation may arise due to long processing times or improper synchronisation during calibration, and there must exist a way to still utilise this previous information for the purpose of safe driving, and object detection in ego vehicle/ multi-agent trajectory prediction. Difficulties arise in the fact that the sensor modalities have captured information at different times and also at different positions in space. Therefore, they are not spatially nor temporally aligned. This paper will investigate the challenge of radar and LiDAR sensors being asynchronous relative to the camera sensors, for various time latencies. The spatial alignment will be resolved before lifting into BEV space via the transformation of the radar/LiDAR point clouds into the new ego frame coordinate system. Only after this can we concatenate the radar/LiDAR point cloud and lifted camera features. Temporal alignment will be remedied for radar data only, we will implement a novel method of inferring the future radar point positions using the velocity information. Our approach to resolving the issue of sensor asynchrony yields promising results. We demonstrate velocity information can drastically improve IoU for asynchronous datasets, as for a time latency of 360 milliseconds (ms), IoU improves from 49.54 to 53.63. Additionally, for a time latency of 550ms, the camera+radar (C+R) model outperforms the camera+LiDAR (C+L) model by 0.18 IoU. This is an advancement in utilising the often-neglected radar sensor modality, which is less favoured than LiDAR for autonomous driving purposes.





## Recurrent Action Transformer with Memory
- **Url**: http://arxiv.org/abs/2306.09459v4
- **Authors**: ['Egor Cherepanov', 'Alexey Staroverov', 'Dmitry Yudin', 'Alexey K. Kovalev', 'Aleksandr I. Panov']
- **Abstrat**: Recently, the use of transformers in offline reinforcement learning has become a rapidly developing area. This is due to their ability to treat the agent's trajectory in the environment as a sequence, thereby reducing the policy learning problem to sequence modeling. In environments where the agent's decisions depend on past events (POMDPs), capturing both the event itself and the decision point in the context of the model is essential. However, the quadratic complexity of the attention mechanism limits the potential for context expansion. One solution to this problem is to enhance transformers with memory mechanisms. This paper proposes a Recurrent Action Transformer with Memory (RATE), a novel model architecture incorporating a recurrent memory mechanism designed to regulate information retention. To evaluate our model, we conducted extensive experiments on memory-intensive environments (ViZDoom-Two-Colors, T-Maze, Memory Maze, Minigrid.Memory), classic Atari games and MuJoCo control environments. The results show that using memory can significantly improve performance in memory-intensive environments while maintaining or improving results in classic environments. We hope our findings will stimulate research on memory mechanisms for transformers applicable to offline reinforcement learning.





## PDF: A Probability-Driven Framework for Open World 3D Point Cloud Semantic Segmentation
- **Url**: http://arxiv.org/abs/2404.00979v2
- **Authors**: ['Jinfeng Xu', 'Siyuan Yang', 'Xianzhi Li', 'Yuan Tang', 'Yixue Hao', 'Long Hu', 'Min Chen']
- **Abstrat**: Existing point cloud semantic segmentation networks cannot identify unknown classes and update their knowledge, due to a closed-set and static perspective of the real world, which would induce the intelligent agent to make bad decisions. To address this problem, we propose a Probability-Driven Framework (PDF) for open world semantic segmentation that includes (i) a lightweight U-decoder branch to identify unknown classes by estimating the uncertainties, (ii) a flexible pseudo-labeling scheme to supply geometry features along with probability distribution features of unknown classes by generating pseudo labels, and (iii) an incremental knowledge distillation strategy to incorporate novel classes into the existing knowledge base gradually. Our framework enables the model to behave like human beings, which could recognize unknown objects and incrementally learn them with the corresponding knowledge. Experimental results on the S3DIS and ScanNetv2 datasets demonstrate that the proposed PDF outperforms other methods by a large margin in both important tasks of open world semantic segmentation.





## Alleviating Non-identifiability: a High-fidelity Calibration Objective for Financial Market Simulation with Multivariate Time Series Data
- **Url**: http://arxiv.org/abs/2407.16566v1
- **Authors**: ['Chenkai Wang', 'Junji Ren', 'Ke Tang', 'Peng Yang']
- **Abstrat**: The non-identifiability issue has been frequently reported in the social simulation works, where different parameters of an agent-based simulation model yield indistinguishable simulated time series data under certain discrepancy metrics. This issue largely undermines the simulation fidelity yet lacks dedicated investigations. This paper theoretically analyzes that incorporating multiple time series data features in the model calibration phase can alleviate the non-identifiability exponentially with the increasing number of features. To implement this theoretical finding, a maximization-based aggregation function is applied to existing discrepancy metrics to form a new calibration objective function. For verification, the financial market simulation, a typical and complex social simulation task, is considered. Empirical studies on both synthetic and real market data witness the significant improvements in alleviating the non-identifiability with much higher simulation fidelity of the chosen agent-based simulation model. Importantly, as a model-agnostic method, it achieves the first successful simulation of the high-frequency market at seconds level. Hence, this work is expected to provide not only a rigorous understanding of non-identifiability in social simulation, but a high-fidelity calibration objective function for financial market simulations.





## AutoGPT+P: Affordance-based Task Planning with Large Language Models
- **Url**: http://arxiv.org/abs/2402.10778v2
- **Authors**: ['Timo Birr', 'Christoph Pohl', 'Abdelrahman Younes', 'Tamim Asfour']
- **Abstrat**: Recent advances in task planning leverage Large Language Models (LLMs) to improve generalizability by combining such models with classical planning algorithms to address their inherent limitations in reasoning capabilities. However, these approaches face the challenge of dynamically capturing the initial state of the task planning problem. To alleviate this issue, we propose AutoGPT+P, a system that combines an affordance-based scene representation with a planning system. Affordances encompass the action possibilities of an agent on the environment and objects present in it. Thus, deriving the planning domain from an affordance-based scene representation allows symbolic planning with arbitrary objects. AutoGPT+P leverages this representation to derive and execute a plan for a task specified by the user in natural language. In addition to solving planning tasks under a closed-world assumption, AutoGPT+P can also handle planning with incomplete information, e. g., tasks with missing objects by exploring the scene, suggesting alternatives, or providing a partial plan. The affordance-based scene representation combines object detection with an automatically generated object-affordance-mapping using ChatGPT. The core planning tool extends existing work by automatically correcting semantic and syntactic errors. Our approach achieves a success rate of 98%, surpassing the current 81% success rate of the current state-of-the-art LLM-based planning method SayCan on the SayCan instruction set. Furthermore, we evaluated our approach on our newly created dataset with 150 scenarios covering a wide range of complex tasks with missing objects, achieving a success rate of 79% on our dataset. The dataset and the code are publicly available at https://git.h2t.iar.kit.edu/birr/autogpt-p-standalone.





## HAPFI: History-Aware Planning based on Fused Information
- **Url**: http://arxiv.org/abs/2407.16533v1
- **Authors**: ['Sujin Jeon', 'Suyeon Shin', 'Byoung-Tak Zhang']
- **Abstrat**: Embodied Instruction Following (EIF) is a task of planning a long sequence of sub-goals given high-level natural language instructions, such as "Rinse a slice of lettuce and place on the white table next to the fork". To successfully execute these long-term horizon tasks, we argue that an agent must consider its past, i.e., historical data, when making decisions in each step. Nevertheless, recent approaches in EIF often neglects the knowledge from historical data and also do not effectively utilize information across the modalities. To this end, we propose History-Aware Planning based on Fused Information (HAPFI), effectively leveraging the historical data from diverse modalities that agents collect while interacting with the environment. Specifically, HAPFI integrates multiple modalities, including historical RGB observations, bounding boxes, sub-goals, and high-level instructions, by effectively fusing modalities via our Mutually Attentive Fusion method. Through experiments with diverse comparisons, we show that an agent utilizing historical multi-modal information surpasses all the compared methods that neglect the historical data in terms of action planning capability, enabling the generation of well-informed action plans for the next step. Moreover, we provided qualitative evidence highlighting the significance of leveraging historical multi-modal data, particularly in scenarios where the agent encounters intermediate failures, showcasing its robust re-planning capabilities.





## Equilibrium control theory for Kihlstrom-Mirman preferences in continuous time
- **Url**: http://arxiv.org/abs/2407.16525v1
- **Authors**: ['Luca De Gennaro Aquino', 'Sascha Desmettre', 'Yevhen Havrylenko', 'Mogens Steffensen']
- **Abstrat**: In intertemporal settings, the multiattribute utility theory of Kihlstrom and Mirman suggests the application of a concave transform of the lifetime utility index. This construction, while allowing time and risk attitudes to be separated, leads to dynamically inconsistent preferences. We address this issue in a game-theoretic sense by formalizing an equilibrium control theory for continuous-time Markov processes. In these terms, we describe the equilibrium strategy and value function as the solution of an extended Hamilton-Jacobi-Bellman system of partial differential equations. We verify that (the solution of) this system is a sufficient condition for an equilibrium and examine some of its novel features. A consumption-investment problem for an agent with CRRA-CES utility showcases our approach.





## AMONGAGENTS: Evaluating Large Language Models in the Interactive Text-Based Social Deduction Game
- **Url**: http://arxiv.org/abs/2407.16521v1
- **Authors**: ['Yizhou Chi', 'Lingjun Mao', 'Zineng Tang']
- **Abstrat**: Strategic social deduction games serve as valuable testbeds for evaluating the understanding and inference skills of language models, offering crucial insights into social science, artificial intelligence, and strategic gaming. This paper focuses on creating proxies of human behavior in simulated environments, with \textit{Among Us} utilized as a tool for studying simulated human behavior.   The study introduces a text-based game environment, named AmongAgent, that mirrors the dynamics of \textit{Among Us}. Players act as crew members aboard a spaceship, tasked with identifying impostors who are sabotaging the ship and eliminating the crew. Within this environment, the behavior of simulated language agents is analyzed. The experiments involve diverse game sequences featuring different configurations of Crewmates and Impostor personality archetypes. Our work demonstrates that state-of-the-art large language models (LLMs) can effectively grasp the game rules and make decisions based on the current context. This work aims to promote further exploration of LLMs in goal-oriented games with incomplete information and complex action spaces, as these settings offer valuable opportunities to assess language model performance in socially driven scenarios.





## Cosmic Web Dynamics: Forces and Strains
- **Url**: http://arxiv.org/abs/2407.16489v1
- **Authors**: ['Roi Kugel', 'Rien van de Weygaert']
- **Abstrat**: This study concerns an inventory of the gravitational force and tidal field induced by filaments, walls, cluster nodes and voids on Megaparsec scales and how they assemble and shape the Cosmic Web. The study is based on a N$_{\rm Part}=512^3$ $\Lambda$CDM dark matter only N-body simulation in a (300$h^{-1}$~Mpc)$^3$ box at $z=0$. We invoke the density field NEXUS+ multiscale morphological procedure to assign the appropriate morphological feature to each location. We then determine the contribution by each of the cosmic web components to the local gravitational and tidal forces. We find that filaments are, by far, the dominant dynamical component in the interior of filaments, in the majority of underdense void regions and in all wall regions. The gravitational influence of cluster nodes is limited, and they are only dominant in their immediate vicinity. The force field induced by voids is marked by divergent outflowing patterns, yielding the impression of a segmented volume in which voids push matter towards their boundaries. Voids manifest themselves strongly in the tidal field as a cellular tapestry that is closely linked to the multiscale cosmic web. However, even within the interior of voids, the dynamical influence of the surrounding filaments is stronger than the outward push by voids. Therefore, the dynamics of voids cannot be understood without taking into account the influence of the environment. We conclude that filaments constitute the overpowering gravitational agent of the cosmic web, while voids are responsible for the cosmic web's spatial organisation and hence of its spatial connectivity.





## The Oscars of AI Theater: A Survey on Role-Playing with Language Models
- **Url**: http://arxiv.org/abs/2407.11484v4
- **Authors**: ['Nuo Chen', 'Yang Deng', 'Jia Li']
- **Abstrat**: This survey explores the burgeoning field of role-playing with language models, focusing on their development from early persona-based models to advanced character-driven simulations facilitated by Large Language Models (LLMs). Initially confined to simple persona consistency due to limited model capabilities, role-playing tasks have now expanded to embrace complex character portrayals involving character consistency, behavioral alignment, and overall attractiveness. We provide a comprehensive taxonomy of the critical components in designing these systems, including data, models and alignment, agent architecture and evaluation. This survey not only outlines the current methodologies and challenges, such as managing dynamic personal profiles and achieving high-level persona consistency but also suggests avenues for future research in improving the depth and realism of role-playing applications. The goal is to guide future research by offering a structured overview of current methodologies and identifying potential areas for improvement. Related resources and papers are available at https://github.com/nuochenpku/Awesome-Role-Play-Papers.





## Solving a Real-World Optimization Problem Using Proximal Policy Optimization with Curriculum Learning and Reward Engineering
- **Url**: http://arxiv.org/abs/2404.02577v2
- **Authors**: ['Abhijeet Pendyala', 'Asma Atamna', 'Tobias Glasmachers']
- **Abstrat**: We present a proximal policy optimization (PPO) agent trained through curriculum learning (CL) principles and meticulous reward engineering to optimize a real-world high-throughput waste sorting facility. Our work addresses the challenge of effectively balancing the competing objectives of operational safety, volume optimization, and minimizing resource usage. A vanilla agent trained from scratch on these multiple criteria fails to solve the problem due to its inherent complexities. This problem is particularly difficult due to the environment's extremely delayed rewards with long time horizons and class (or action) imbalance, with important actions being infrequent in the optimal policy. This forces the agent to anticipate long-term action consequences and prioritize rare but rewarding behaviours, creating a non-trivial reinforcement learning task. Our five-stage CL approach tackles these challenges by gradually increasing the complexity of the environmental dynamics during policy transfer while simultaneously refining the reward mechanism. This iterative and adaptable process enables the agent to learn a desired optimal policy. Results demonstrate that our approach significantly improves inference-time safety, achieving near-zero safety violations in addition to enhancing waste sorting plant efficiency.





## Positive pressure matters in acoustic droplet vaporization
- **Url**: http://arxiv.org/abs/2407.16455v1
- **Authors**: ['Samuele Fiorini', 'Anunay Prasanna', 'Gazendra Shakya', 'Marco Cattaneo', 'Outi Supponen']
- **Abstrat**: Acoustically vaporizable droplets are phase-change agents that can improve the effectiveness of ultrasound-based therapies. In this study, we demonstrate that the compression part of an acoustic wave can generate tension that initiates the vaporization. This counter-intuitive process is explained by the occurrence of Gouy phase shift due to the focusing of the acoustic wave inside the droplet. Our analysis unifies the existing theories for acoustic droplet vaporization under a single framework and is supported by experiments and simulations. We use our theory to identify governing parameters that allow to vaporize droplets using predominantly compression waves, which are safer in medical use.





## Sample-Efficient Constrained Reinforcement Learning with General Parameterization
- **Url**: http://arxiv.org/abs/2405.10624v2
- **Authors**: ['Washim Uddin Mondal', 'Vaneet Aggarwal']
- **Abstrat**: We consider a constrained Markov Decision Problem (CMDP) where the goal of an agent is to maximize the expected discounted sum of rewards over an infinite horizon while ensuring that the expected discounted sum of costs exceeds a certain threshold. Building on the idea of momentum-based acceleration, we develop the Primal-Dual Accelerated Natural Policy Gradient (PD-ANPG) algorithm that guarantees an $\epsilon$ global optimality gap and $\epsilon$ constraint violation with $\tilde{\mathcal{O}}(\epsilon^{-2})$ sample complexity for general parameterized policies. This improves the state-of-the-art sample complexity in general parameterized CMDPs by a factor of $\mathcal{O}(\epsilon^{-2})$ and achieves the theoretical lower bound.





## Evaluating Uncertainties in Electricity Markets via Machine Learning and Quantum Computing
- **Url**: http://arxiv.org/abs/2407.16404v1
- **Authors**: ['Shuyang Zhu', 'Ziqing Zhu', 'Linghua Zhu', 'Yujian Ye', 'Siqi Bu', 'Sasa Z. Djokic']
- **Abstrat**: The analysis of decision-making process in electricity markets is crucial for understanding and resolving issues related to market manipulation and reduced social welfare. Traditional Multi-Agent Reinforcement Learning (MARL) method can model decision-making of generation companies (GENCOs), but faces challenges due to uncertainties in policy functions, reward functions, and inter-agent interactions. Quantum computing offers a promising solution to resolve these uncertainties, and this paper introduces the Quantum Multi-Agent Deep Q-Network (Q-MADQN) method, which integrates variational quantum circuits into the traditional MARL framework. The main contributions of the paper are: identifying the correspondence between market uncertainties and quantum properties, proposing the Q-MADQN algorithm for simulating electricity market bidding, and demonstrating that Q-MADQN allows for a more thorough exploration and simulates more potential bidding strategies of profit-oriented GENCOs, compared to conventional methods, without compromising computational efficiency. The proposed method is illustrated on IEEE 30-bus test network, confirming that it offers a more accurate model for simulating complex market dynamics.





## q-deformed evolutionary dynamics in simple matrix games
- **Url**: http://arxiv.org/abs/2407.16380v1
- **Authors**: ['Christopher R. Kitching', 'Tobias Galla']
- **Abstrat**: We consider evolutionary games in which the agent selected for update compares their payoff to q neighbours, rather than a single neighbour as in standard evolutionary game theory. Through studying fixed point stability and fixation times for 2x2 games with all-to-all interactions, we find that the flow changes significantly as a function of q. Further, we investigate the effects of changing the underlying topology from an all-to-all interacting system to an uncorrelated graph via the pair approximation. We also develop the framework for studying games with more than two strategies, such as the rock-paper-scissors game where we show that changing q leads to the emergence of new types of flow.





## Compatibility of Fairness and Nash Welfare under Subadditive Valuations
- **Url**: http://arxiv.org/abs/2407.12461v2
- **Authors**: ['Siddharth Barman', 'Mashbat Suzuki']
- **Abstrat**: We establish a compatibility between fairness and efficiency, captured via Nash Social Welfare (NSW), under the broad class of subadditive valuations. We prove that, for subadditive valuations, there always exists a partial allocation that is envy-free up to the removal of any good (EFx) and has NSW at least half of the optimal; here, optimality is considered across all allocations, fair or otherwise. We also prove, for subadditive valuations, the universal existence of complete allocations that are envy-free up to one good (EF1) and also achieve a factor $1/2$ approximation to the optimal NSW. Our EF1 result resolves an open question posed by Garg et al. (STOC 2023).   In addition, we develop a polynomial-time algorithm which, given an arbitrary allocation \~A as input, returns an EF1 allocation with NSW at least $1/3$ times that of \~A. Therefore, our results imply that the EF1 criterion can be attained simultaneously with a constant-factor approximation to optimal NSW in polynomial time (with demand queries), for subadditive valuations. The previously best-known approximation factor for optimal NSW, under EF1 and among $n$ agents, was $O(n)$ - we improve this bound to $O(1)$.   It is known that EF1 and exact Pareto efficiency (PO) are incompatible with subadditive valuations. Complementary to this negative result, the current work shows that we regain compatibility by just considering a factor $1/2$ approximation: EF1 can be achieved in conjunction with $\frac{1}{2}$-PO under subadditive valuations. As such, our results serve as a general tool that can be used as a black box to convert any efficient outcome into a fair one, with only a marginal decrease in efficiency.





## Nudging Using Autonomous Agents: Risks and Ethical Considerations
- **Url**: http://arxiv.org/abs/2407.16362v1
- **Authors**: ['Vivek Nallur', 'Karen Renaud', 'Aleksei Gudkov']
- **Abstrat**: This position paper briefly discusses nudging, its use by autonomous agents, potential risks and ethical considerations while creating such systems. Instead of taking a normative approach, which guides all situations, the paper proposes a risk-driven questions-and-answer approach. The paper takes the position that this is a pragmatic method, that is transparent about beneficial intentions, foreseeable risks, and mitigations. Given the uncertainty in AI and autonomous agent capabilities, we believe that such pragmatic methods offer a plausibly safe path, without sacrificing flexibility in domain and technology.





## AutoLegend: A User Feedback-Driven Adaptive Legend Generator for Visualizations
- **Url**: http://arxiv.org/abs/2407.16331v1
- **Authors**: ['Can Liu', 'Xiyao Mei', 'Zhibang Jiang', 'Shaocong Tan', 'Xiaoru Yuan']
- **Abstrat**: We propose AutoLegend to generate interactive visualization legends using online learning with user feedback. AutoLegend accurately extracts symbols and channels from visualizations and then generates quality legends. AutoLegend enables a two-way interaction between legends and interactions, including highlighting, filtering, data retrieval, and retargeting. After analyzing visualization legends from IEEE VIS papers over the past 20 years, we summarized the design space and evaluation metrics for legend design in visualizations, particularly charts. The generation process consists of three interrelated components: a legend search agent, a feedback model, and an adversarial loss model. The search agent determines suitable legend solutions by exploring the design space and receives guidance from the feedback model through scalar scores. The feedback model is continuously updated by the adversarial loss model based on user input. The user study revealed that AutoLegend can learn users' preferences through legend editing.




