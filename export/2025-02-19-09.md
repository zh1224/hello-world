# reinforcement learning
## Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct Preference Optimization
- **Url**: http://arxiv.org/abs/2502.13146v1
- **Authors**: ['Shuo Xing', 'Yuping Wang', 'Peiran Li', 'Ruizheng Bai', 'Yueqi Wang', 'Chengxuan Qian', 'Huaxiu Yao', 'Zhengzhong Tu']
- **Abstrat**: The emergence of large Vision Language Models (VLMs) has broadened the scope and capabilities of single-modal Large Language Models (LLMs) by integrating visual modalities, thereby unlocking transformative cross-modal applications in a variety of real-world scenarios. Despite their impressive performance, VLMs are prone to significant hallucinations, particularly in the form of cross-modal inconsistencies. Building on the success of Reinforcement Learning from Human Feedback (RLHF) in aligning LLMs, recent advancements have focused on applying direct preference optimization (DPO) on carefully curated datasets to mitigate these issues. Yet, such approaches typically introduce preference signals in a brute-force manner, neglecting the crucial role of visual information in the alignment process. In this paper, we introduce Re-Align, a novel alignment framework that leverages image retrieval to construct a dual-preference dataset, effectively incorporating both textual and visual preference signals. We further introduce rDPO, an extension of the standard direct preference optimization that incorporates an additional visual preference objective during fine-tuning. Our experimental results demonstrate that Re-Align not only mitigates hallucinations more effectively than previous methods but also yields significant performance gains in general visual question-answering (VQA) tasks. Moreover, we show that Re-Align maintains robustness and scalability across a wide range of VLM sizes and architectures. This work represents a significant step forward in aligning multimodal LLMs, paving the way for more reliable and effective cross-modal applications. We release all the code in https://github.com/taco-group/Re-Align.





## RAD: Training an End-to-End Driving Policy via Large-Scale 3DGS-based Reinforcement Learning
- **Url**: http://arxiv.org/abs/2502.13144v1
- **Authors**: ['Hao Gao', 'Shaoyu Chen', 'Bo Jiang', 'Bencheng Liao', 'Yiang Shi', 'Xiaoyang Guo', 'Yuechuan Pu', 'Haoran Yin', 'Xiangyu Li', 'Xinbang Zhang', 'Ying Zhang', 'Wenyu Liu', 'Qian Zhang', 'Xinggang Wang']
- **Abstrat**: Existing end-to-end autonomous driving (AD) algorithms typically follow the Imitation Learning (IL) paradigm, which faces challenges such as causal confusion and the open-loop gap. In this work, we establish a 3DGS-based closed-loop Reinforcement Learning (RL) training paradigm. By leveraging 3DGS techniques, we construct a photorealistic digital replica of the real physical world, enabling the AD policy to extensively explore the state space and learn to handle out-of-distribution scenarios through large-scale trial and error. To enhance safety, we design specialized rewards that guide the policy to effectively respond to safety-critical events and understand real-world causal relationships. For better alignment with human driving behavior, IL is incorporated into RL training as a regularization term. We introduce a closed-loop evaluation benchmark consisting of diverse, previously unseen 3DGS environments. Compared to IL-based methods, RAD achieves stronger performance in most closed-loop metrics, especially 3x lower collision rate. Abundant closed-loop results are presented at https://hgao-cv.github.io/RAD.





## Theorem Prover as a Judge for Synthetic Data Generation
- **Url**: http://arxiv.org/abs/2502.13137v1
- **Authors**: ['Joshua Ong Jun Leang', 'Giwon Hong', 'Wenda Li', 'Shay B. Cohen']
- **Abstrat**: The demand for synthetic data in mathematical reasoning has increased due to its potential to enhance the mathematical capabilities of large language models (LLMs). However, ensuring the validity of intermediate reasoning steps remains a significant challenge, affecting data quality. While formal verification via theorem provers effectively validates LLM reasoning, the autoformalisation of mathematical proofs remains error-prone. In response, we introduce iterative autoformalisation, an approach that iteratively refines theorem prover formalisation to mitigate errors, thereby increasing the execution rate on the Lean prover from 60% to 87%. Building upon that, we introduce Theorem Prover as a Judge (TP-as-a-Judge), a method that employs theorem prover formalisation to rigorously assess LLM intermediate reasoning, effectively integrating autoformalisation with synthetic data generation. Finally, we present Reinforcement Learning from Theorem Prover Feedback (RLTPF), a framework that replaces human annotation with theorem prover feedback in Reinforcement Learning from Human Feedback (RLHF). Across multiple LLMs, applying TP-as-a-Judge and RLTPF improves benchmarks with only 3,508 samples, achieving 5.56% accuracy gain on Mistral-7B for MultiArith, 6.00% on Llama-2-7B for SVAMP, and 3.55% on Llama-3.1-8B for AQUA.





## Scaling Test-Time Compute Without Verification or RL is Suboptimal
- **Url**: http://arxiv.org/abs/2502.12118v2
- **Authors**: ['Amrith Setlur', 'Nived Rajaraman', 'Sergey Levine', 'Aviral Kumar']
- **Abstrat**: Despite substantial advances in scaling test-time compute, an ongoing debate in the community is how it should be scaled up to enable continued and efficient improvements with scaling. There are largely two approaches: first, distilling successful search or thinking traces; and second, using verification (e.g., 0/1 outcome rewards, reward models, or verifiers) to guide reinforcement learning (RL) and search algorithms. In this paper, we prove that finetuning LLMs with verifier-based (VB) methods based on RL or search is far superior to verifier-free (VF) approaches based on distilling or cloning search traces, given a fixed amount of compute/data budget. Further, we show that as we scale test-time compute (measured as the output token length) and training data, suboptimality of VF methods scales poorly compared to VB when the base pre-trained LLM presents a heterogeneous distribution over correct solution traces (e.g., different lengths, styles, etc.) and admits a non-sharp distribution over rewards on traces sampled from it. We formalize this condition using anti-concentration [Erd\H{o}s, 1945]. This implies a stronger result that VB methods scale better asymptotically, with the performance gap between VB and VF methods widening as test-time budget grows. We corroborate our theory empirically on both didactic and math reasoning problems with 3/8/32B-sized pre-trained LLMs, where we find verification is crucial for scaling test-time compute.





## An Attentive Graph Agent for Topology-Adaptive Cyber Defence
- **Url**: http://arxiv.org/abs/2501.14700v3
- **Authors**: ['Ilya Orson Sandoval', 'Isaac Symes Thompson', 'Vasilios Mavroudis', 'Chris Hicks']
- **Abstrat**: As cyber threats grow increasingly sophisticated, reinforcement learning (RL) is emerging as a promising technique to create intelligent and adaptive cyber defense systems. However, most existing autonomous defensive agents have overlooked the inherent graph structure of computer networks subject to cyber attacks, potentially missing critical information and constraining their adaptability. To overcome these limitations, we developed a custom version of the Cyber Operations Research Gym (CybORG) environment, encoding network state as a directed graph with realistic low-level features. We employ a Graph Attention Network (GAT) architecture to process node, edge, and global features, and adapt its output to be compatible with policy gradient methods in RL. Our GAT-based approach offers key advantages over flattened alternatives: policies that demonstrate resilience to certain types of unexpected dynamic network topology changes, reasonable generalisation to networks of varying sizes within the same structural distribution, and interpretable defensive actions grounded in tangible network properties. We demonstrate that GAT defensive policies can be trained using our low-level directed graph observations, even when unexpected connections arise during simulation. Evaluations across networks of different sizes, but consistent subnetwork structure, show our policies achieve comparable performance to policies trained specifically for each network configuration. Our study contributes to the development of robust cyber defence systems that can better adapt to real-world network security challenges.





## Text2World: Benchmarking Large Language Models for Symbolic World Model Generation
- **Url**: http://arxiv.org/abs/2502.13092v1
- **Authors**: ['Mengkang Hu', 'Tianxing Chen', 'Yude Zou', 'Yuheng Lei', 'Qiguang Chen', 'Ming Li', 'Hongyuan Zhang', 'Wenqi Shao', 'Ping Luo']
- **Abstrat**: Recently, there has been growing interest in leveraging large language models (LLMs) to generate symbolic world models from textual descriptions. Although LLMs have been extensively explored in the context of world modeling, prior studies encountered several challenges, including evaluation randomness, dependence on indirect metrics, and a limited domain scope. To address these limitations, we introduce a novel benchmark, Text2World, based on planning domain definition language (PDDL), featuring hundreds of diverse domains and employing multi-criteria, execution-based metrics for a more robust evaluation. We benchmark current LLMs using Text2World and find that reasoning models trained with large-scale reinforcement learning outperform others. However, even the best-performing model still demonstrates limited capabilities in world modeling. Building on these insights, we examine several promising strategies to enhance the world modeling capabilities of LLMs, including test-time scaling, agent training, and more. We hope that Text2World can serve as a crucial resource, laying the groundwork for future research in leveraging LLMs as world models. The project page is available at https://text-to-world.github.io/.





## Selective Reviews of Bandit Problems in AI via a Statistical View
- **Url**: http://arxiv.org/abs/2412.02251v2
- **Authors**: ['Pengjie Zhou', 'Haoyu Wei', 'Huiming Zhang']
- **Abstrat**: Reinforcement Learning (RL) is a widely researched area in artificial intelligence that focuses on teaching agents decision-making through interactions with their environment. A key subset includes stochastic multi-armed bandit (MAB) and continuum-armed bandit (SCAB) problems, which model sequential decision-making under uncertainty. This review outlines the foundational models and assumptions of bandit problems, explores non-asymptotic theoretical tools like concentration inequalities and minimax regret bounds, and compares frequentist and Bayesian algorithms for managing exploration-exploitation trade-offs. Additionally, we explore K-armed contextual bandits and SCAB, focusing on their methodologies and regret analyses. We also examine the connections between SCAB problems and functional data analysis. Finally, we highlight recent advances and ongoing challenges in the field.





## DeepSeek-V3 Technical Report
- **Url**: http://arxiv.org/abs/2412.19437v2
- **Authors**: ['DeepSeek-AI', 'Aixin Liu', 'Bei Feng', 'Bing Xue', 'Bingxuan Wang', 'Bochao Wu', 'Chengda Lu', 'Chenggang Zhao', 'Chengqi Deng', 'Chenyu Zhang', 'Chong Ruan', 'Damai Dai', 'Daya Guo', 'Dejian Yang', 'Deli Chen', 'Dongjie Ji', 'Erhang Li', 'Fangyun Lin', 'Fucong Dai', 'Fuli Luo', 'Guangbo Hao', 'Guanting Chen', 'Guowei Li', 'H. Zhang', 'Han Bao', 'Hanwei Xu', 'Haocheng Wang', 'Haowei Zhang', 'Honghui Ding', 'Huajian Xin', 'Huazuo Gao', 'Hui Li', 'Hui Qu', 'J. L. Cai', 'Jian Liang', 'Jianzhong Guo', 'Jiaqi Ni', 'Jiashi Li', 'Jiawei Wang', 'Jin Chen', 'Jingchang Chen', 'Jingyang Yuan', 'Junjie Qiu', 'Junlong Li', 'Junxiao Song', 'Kai Dong', 'Kai Hu', 'Kaige Gao', 'Kang Guan', 'Kexin Huang', 'Kuai Yu', 'Lean Wang', 'Lecong Zhang', 'Lei Xu', 'Leyi Xia', 'Liang Zhao', 'Litong Wang', 'Liyue Zhang', 'Meng Li', 'Miaojun Wang', 'Mingchuan Zhang', 'Minghua Zhang', 'Minghui Tang', 'Mingming Li', 'Ning Tian', 'Panpan Huang', 'Peiyi Wang', 'Peng Zhang', 'Qiancheng Wang', 'Qihao Zhu', 'Qinyu Chen', 'Qiushi Du', 'R. J. Chen', 'R. L. Jin', 'Ruiqi Ge', 'Ruisong Zhang', 'Ruizhe Pan', 'Runji Wang', 'Runxin Xu', 'Ruoyu Zhang', 'Ruyi Chen', 'S. S. Li', 'Shanghao Lu', 'Shangyan Zhou', 'Shanhuang Chen', 'Shaoqing Wu', 'Shengfeng Ye', 'Shengfeng Ye', 'Shirong Ma', 'Shiyu Wang', 'Shuang Zhou', 'Shuiping Yu', 'Shunfeng Zhou', 'Shuting Pan', 'T. Wang', 'Tao Yun', 'Tian Pei', 'Tianyu Sun', 'W. L. Xiao', 'Wangding Zeng', 'Wanjia Zhao', 'Wei An', 'Wen Liu', 'Wenfeng Liang', 'Wenjun Gao', 'Wenqin Yu', 'Wentao Zhang', 'X. Q. Li', 'Xiangyue Jin', 'Xianzu Wang', 'Xiao Bi', 'Xiaodong Liu', 'Xiaohan Wang', 'Xiaojin Shen', 'Xiaokang Chen', 'Xiaokang Zhang', 'Xiaosha Chen', 'Xiaotao Nie', 'Xiaowen Sun', 'Xiaoxiang Wang', 'Xin Cheng', 'Xin Liu', 'Xin Xie', 'Xingchao Liu', 'Xingkai Yu', 'Xinnan Song', 'Xinxia Shan', 'Xinyi Zhou', 'Xinyu Yang', 'Xinyuan Li', 'Xuecheng Su', 'Xuheng Lin', 'Y. K. Li', 'Y. Q. Wang', 'Y. X. Wei', 'Y. X. Zhu', 'Yang Zhang', 'Yanhong Xu', 'Yanhong Xu', 'Yanping Huang', 'Yao Li', 'Yao Zhao', 'Yaofeng Sun', 'Yaohui Li', 'Yaohui Wang', 'Yi Yu', 'Yi Zheng', 'Yichao Zhang', 'Yifan Shi', 'Yiliang Xiong', 'Ying He', 'Ying Tang', 'Yishi Piao', 'Yisong Wang', 'Yixuan Tan', 'Yiyang Ma', 'Yiyuan Liu', 'Yongqiang Guo', 'Yu Wu', 'Yuan Ou', 'Yuchen Zhu', 'Yuduan Wang', 'Yue Gong', 'Yuheng Zou', 'Yujia He', 'Yukun Zha', 'Yunfan Xiong', 'Yunxian Ma', 'Yuting Yan', 'Yuxiang Luo', 'Yuxiang You', 'Yuxuan Liu', 'Yuyang Zhou', 'Z. F. Wu', 'Z. Z. Ren', 'Zehui Ren', 'Zhangli Sha', 'Zhe Fu', 'Zhean Xu', 'Zhen Huang', 'Zhen Zhang', 'Zhenda Xie', 'Zhengyan Zhang', 'Zhewen Hao', 'Zhibin Gou', 'Zhicheng Ma', 'Zhigang Yan', 'Zhihong Shao', 'Zhipeng Xu', 'Zhiyu Wu', 'Zhongyu Zhang', 'Zhuoshu Li', 'Zihui Gu', 'Zijia Zhu', 'Zijun Liu', 'Zilin Li', 'Ziwei Xie', 'Ziyang Song', 'Ziyi Gao', 'Zizheng Pan']
- **Abstrat**: We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.





## Correcting the Mythos of KL-Regularization: Direct Alignment without Overoptimization via Chi-Squared Preference Optimization
- **Url**: http://arxiv.org/abs/2407.13399v3
- **Authors**: ['Audrey Huang', 'Wenhao Zhan', 'Tengyang Xie', 'Jason D. Lee', 'Wen Sun', 'Akshay Krishnamurthy', 'Dylan J. Foster']
- **Abstrat**: Language model alignment methods such as reinforcement learning from human feedback (RLHF) have led to impressive advances in language model capabilities, but are limited by a widely observed phenomenon known as overoptimization, where the quality of the language model degrades over the course of the alignment process. As the model optimizes performance with respect to an offline reward model, it overfits to inaccuracies and drifts away from preferred responses covered by the data. To discourage such distribution shift, KL-regularization is widely employed in existing offline alignment methods, but overoptimization continues to harm performance. Lending theoretical insight into the source of these empirical observations, we first show that the KL-regularization is too weak to prevent overfitting, then raise the following question: is it possible to design an efficient algorithm that is provably robust to overoptimization?   We address this question with a new algorithm for offline alignment, $\chi^2$-Preference Optimization ($\chi$PO). $\chi$PO is a one-line change to Direct Preference Optimization (DPO; Rafailov et al., 2023), which only involves modifying the logarithmic link function in the DPO objective. Despite this minimal change, $\chi$PO implicitly implements the principle of pessimism in the face of uncertainty via regularization with the $\chi^2$-divergence --which quantifies uncertainty more effectively than KL-regularization -- and provably alleviates overoptimization, achieving sample-complexity guarantees based on single-policy concentrability -- the gold standard in offline reinforcement learning. $\chi$PO's simplicity and strong guarantees make it the first practical and general-purpose offline alignment algorithm that is provably robust to overoptimization.





## A New Paradigm in Tuning Learned Indexes: A Reinforcement Learning Enhanced Approach
- **Url**: http://arxiv.org/abs/2502.05001v2
- **Authors**: ['Taiyi Wang', 'Liang Liang', 'Guang Yang', 'Thomas Heinis', 'Eiko Yoneki']
- **Abstrat**: Learned Index Structures (LIS) have significantly advanced data management by leveraging machine learning models to optimize data indexing. However, designing these structures often involves critical trade-offs, making it challenging for both designers and end-users to find an optimal balance tailored to specific workloads and scenarios. While some indexes offer adjustable parameters that demand intensive manual tuning, others rely on fixed configurations based on heuristic auto-tuners or expert knowledge, which may not consistently deliver optimal performance. This paper introduces LITune, a novel framework for end-to-end automatic tuning of Learned Index Structures. LITune employs an adaptive training pipeline equipped with a tailor-made Deep Reinforcement Learning (DRL) approach to ensure stable and efficient tuning. To accommodate long-term dynamics arising from online tuning, we further enhance LITune with an on-the-fly updating mechanism termed the O2 system. These innovations allow LITune to effectively capture state transitions in online tuning scenarios and dynamically adjust to changing data distributions and workloads, marking a significant improvement over other tuning methods. Our experimental results demonstrate that LITune achieves up to a 98% reduction in runtime and a 17-fold increase in throughput compared to default parameter settings given a selected Learned Index instance. These findings highlight LITune's effectiveness and its potential to facilitate broader adoption of LIS in real-world applications.





## A Real-to-Sim-to-Real Approach to Robotic Manipulation with VLM-Generated Iterative Keypoint Rewards
- **Url**: http://arxiv.org/abs/2502.08643v2
- **Authors**: ['Shivansh Patel', 'Xinchen Yin', 'Wenlong Huang', 'Shubham Garg', 'Hooshang Nayyeri', 'Li Fei-Fei', 'Svetlana Lazebnik', 'Yunzhu Li']
- **Abstrat**: Task specification for robotic manipulation in open-world environments is challenging, requiring flexible and adaptive objectives that align with human intentions and can evolve through iterative feedback. We introduce Iterative Keypoint Reward (IKER), a visually grounded, Python-based reward function that serves as a dynamic task specification. Our framework leverages VLMs to generate and refine these reward functions for multi-step manipulation tasks. Given RGB-D observations and free-form language instructions, we sample keypoints in the scene and generate a reward function conditioned on these keypoints. IKER operates on the spatial relationships between keypoints, leveraging commonsense priors about the desired behaviors, and enabling precise SE(3) control. We reconstruct real-world scenes in simulation and use the generated rewards to train reinforcement learning (RL) policies, which are then deployed into the real world-forming a real-to-sim-to-real loop. Our approach demonstrates notable capabilities across diverse scenarios, including both prehensile and non-prehensile tasks, showcasing multi-step task execution, spontaneous error recovery, and on-the-fly strategy adjustments. The results highlight IKER's effectiveness in enabling robots to perform multi-step tasks in dynamic environments through iterative reward shaping.





## Oreo: A Plug-in Context Reconstructor to Enhance Retrieval-Augmented Generation
- **Url**: http://arxiv.org/abs/2502.13019v1
- **Authors**: ['Sha Li', 'Naren Ramarkrishnan']
- **Abstrat**: Despite the remarkable capabilities of Large Language Models (LLMs) in various NLP tasks, they remain vulnerable to hallucinations due to their limited parametric knowledge and lack of domain-specific expertise. Retrieval-Augmented Generation (RAG) addresses this challenge by incorporating external document retrieval to augment the knowledge base of LLMs. In this approach, RAG retrieves document chunks from an external corpus in response to a query, which are then used as context for the downstream language model to generate an answer. However, these retrieved knowledge sources often include irrelevant or erroneous information, undermining the effectiveness of RAG in downstream tasks. To overcome this limitation, we introduce a compact, efficient, and pluggable module designed to refine external knowledge sources before feeding them to the generator. The module reconstructs retrieved content by extracting the most relevant and supportive information and reorganising it into a concise, query-specific format. Through a three-stage training paradigm - comprising supervised fine-tuning, contrastive multi-task learning, and reinforcement learning-based alignment - it prioritises critical knowledge and aligns it with the generator's preferences. This method enables LLMs to produce outputs that are more accurate, reliable, and contextually appropriate.





## HOMIE: Humanoid Loco-Manipulation with Isomorphic Exoskeleton Cockpit
- **Url**: http://arxiv.org/abs/2502.13013v1
- **Authors**: ['Qingwei Ben', 'Feiyu Jia', 'Jia Zeng', 'Junting Dong', 'Dahua Lin', 'Jiangmiao Pang']
- **Abstrat**: Current humanoid teleoperation systems either lack reliable low-level control policies, or struggle to acquire accurate whole-body control commands, making it difficult to teleoperate humanoids for loco-manipulation tasks. To solve these issues, we propose HOMIE, a novel humanoid teleoperation cockpit integrates a humanoid loco-manipulation policy and a low-cost exoskeleton-based hardware system. The policy enables humanoid robots to walk and squat to specific heights while accommodating arbitrary upper-body poses. This is achieved through our novel reinforcement learning-based training framework that incorporates upper-body pose curriculum, height-tracking reward, and symmetry utilization, without relying on any motion priors. Complementing the policy, the hardware system integrates isomorphic exoskeleton arms, a pair of motion-sensing gloves, and a pedal, allowing a single operator to achieve full control of the humanoid robot. Our experiments show our cockpit facilitates more stable, rapid, and precise humanoid loco-manipulation teleoperation, accelerating task completion and eliminating retargeting errors compared to inverse kinematics-based methods. We also validate the effectiveness of the data collected by our cockpit for imitation learning. Our project is fully open-sourced, demos and code can be found in https://homietele.github.io/.





## Integrating Reinforcement Learning, Action Model Learning, and Numeric Planning for Tackling Complex Tasks
- **Url**: http://arxiv.org/abs/2502.13006v1
- **Authors**: ['Yarin Benyamin', 'Argaman Mordoch', 'Shahaf S. Shperberg', 'Roni Stern']
- **Abstrat**: Automated Planning algorithms require a model of the domain that specifies the preconditions and effects of each action. Obtaining such a domain model is notoriously hard. Algorithms for learning domain models exist, yet it remains unclear whether learning a domain model and planning is an effective approach for numeric planning environments, i.e., where states include discrete and numeric state variables. In this work, we explore the benefits of learning a numeric domain model and compare it with alternative model-free solutions. As a case study, we use two tasks in Minecraft, a popular sandbox game that has been used as an AI challenge. First, we consider an offline learning setting, where a set of expert trajectories are available to learn from. This is the standard setting for learning domain models. We used the Numeric Safe Action Model Learning (NSAM) algorithm to learn a numeric domain model and solve new problems with the learned domain model and a numeric planner. We call this model-based solution NSAM_(+p), and compare it to several model-free Imitation Learning (IL) and Offline Reinforcement Learning (RL) algorithms. Empirical results show that some IL algorithms can learn faster to solve simple tasks, while NSAM_(+p) allows solving tasks that require long-term planning and enables generalizing to solve problems in larger environments. Then, we consider an online learning setting, where learning is done by moving an agent in the environment. For this setting, we introduce RAMP. In RAMP, observations collected during the agent's execution are used to simultaneously train an RL policy and learn a planning domain action model. This forms a positive feedback loop between the RL policy and the learned domain model. We demonstrate experimentally the benefits of using RAMP, showing that it finds more efficient plans and solves more problems than several RL baselines.





## R3L: Relative Representations for Reinforcement Learning
- **Url**: http://arxiv.org/abs/2404.12917v3
- **Authors**: ['Antonio Pio Ricciardi', 'Valentino Maiorca', 'Luca Moschella', 'Riccardo Marin', 'Emanuele Rodolà']
- **Abstrat**: Visual Reinforcement Learning is a popular and powerful framework that takes full advantage of the Deep Learning breakthrough. It is known that variations in input domains (e.g., different panorama colors due to seasonal changes) or task domains (e.g., altering the target speed of a car) can disrupt agent performance, necessitating new training for each variation. Recent advancements in the field of representation learning have demonstrated the possibility of combining components from different neural networks to create new models in a zero-shot fashion. In this paper, we build upon relative representations, a framework that maps encoder embeddings to a universal space. We adapt this framework to the Visual Reinforcement Learning setting, allowing to combine agents components to create new agents capable of effectively handling novel visual-task pairs not encountered during training. Our findings highlight the potential for model reuse, significantly reducing the need for retraining and, consequently, the time and computational resources required.





## Sable: a Performant, Efficient and Scalable Sequence Model for MARL
- **Url**: http://arxiv.org/abs/2410.01706v3
- **Authors**: ['Omayma Mahjoub', 'Sasha Abramowitz', 'Ruan de Kock', 'Wiem Khlifi', 'Simon du Toit', 'Jemma Daniel', 'Louay Ben Nessir', 'Louise Beyers', 'Claude Formanek', 'Liam Clark', 'Arnu Pretorius']
- **Abstrat**: As multi-agent reinforcement learning (MARL) progresses towards solving larger and more complex problems, it becomes increasingly important that algorithms exhibit the key properties of (1) strong performance, (2) memory efficiency and (3) scalability. In this work, we introduce Sable, a performant, memory efficient and scalable sequence modeling approach to MARL. Sable works by adapting the retention mechanism in Retentive Networks (Sun et al., 2023) to achieve computationally efficient processing of multi-agent observations with long context memory for temporal reasoning. Through extensive evaluations across six diverse environments, we demonstrate how Sable is able to significantly outperform existing state-of-the-art methods in a large number of diverse tasks (34 out of 45 tested). Furthermore, Sable maintains performance as we scale the number of agents, handling environments with more than a thousand agents while exhibiting a linear increase in memory usage. Finally, we conduct ablation studies to isolate the source of Sable's performance gains and confirm its efficient computational memory usage.





## Flow-of-Options: Diversified and Improved LLM Reasoning by Thinking Through Options
- **Url**: http://arxiv.org/abs/2502.12929v1
- **Authors**: ['Lakshmi Nair', 'Ian Trase', 'Mark Kim']
- **Abstrat**: We present a novel reasoning approach called Flow-of-Options (FoO), designed to address intrinsic biases in Large Language Models (LLMs). FoO enables LLMs to systematically explore a diverse range of possibilities in their reasoning, as demonstrated by an FoO-based agentic system for autonomously solving Machine Learning tasks (AutoML). Our framework outperforms state-of-the-art baselines, achieving improvements of 38.2% - 69.2% on standard data science tasks, and 37.4% - 47.9% on therapeutic chemistry tasks. With an overall operation cost under $1 per task, our framework is well-suited for cost-sensitive applications. Beyond classification and regression, we illustrate the broader applicability of our FoO-based agentic system to tasks such as reinforcement learning and image generation. Our framework presents significant advancements compared to current state-of-the-art agentic systems for AutoML, due to the benefits of FoO in enforcing diversity in LLM solutions through compressed, explainable representations that also support long-term memory when combined with case-based reasoning.





# TD3
# Prioritized Experience Replay
# path planning
## NTP-INT: Network Traffic Prediction-Driven In-band Network Telemetry for High-load Switches
- **Url**: http://arxiv.org/abs/2502.12834v1
- **Authors**: ['Penghui Zhang', 'Hua Zhang', 'Yuqi Dai', 'Cheng Zeng', 'Jingyu Wang', 'Jianxin Liao']
- **Abstrat**: In-band network telemetry (INT) is essential to network management due to its real-time visibility. However, because of the rapid increase in network devices and services, it has become crucial to have targeted access to detailed network information in a dynamic network environment. This paper proposes an intelligent network telemetry system called NTP-INT to obtain more fine-grained network information on high-load switches. Specifically, NTP-INT consists of three modules: network traffic prediction module, network pruning module, and probe path planning module. Firstly, the network traffic prediction module adopts a Multi-Temporal Graph Neural Network (MTGNN) to predict future network traffic and identify high-load switches. Then, we design the network pruning algorithm to generate a subnetwork covering all high-load switches to reduce the complexity of probe path planning. Finally, the probe path planning module uses an attention-mechanism-based deep reinforcement learning (DEL) model to plan efficient probe paths in the network slice. The experimental results demonstrate that NTP-INT can acquire more precise network information on high-load switches while decreasing the control overhead by 50\%.





## Introducing ROADS: A Systematic Comparison of Remote Control Interaction Concepts for Automated Vehicles at Road Works
- **Url**: http://arxiv.org/abs/2502.12680v1
- **Authors**: ['Mark Colley', 'Jonathan Westhauser', 'Jonas Andersson', 'Alexander G. Mirnig', 'Enrico Rukzio']
- **Abstrat**: As vehicle automation technology continues to mature, there is a necessity for robust remote monitoring and intervention features. These are essential for intervening during vehicle malfunctions, challenging road conditions, or in areas that are difficult to navigate. This evolution in the role of the human operator - from a constant driver to an intermittent teleoperator -necessitates the development of suitable interaction interfaces. While some interfaces were suggested, a comparative study is missing. We designed, implemented, and evaluated three interaction concepts (path planning, trajectory guidance, and waypoint guidance) with up to four concurrent requests of automated vehicles in a within-subjects study with N=23 participants. The results showed a clear preference for the path planning concept. It also led to the highest usability but lower satisfaction. With trajectory guidance, the fewest requests were resolved. The study's findings contribute to the ongoing development of HMIs focused on the remote assistance of automated vehicles.




