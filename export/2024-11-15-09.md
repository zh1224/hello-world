# reinforcement learning
## A Risk Sensitive Contract-unified Reinforcement Learning Approach for Option Hedging
- **Url**: http://arxiv.org/abs/2411.09659v1
- **Authors**: ['Xianhua Peng', 'Xiang Zhou', 'Bo Xiao', 'Yi Wu']
- **Abstrat**: We propose a new risk sensitive reinforcement learning approach for the dynamic hedging of options. The approach focuses on the minimization of the tail risk of the final P&L of the seller of an option. Different from most existing reinforcement learning approaches that require a parametric model of the underlying asset, our approach can learn the optimal hedging strategy directly from the historical market data without specifying a parametric model; in addition, the learned optimal hedging strategy is contract-unified, i.e., it applies to different options contracts with different initial underlying prices, strike prices, and maturities. Our approach extends existing reinforcement learning methods by learning the tail risk measures of the final hedging P&L and the optimal hedging strategy at the same time. We carry out comprehensive empirical study to show that, in the out-of-sample tests, the proposed reinforcement learning hedging strategy can obtain statistically significantly lower tail risk and higher mean of the final P&L than delta hedging methods.





## Learning Multi-Agent Loco-Manipulation for Long-Horizon Quadrupedal Pushing
- **Url**: http://arxiv.org/abs/2411.07104v2
- **Authors**: ['Yuming Feng', 'Chuye Hong', 'Yaru Niu', 'Shiqi Liu', 'Yuxiang Yang', 'Wenhao Yu', 'Tingnan Zhang', 'Jie Tan', 'Ding Zhao']
- **Abstrat**: Recently, quadrupedal locomotion has achieved significant success, but their manipulation capabilities, particularly in handling large objects, remain limited, restricting their usefulness in demanding real-world applications such as search and rescue, construction, industrial automation, and room organization. This paper tackles the task of obstacle-aware, long-horizon pushing by multiple quadrupedal robots. We propose a hierarchical multi-agent reinforcement learning framework with three levels of control. The high-level controller integrates an RRT planner and a centralized adaptive policy to generate subgoals, while the mid-level controller uses a decentralized goal-conditioned policy to guide the robots toward these sub-goals. A pre-trained low-level locomotion policy executes the movement commands. We evaluate our method against several baselines in simulation, demonstrating significant improvements over baseline approaches, with 36.0% higher success rates and 24.5% reduction in completion time than the best baseline. Our framework successfully enables long-horizon, obstacle-aware manipulation tasks like Push-Cuboid and Push-T on Go1 robots in the real world.





## Tailoring interactions between active nematic defects with reinforcement learning
- **Url**: http://arxiv.org/abs/2411.09588v1
- **Authors**: ['Carlos Floyd', 'Aaron R. Dinner', 'Suriyanarayanan Vaikuntanathan']
- **Abstrat**: Active nematics, formed from a liquid crystalline suspension of active force dipoles, are a paradigmatic active matter system whose study provides insights into how chemical driving produces the cellular mechanical forces essential for life. Recent advances in optogenetic control over molecular motors and cell-signaling pathways now allow experimenters to mimic the spatiotemporal regulation of activity necessary to drive biologically relevant active nematic flows in vivo. However, engineering effective activity protocols remains challenging due to the system's complex dynamics. Here, we explore a model-free approach for controlling active nematic fields using reinforcement learning. Specifically, we demonstrate how local activity fields can induce interactions between pairs of nematic defects, enabling them to follow designer dynamical laws such as those of overdamped springs with varying stiffnesses. Reinforcement learning bypasses the need for accurate parameterization and model representation of the nematic system, and could thus transfer straightforwardly to experimental implementation. Moreover, the sufficiency of our low-dimensional system observables and actions suggests that coarse projections of the active nematic field can be used for precise feedback control, making the biological implementation of such feedback loops plausible.





## From Imitation to Refinement -- Residual RL for Precise Assembly
- **Url**: http://arxiv.org/abs/2407.16677v3
- **Authors**: ['Lars Ankile', 'Anthony Simeonov', 'Idan Shenfeld', 'Marcel Torne', 'Pulkit Agrawal']
- **Abstrat**: Advances in behavior cloning (BC), like action-chunking and diffusion, have enabled impressive capabilities. Still, imitation alone remains insufficient for learning reliable policies for tasks requiring precise aligning and inserting of objects, like assembly. Our key insight is that chunked BC policies effectively function as trajectory planners, enabling long-horizon tasks. Conversely, as they execute action chunks open-loop, they lack the fine-grained reactivity necessary for reliable execution. Further, we find that the performance of BC policies saturates despite increasing data. Reinforcement learning (RL) is a natural way to overcome BC's limitations, but it is not straightforward to apply directly to action-chunked models like diffusion policies. We present a simple yet effective method, ResiP (Residual for Precise Manipulation), that sidesteps these challenges by augmenting a frozen, chunked BC model with a fully closed-loop residual policy trained with RL. The residual policy is trained via on-policy RL, addressing distribution shifts and introducing reactive control without altering the BC trajectory planner. Evaluation on high-precision manipulation tasks demonstrates strong performance of ResiP over BC methods and direct RL fine-tuning. Videos, code, and data are available at https://residual-assembly.github.io.





## Terracorder: Sense Long and Prosper
- **Url**: http://arxiv.org/abs/2408.02407v3
- **Authors**: ['Josh Millar', 'Sarab Sethi', 'Hamed Haddadi', 'Anil Madhavapeddy']
- **Abstrat**: In-situ sensing devices need to be deployed in remote environments for long periods of time; minimizing their power consumption is vital for maximising both their operational lifetime and coverage. We introduce Terracorder -- a versatile multi-sensor device -- and showcase its exceptionally low power consumption using an on-device reinforcement learning scheduler. We prototype a unique device setup for biodiversity monitoring and compare its battery life using our scheduler against a number of fixed schedules; the scheduler captures more than 80% of events at less than 50% of the number of activations of the best-performing fixed schedule. We then explore how a collaborative scheduler can maximise the useful operation of a network of devices, improving overall network power consumption and robustness.





## CaRL: Cascade Reinforcement Learning with State Space Splitting for O-RAN based Traffic Steering
- **Url**: http://arxiv.org/abs/2312.01970v2
- **Authors**: ['Chuanneng Sun', 'Gueyoung Jung', 'Tuyen Xuan Tran', 'Dario Pompili']
- **Abstrat**: The Open Radio Access Network (O-RAN) architecture empowers intelligent and automated optimization of the RAN through applications deployed on the RAN Intelligent Controller (RIC) platform, enabling capabilities beyond what is achievable with traditional RAN solutions. Within this paradigm, Traffic Steering (TS) emerges as a pivotal RIC application that focuses on optimizing cell-level mobility settings in near-real-time, aiming to significantly improve network spectral efficiency. In this paper, we design a novel TS algorithm based on a Cascade Reinforcement Learning (CaRL) framework. We propose state space factorization and policy decomposition to reduce the need for large models and well-labeled datasets. For each sub-state space, an RL sub-policy will be trained to learn an optimized mapping onto the action space. To apply CaRL on new network regions, we propose a knowledge transfer approach to initialize a new sub-policy based on knowledge learned by the trained policies. To evaluate CaRL, we build a data-driven and scalable RIC digital twin (DT) that is modeled using important real-world data, including network configuration, user geo-distribution, and traffic demand, among others, from a tier-1 mobile operator in the US. We evaluate CaRL on two DT scenarios representing two network clusters in two different cities and compare its performance with the business-as-usual (BAU) policy and other competing optimization approaches using heuristic and Q-table algorithms. Benchmarking results show that CaRL performs the best and improves the average cluster-aggregated downlink throughput over the BAU policy by 24% and 18% in these two scenarios, respectively.





## Tract-RLFormer: A Tract-Specific RL policy based Decoder-only Transformer Network
- **Url**: http://arxiv.org/abs/2411.05757v2
- **Authors**: ['Ankita Joshi', 'Ashutosh Sharma', 'Anoushkrit Goel', 'Ranjeet Ranjan Jha', 'Chirag Ahuja', 'Arnav Bhavsar', 'Aditya Nigam']
- **Abstrat**: Fiber tractography is a cornerstone of neuroimaging, enabling the detailed mapping of the brain's white matter pathways through diffusion MRI. This is crucial for understanding brain connectivity and function, making it a valuable tool in neurological applications. Despite its importance, tractography faces challenges due to its complexity and susceptibility to false positives, misrepresenting vital pathways. To address these issues, recent strategies have shifted towards deep learning, utilizing supervised learning, which depends on precise ground truth, or reinforcement learning, which operates without it. In this work, we propose Tract-RLFormer, a network utilizing both supervised and reinforcement learning, in a two-stage policy refinement process that markedly improves the accuracy and generalizability across various data-sets. By employing a tract-specific approach, our network directly delineates the tracts of interest, bypassing the traditional segmentation process. Through rigorous validation on datasets such as TractoInferno, HCP, and ISMRM-2015, our methodology demonstrates a leap forward in tractography, showcasing its ability to accurately map the brain's white matter tracts.





## Approximated Variational Bayesian Inverse Reinforcement Learning for Large Language Model Alignment
- **Url**: http://arxiv.org/abs/2411.09341v1
- **Authors**: ['Yuang Cai', 'Yuyu Yuan', 'Jinsheng Shi', 'Qinhong Lin']
- **Abstrat**: The alignment of large language models (LLMs) is crucial for generating helpful and harmless content. Existing approaches leverage preference-based human feedback data to learn the reward function and align the LLM with the feedback data. However, these approaches focus on modeling the reward difference between the chosen and rejected demonstrations, rather than directly modeling the true reward from each demonstration. Moreover, these approaches assume that the reward is only obtained at the end of the sentence, which overlooks the modeling of intermediate rewards. These issues lead to insufficient use of training signals in the feedback data, limiting the representation and generalization ability of the reward and potentially resulting in reward hacking. In this paper, we formulate LLM alignment as a Bayesian Inverse Reinforcement Learning (BIRL) problem and propose a novel training objective, Approximated Variational Alignment (AVA), to perform LLM alignment through Approximated Variational Reward Imitation Learning (AVRIL). The BIRL formulation facilitates intermediate reward modeling and direct reward modeling on each single demonstration, which enhances the utilization of training signals in the feedback data. Experiments show that AVA outperforms existing LLM alignment approaches in reward modeling, RL fine-tuning, and direct optimization.





## Socio-Economic Consequences of Generative AI: A Review of Methodological Approaches
- **Url**: http://arxiv.org/abs/2411.09313v1
- **Authors**: ['Carlos J. Costa', 'Joao Tiago Aparicio', 'Manuela Aparicio']
- **Abstrat**: The widespread adoption of generative artificial intelligence (AI) has fundamentally transformed technological landscapes and societal structures in recent years. Our objective is to identify the primary methodologies that may be used to help predict the economic and social impacts of generative AI adoption. Through a comprehensive literature review, we uncover a range of methodologies poised to assess the multifaceted impacts of this technological revolution. We explore Agent-Based Simulation (ABS), Econometric Models, Input-Output Analysis, Reinforcement Learning (RL) for Decision-Making Agents, Surveys and Interviews, Scenario Analysis, Policy Analysis, and the Delphi Method. Our findings have allowed us to identify these approaches' main strengths and weaknesses and their adequacy in coping with uncertainty, robustness, and resource requirements.





# TD3
# Prioritized Experience Replay
# path planning