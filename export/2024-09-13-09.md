# reinforcement learning
## Hand-Object Interaction Pretraining from Videos
- **Url**: http://arxiv.org/abs/2409.08273v1
- **Authors**: ['Himanshu Gaurav Singh', 'Antonio Loquercio', 'Carmelo Sferrazza', 'Jane Wu', 'Haozhi Qi', 'Pieter Abbeel', 'Jitendra Malik']
- **Abstrat**: We present an approach to learn general robot manipulation priors from 3D hand-object interaction trajectories. We build a framework to use in-the-wild videos to generate sensorimotor robot trajectories. We do so by lifting both the human hand and the manipulated object in a shared 3D space and retargeting human motions to robot actions. Generative modeling on this data gives us a task-agnostic base policy. This policy captures a general yet flexible manipulation prior. We empirically demonstrate that finetuning this policy, with both reinforcement learning (RL) and behavior cloning (BC), enables sample-efficient adaptation to downstream tasks and simultaneously improves robustness and generalizability compared to prior approaches. Qualitative experiments are available at: \url{https://hgaurav2k.github.io/hop/}.





## Multi-Model based Federated Learning Against Model Poisoning Attack: A Deep Learning Based Model Selection for MEC Systems
- **Url**: http://arxiv.org/abs/2409.08237v1
- **Authors**: ['Somayeh Kianpisheh', 'Chafika Benzaid', 'Tarik Taleb']
- **Abstrat**: Federated Learning (FL) enables training of a global model from distributed data, while preserving data privacy. However, the singular-model based operation of FL is open with uploading poisoned models compatible with the global model structure and can be exploited as a vulnerability to conduct model poisoning attacks. This paper proposes a multi-model based FL as a proactive mechanism to enhance the opportunity of model poisoning attack mitigation. A master model is trained by a set of slave models. To enhance the opportunity of attack mitigation, the structure of client models dynamically change within learning epochs, and the supporter FL protocol is provided. For a MEC system, the model selection problem is modeled as an optimization to minimize loss and recognition time, while meeting a robustness confidence. In adaption with dynamic network condition, a deep reinforcement learning based model selection is proposed. For a DDoS attack detection scenario, results illustrate a competitive accuracy gain under poisoning attack with the scenario that the system is without attack, and also a potential of recognition time improvement.





## Towards Online Safety Corrections for Robotic Manipulation Policies
- **Url**: http://arxiv.org/abs/2409.08233v1
- **Authors**: ['Ariana Spalter', 'Mark Roberts', 'Laura M. Hiatt']
- **Abstrat**: Recent successes in applying reinforcement learning (RL) for robotics has shown it is a viable approach for constructing robotic controllers. However, RL controllers can produce many collisions in environments where new obstacles appear during execution. This poses a problem in safety-critical settings. We present a hybrid approach, called iKinQP-RL, that uses an Inverse Kinematics Quadratic Programming (iKinQP) controller to correct actions proposed by an RL policy at runtime. This ensures safe execution in the presence of new obstacles not present during training. Preliminary experiments illustrate our iKinQP-RL framework completely eliminates collisions with new obstacles while maintaining a high task success rate.





## Design Optimization of Nuclear Fusion Reactor through Deep Reinforcement Learning
- **Url**: http://arxiv.org/abs/2409.08231v1
- **Authors**: ['Jinsu Kim', 'Jaemin Seo']
- **Abstrat**: This research explores the application of Deep Reinforcement Learning (DRL) to optimize the design of a nuclear fusion reactor. DRL can efficiently address the challenging issues attributed to multiple physics and engineering constraints for steady-state operation. The fusion reactor design computation and the optimization code applicable to parallelization with DRL are developed. The proposed framework enables finding the optimal reactor design that satisfies the operational requirements while reducing building costs. Multi-objective design optimization for a fusion reactor is now simplified by DRL, indicating the high potential of the proposed framework for advancing the efficient and sustainable design of future reactors.





## Rethinking Teacher-Student Curriculum Learning through the Cooperative Mechanics of Experience
- **Url**: http://arxiv.org/abs/2404.03084v2
- **Authors**: ['Manfred Diaz', 'Liam Paull', 'Andrea Tacchetti']
- **Abstrat**: Teacher-Student Curriculum Learning (TSCL) is a curriculum learning framework that draws inspiration from human cultural transmission and learning. It involves a teacher algorithm shaping the learning process of a learner algorithm by exposing it to controlled experiences. Despite its success, understanding the conditions under which TSCL is effective remains challenging. In this paper, we propose a data-centric perspective to analyze the underlying mechanics of the teacher-student interactions in TSCL. We leverage cooperative game theory to describe how the composition of the set of experiences presented by the teacher to the learner, as well as their order, influences the performance of the curriculum that is found by TSCL approaches. To do so, we demonstrate that for every TSCL problem, an equivalent cooperative game exists, and several key components of the TSCL framework can be reinterpreted using game-theoretic principles. Through experiments covering supervised learning, reinforcement learning, and classical games, we estimate the cooperative values of experiences and use value-proportional curriculum mechanisms to construct curricula, even in cases where TSCL struggles. The framework and experimental setup we present in this work represents a novel foundation for a deeper exploration of TSCL, shedding light on its underlying mechanisms and providing insights into its broader applicability in machine learning.





## Adaptive Language-Guided Abstraction from Contrastive Explanations
- **Url**: http://arxiv.org/abs/2409.08212v1
- **Authors**: ['Andi Peng', 'Belinda Z. Li', 'Ilia Sucholutsky', 'Nishanth Kumar', 'Julie A. Shah', 'Jacob Andreas', 'Andreea Bobu']
- **Abstrat**: Many approaches to robot learning begin by inferring a reward function from a set of human demonstrations. To learn a good reward, it is necessary to determine which features of the environment are relevant before determining how these features should be used to compute reward. End-to-end methods for joint feature and reward learning (e.g., using deep networks or program synthesis techniques) often yield brittle reward functions that are sensitive to spurious state features. By contrast, humans can often generalizably learn from a small number of demonstrations by incorporating strong priors about what features of a demonstration are likely meaningful for a task of interest. How do we build robots that leverage this kind of background knowledge when learning from new demonstrations? This paper describes a method named ALGAE (Adaptive Language-Guided Abstraction from [Contrastive] Explanations) which alternates between using language models to iteratively identify human-meaningful features needed to explain demonstrated behavior, then standard inverse reinforcement learning techniques to assign weights to these features. Experiments across a variety of both simulated and real-world robot environments show that ALGAE learns generalizable reward functions defined on interpretable features using only small numbers of demonstrations. Importantly, ALGAE can recognize when features are missing, then extract and define those features without any human input -- making it possible to quickly and efficiently acquire rich representations of user behavior.





## Optimal Management of Grid-Interactive Efficient Buildings via Safe Reinforcement Learning
- **Url**: http://arxiv.org/abs/2409.08132v1
- **Authors**: ['Xiang Huo', 'Boming Liu', 'Jin Dong', 'Jianming Lian', 'Mingxi Liu']
- **Abstrat**: Reinforcement learning (RL)-based methods have achieved significant success in managing grid-interactive efficient buildings (GEBs). However, RL does not carry intrinsic guarantees of constraint satisfaction, which may lead to severe safety consequences. Besides, in GEB control applications, most existing safe RL approaches rely only on the regularisation parameters in neural networks or penalty of rewards, which often encounter challenges with parameter tuning and lead to catastrophic constraint violations. To provide enforced safety guarantees in controlling GEBs, this paper designs a physics-inspired safe RL method whose decision-making is enhanced through safe interaction with the environment. Different energy resources in GEBs are optimally managed to minimize energy costs and maximize customer comfort. The proposed approach can achieve strict constraint guarantees based on prior knowledge of a set of developed hard steady-state rules. Simulations on the optimal management of GEBs, including heating, ventilation, and air conditioning (HVAC), solar photovoltaics, and energy storage systems, demonstrate the effectiveness of the proposed approach.





## Linear Complementary Dual Codes Constructed from Reinforcement Learning
- **Url**: http://arxiv.org/abs/2409.08114v1
- **Authors**: ['Yansheng Wu', 'Jin Ma', 'Shandong Yang']
- **Abstrat**: Recently, Linear Complementary Dual (LCD) codes have garnered substantial interest within coding theory research due to their diverse applications and favorable attributes. This paper directs its attention to the construction of binary and ternary LCD codes leveraging curiosity-driven reinforcement learning (RL). By establishing reward and devising well-reasoned mappings from actions to states, it aims to facilitate the successful synthesis of binary or ternary LCD codes. Experimental results indicate that LCD codes constructed using RL exhibit slightly superior error-correction performance compared to those conventionally constructed LCD codes and those developed via standard RL methodologies. The paper introduces novel binary and ternary LCD codes with enhanced minimum distance bounds. Finally, it showcases how Random Network Distillation aids agents in exploring beyond local optima, enhancing the overall performance of the models without compromising convergence.





## Q-value Regularized Decision ConvFormer for Offline Reinforcement Learning
- **Url**: http://arxiv.org/abs/2409.08062v1
- **Authors**: ['Teng Yan', 'Zhendong Ruan', 'Yaobang Cai', 'Yu Han', 'Wenxian Li', 'Yang Zhang']
- **Abstrat**: As a data-driven paradigm, offline reinforcement learning (Offline RL) has been formulated as sequence modeling, where the Decision Transformer (DT) has demonstrated exceptional capabilities. Unlike previous reinforcement learning methods that fit value functions or compute policy gradients, DT adjusts the autoregressive model based on the expected returns, past states, and actions, using a causally masked Transformer to output the optimal action. However, due to the inconsistency between the sampled returns within a single trajectory and the optimal returns across multiple trajectories, it is challenging to set an expected return to output the optimal action and stitch together suboptimal trajectories. Decision ConvFormer (DC) is easier to understand in the context of modeling RL trajectories within a Markov Decision Process compared to DT. We propose the Q-value Regularized Decision ConvFormer (QDC), which combines the understanding of RL trajectories by DC and incorporates a term that maximizes action values using dynamic programming methods during training. This ensures that the expected returns of the sampled actions are consistent with the optimal returns. QDC achieves excellent performance on the D4RL benchmark, outperforming or approaching the optimal level in all tested environments. It particularly demonstrates outstanding competitiveness in trajectory stitching capability.





## MAPF-GPT: Imitation Learning for Multi-Agent Pathfinding at Scale
- **Url**: http://arxiv.org/abs/2409.00134v2
- **Authors**: ['Anton Andreychuk', 'Konstantin Yakovlev', 'Aleksandr Panov', 'Alexey Skrynnik']
- **Abstrat**: Multi-agent pathfinding (MAPF) is a challenging computational problem that typically requires to find collision-free paths for multiple agents in a shared environment. Solving MAPF optimally is NP-hard, yet efficient solutions are critical for numerous applications, including automated warehouses and transportation systems. Recently, learning-based approaches to MAPF have gained attention, particularly those leveraging deep reinforcement learning. Following current trends in machine learning, we have created a foundation model for the MAPF problems called MAPF-GPT. Using imitation learning, we have trained a policy on a set of pre-collected sub-optimal expert trajectories that can generate actions in conditions of partial observability without additional heuristics, reward functions, or communication with other agents. The resulting MAPF-GPT model demonstrates zero-shot learning abilities when solving the MAPF problem instances that were not present in the training dataset. We show that MAPF-GPT notably outperforms the current best-performing learnable-MAPF solvers on a diverse range of problem instances and is efficient in terms of computation (in the inference mode).





## Long and Short-Term Constraints Driven Safe Reinforcement Learning for Autonomous Driving
- **Url**: http://arxiv.org/abs/2403.18209v2
- **Authors**: ['Xuemin Hu', 'Pan Chen', 'Yijun Wen', 'Bo Tang', 'Long Chen']
- **Abstrat**: Reinforcement learning (RL) has been widely used in decision-making and control tasks, but the risk is very high for the agent in the training process due to the requirements of interaction with the environment, which seriously limits its industrial applications such as autonomous driving systems. Safe RL methods are developed to handle this issue by constraining the expected safety violation costs as a training objective, but the occurring probability of an unsafe state is still high, which is unacceptable in autonomous driving tasks. Moreover, these methods are difficult to achieve a balance between the cost and return expectations, which leads to learning performance degradation for the algorithms. In this paper, we propose a novel algorithm based on the long and short-term constraints (LSTC) for safe RL. The short-term constraint aims to enhance the short-term state safety that the vehicle explores, while the long-term constraint enhances the overall safety of the vehicle throughout the decision-making process, both of which are jointly used to enhance the vehicle safety in the training process. In addition, we develop a safe RL method with dual-constraint optimization based on the Lagrange multiplier to optimize the training process for end-to-end autonomous driving. Comprehensive experiments were conducted on the MetaDrive simulator. Experimental results demonstrate that the proposed method achieves higher safety in continuous state and action tasks, and exhibits higher exploration performance in long-distance decision-making tasks compared with state-of-the-art methods.





## Learning Causally Invariant Reward Functions from Diverse Demonstrations
- **Url**: http://arxiv.org/abs/2409.08012v1
- **Authors**: ['Ivan Ovinnikov', 'Eugene Bykovets', 'Joachim M. Buhmann']
- **Abstrat**: Inverse reinforcement learning methods aim to retrieve the reward function of a Markov decision process based on a dataset of expert demonstrations. The commonplace scarcity and heterogeneous sources of such demonstrations can lead to the absorption of spurious correlations in the data by the learned reward function. Consequently, this adaptation often exhibits behavioural overfitting to the expert data set when a policy is trained on the obtained reward function under distribution shift of the environment dynamics. In this work, we explore a novel regularization approach for inverse reinforcement learning methods based on the causal invariance principle with the goal of improved reward function generalization. By applying this regularization to both exact and approximate formulations of the learning task, we demonstrate superior policy performance when trained using the recovered reward functions in a transfer setting





## Guided Safe Shooting: model based reinforcement learning with safety constraints
- **Url**: http://arxiv.org/abs/2206.09743v2
- **Authors**: ['Giuseppe Paolo', 'Jonas Gonzalez-Billandon', 'Albert Thomas', 'Balázs Kégl']
- **Abstrat**: In the last decade, reinforcement learning successfully solved complex control tasks and decision-making problems, like the Go board game. Yet, there are few success stories when it comes to deploying those algorithms to real-world scenarios. One of the reasons is the lack of guarantees when dealing with and avoiding unsafe states, a fundamental requirement in critical control engineering systems. In this paper, we introduce Guided Safe Shooting (GuSS), a model-based RL approach that can learn to control systems with minimal violations of the safety constraints. The model is learned on the data collected during the operation of the system in an iterated batch fashion, and is then used to plan for the best action to perform at each time step. We propose three different safe planners, one based on a simple random shooting strategy and two based on MAP-Elites, a more advanced divergent-search algorithm. Experiments show that these planners help the learning agent avoid unsafe situations while maximally exploring the state space, a necessary aspect when learning an accurate model of the system. Furthermore, compared to model-free approaches, learning a model allows GuSS reducing the number of interactions with the real-system while still reaching high rewards, a fundamental requirement when handling engineering systems.





## Digital Twin for Autonomous Guided Vehicles based on Integrated Sensing and Communications
- **Url**: http://arxiv.org/abs/2409.08005v1
- **Authors**: ['Van-Phuc Bui', 'Pedro Maia de Sant Ana', 'Soheil Gherekhloo', 'Shashi Raj Pandey', 'Petar Popovski']
- **Abstrat**: This paper presents a Digital Twin (DT) framework for the remote control of an Autonomous Guided Vehicle (AGV) within a Network Control System (NCS). The AGV is monitored and controlled using Integrated Sensing and Communications (ISAC). In order to meet the real-time requirements, the DT computes the control signals and dynamically allocates resources for sensing and communication. A Reinforcement Learning (RL) algorithm is derived to learn and provide suitable actions while adjusting for the uncertainty in the AGV's position. We present closed-form expressions for the achievable communication rate and the Cramer-Rao bound (CRB) to determine the required number of Orthogonal Frequency-Division Multiplexing (OFDM) subcarriers, meeting the needs of both sensing and communication. The proposed algorithm is validated through a millimeter-Wave (mmWave) simulation, demonstrating significant improvements in both control precision and communication efficiency.





## Reinforcement Learning Discovers Efficient Decentralized Graph Path Search Strategies
- **Url**: http://arxiv.org/abs/2409.07932v1
- **Authors**: ['Alexei Pisacane', 'Victor-Alexandru Darvariu', 'Mirco Musolesi']
- **Abstrat**: Graph path search is a classic computer science problem that has been recently approached with Reinforcement Learning (RL) due to its potential to outperform prior methods. Existing RL techniques typically assume a global view of the network, which is not suitable for large-scale, dynamic, and privacy-sensitive settings. An area of particular interest is search in social networks due to its numerous applications. Inspired by seminal work in experimental sociology, which showed that decentralized yet efficient search is possible in social networks, we frame the problem as a collaborative task between multiple agents equipped with a limited local view of the network. We propose a multi-agent approach for graph path search that successfully leverages both homophily and structural heterogeneity. Our experiments, carried out over synthetic and real-world social networks, demonstrate that our model significantly outperforms learned and heuristic baselines. Furthermore, our results show that meaningful embeddings for graph navigation can be constructed using reward-driven learning.





## Tidal MerzA: Combining affective modelling and autonomous code generation through Reinforcement Learning
- **Url**: http://arxiv.org/abs/2409.07918v1
- **Authors**: ['Elizabeth Wilson', 'György Fazekas', 'Geraint Wiggins']
- **Abstrat**: This paper presents Tidal-MerzA, a novel system designed for collaborative performances between humans and a machine agent in the context of live coding, specifically focusing on the generation of musical patterns. Tidal-MerzA fuses two foundational models: ALCAA (Affective Live Coding Autonomous Agent) and Tidal Fuzz, a computational framework. By integrating affective modelling with computational generation, this system leverages reinforcement learning techniques to dynamically adapt music composition parameters within the TidalCycles framework, ensuring both affective qualities to the patterns and syntactical correctness. The development of Tidal-MerzA introduces two distinct agents: one focusing on the generation of mini-notation strings for musical expression, and another on the alignment of music with targeted affective states through reinforcement learning. This approach enhances the adaptability and creative potential of live coding practices and allows exploration of human-machine creative interactions. Tidal-MerzA advances the field of computational music generation, presenting a novel methodology for incorporating artificial intelligence into artistic practices.





## Tera-SpaceCom: GNN-based Deep Reinforcement Learning for Joint Resource Allocation and Task Offloading in TeraHertz Band Space Networks
- **Url**: http://arxiv.org/abs/2409.07911v1
- **Authors**: ['Zhifeng Hu', 'Chong Han', 'Wolfgang Gerstacker', 'Ian F. Akyildiz']
- **Abstrat**: Terahertz (THz) space communications (Tera-SpaceCom) is envisioned as a promising technology to enable various space science and communication applications. Mainly, the realm of Tera-SpaceCom consists of THz sensing for space exploration, data centers in space providing cloud services for space exploration tasks, and a low earth orbit (LEO) mega-constellation relaying these tasks to ground stations (GSs) or data centers via THz links. Moreover, to reduce the computational burden on data centers as well as resource consumption and latency in the relaying process, the LEO mega-constellation provides satellite edge computing (SEC) services to directly compute space exploration tasks without relaying these tasks to data centers. The LEO satellites that receive space exploration tasks offload (i.e., distribute) partial tasks to their neighboring LEO satellites, to further reduce their computational burden. However, efficient joint communication resource allocation and computing task offloading for the Tera-SpaceCom SEC network is an NP-hard mixed-integer nonlinear programming problem (MINLP), due to the discrete nature of space exploration tasks and sub-arrays as well as the continuous nature of transmit power. To tackle this challenge, a graph neural network (GNN)-deep reinforcement learning (DRL)-based joint resource allocation and task offloading (GRANT) algorithm is proposed with the target of long-term resource efficiency (RE). Particularly, GNNs learn relationships among different satellites from their connectivity information. Furthermore, multi-agent and multi-task mechanisms cooperatively train task offloading and resource allocation. Compared with benchmark solutions, GRANT not only achieves the highest RE with relatively low latency, but realizes the fewest trainable parameters and the shortest running time.





## Stochastic Principal-Agent Problems: Efficient Computation and Learning
- **Url**: http://arxiv.org/abs/2306.03832v3
- **Authors**: ['Jiarui Gan', 'Rupak Majumdar', 'Debmalya Mandal', 'Goran Radanovic']
- **Abstrat**: We introduce a stochastic principal-agent model. A principal and an agent interact in a stochastic environment, each privy to observations about the state not available to the other. The principal has the power of commitment, both to elicit information from the agent and to provide signals about her own information. The players communicate with each other and then select actions independently. Each of them receives a payoff based on the state and their joint action, and the environment transitions to a new state. The interaction continues over a finite time horizon. Both players are far-sighted, aiming to maximize their total payoffs over the time horizon. The model encompasses as special cases extensive-form games (EFGs) and stochastic games of incomplete information, partially observable Markov decision processes (POMDPs), as well as other forms of sequential principal-agent interactions, including Bayesian persuasion and automated mechanism design problems.   We consider both the computation and learning of the principal's optimal policy. Since the general problem, which subsumes POMDPs, is intractable, we explore algorithmic solutions under hindsight observability, where the state and the interaction history are revealed at the end of each step. Though the problem becomes more amenable under this condition, the number of possible histories remains exponential in the length of the time horizon, making approaches for EFG-based models infeasible. We present an efficient algorithm based on the inducible value sets. The algorithm computes an $\epsilon$-approximate optimal policy in time polynomial in $1/\epsilon$. Additionally, we show an efficient learning algorithm for an episodic reinforcement learning setting where the transition probabilities are unknown. The algorithm guarantees sublinear regret $\tilde{O}(T^{2/3})$ for both players over $T$ episodes.





## GOPT: Generalizable Online 3D Bin Packing via Transformer-based Deep Reinforcement Learning
- **Url**: http://arxiv.org/abs/2409.05344v2
- **Authors**: ['Heng Xiong', 'Changrong Guo', 'Jian Peng', 'Kai Ding', 'Wenjie Chen', 'Xuchong Qiu', 'Long Bai', 'Jianfeng Xu']
- **Abstrat**: Robotic object packing has broad practical applications in the logistics and automation industry, often formulated by researchers as the online 3D Bin Packing Problem (3D-BPP). However, existing DRL-based methods primarily focus on enhancing performance in limited packing environments while neglecting the ability to generalize across multiple environments characterized by different bin dimensions. To this end, we propose GOPT, a generalizable online 3D Bin Packing approach via Transformer-based deep reinforcement learning (DRL). First, we design a Placement Generator module to yield finite subspaces as placement candidates and the representation of the bin. Second, we propose a Packing Transformer, which fuses the features of the items and bin, to identify the spatial correlation between the item to be packed and available sub-spaces within the bin. Coupling these two components enables GOPT's ability to perform inference on bins of varying dimensions. We conduct extensive experiments and demonstrate that GOPT not only achieves superior performance against the baselines, but also exhibits excellent generalization capabilities. Furthermore, the deployment with a robot showcases the practical applicability of our method in the real world. The source code will be publicly available at https://github.com/Xiong5Heng/GOPT.





## Learning Skateboarding for Humanoid Robots through Massively Parallel Reinforcement Learning
- **Url**: http://arxiv.org/abs/2409.07846v1
- **Authors**: ['William Thibault', 'Vidyasagar Rajendran', 'William Melek', 'Katja Mombaur']
- **Abstrat**: Learning-based methods have proven useful at generating complex motions for robots, including humanoids. Reinforcement learning (RL) has been used to learn locomotion policies, some of which leverage a periodic reward formulation. This work extends the periodic reward formulation of locomotion to skateboarding for the REEM-C robot. Brax/MJX is used to implement the RL problem to achieve fast training. Initial results in simulation are presented with hardware experiments in progress.





# TD3
# Prioritized Experience Replay
# path planning
## Multi-period railway line planning for integrated passenger-freight transportation
- **Url**: http://arxiv.org/abs/2409.08256v1
- **Authors**: ['Wanru Chen', 'Rolf N. van Lieshout', 'Dezhi Zhang', 'Tom Van Woensel']
- **Abstrat**: This paper addresses a multi-period line planning problem in an integrated passenger-freight railway system, aiming to maximize profit while serving passengers and freight using a combination of dedicated passenger trains, dedicated freight trains, and mixed trains. To accommodate demand with different time sensitivities, we develop a period-extended change&go-network that tracks the paths taken by passengers and freight. The problem is formulated as a path-based mixed integer programming model, with the linear relaxation solved using column generation. Paths for passengers and freight are dynamically generated by solving pricing problems defined as elementary shortest-path problems with duration constraints. We propose two heuristic approaches: price-and-branch and a diving heuristic, with acceleration strategies, to find integer feasible solutions efficiently. Computational experiments on the Chinese high-speed railway network demonstrate that the diving heuristic outperforms the price-and-branch heuristic in both computational time and solution quality. Additionally, the experiments highlight the benefits of integrating freight, the advantages of multi-period line planning, and the impact of different demand patterns on line operations.




