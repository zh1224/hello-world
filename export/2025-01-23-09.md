# reinforcement learning
## Which Sensor to Observe? Timely Tracking of a Joint Markov Source with Model Predictive Control
- **Url**: http://arxiv.org/abs/2501.13099v1
- **Authors**: ['Ismail Cosandal', 'Sennur Ulukus', 'Nail Akar']
- **Abstrat**: In this paper, we investigate the problem of remote estimation of a discrete-time joint Markov process using multiple sensors. Each sensor observes a different component of the joint Markov process, and in each time slot, the monitor obtains a partial state value by sending a pull request to one of the sensors. The monitor chooses the sequence of sensors to observe with the goal of minimizing the mean of age of incorrect information (MAoII) by using the partial state observations obtained, which have different freshness levels. For instance, a monitor may be interested in tracking the location of an object by obtaining observations from two sensors, which observe the $x$ and $y$ coordinates of the object separately, in different time slots. The monitor, then, needs to decide which coordinate to observe in the next time slot given the history. In addition to this partial observability of the state of Markov process, there is an erasure channel with a fixed one-slot delay between each sensor and the monitor. First, we obtain a sufficient statistic, namely the \emph{belief}, representing the joint distribution of the age of incorrect information (AoII) and the current state of the observed process by using the history of all pull requests and observations. Then, we formulate the problem with a continuous state-space Markov decision problem (MDP), namely belief MDP. To solve the problem, we propose two model predictive control (MPC) methods, namely MPC without terminal costs (MPC-WTC) and reinforcement learning MPC (RL-MPC), that have different advantages in implementation.





## Attention-Driven Hierarchical Reinforcement Learning with Particle Filtering for Source Localization in Dynamic Fields
- **Url**: http://arxiv.org/abs/2501.13084v1
- **Authors**: ['Yiwei Shi', 'Mengyue Yang', 'Qi Zhang', 'Weinan Zhang', 'Cunjia Liu', 'Weiru Liu']
- **Abstrat**: In many real-world scenarios, such as gas leak detection or environmental pollutant tracking, solving the Inverse Source Localization and Characterization problem involves navigating complex, dynamic fields with sparse and noisy observations. Traditional methods face significant challenges, including partial observability, temporal and spatial dynamics, out-of-distribution generalization, and reward sparsity. To address these issues, we propose a hierarchical framework that integrates Bayesian inference and reinforcement learning. The framework leverages an attention-enhanced particle filtering mechanism for efficient and accurate belief updates, and incorporates two complementary execution strategies: Attention Particle Filtering Planning and Attention Particle Filtering Reinforcement Learning. These approaches optimize exploration and adaptation under uncertainty. Theoretical analysis proves the convergence of the attention-enhanced particle filter, while extensive experiments across diverse scenarios validate the framework's superior accuracy, adaptability, and computational efficiency. Our results highlight the framework's potential for broad applications in dynamic field estimation tasks.





## Evolution and The Knightian Blindspot of Machine Learning
- **Url**: http://arxiv.org/abs/2501.13075v1
- **Authors**: ['Joel Lehman', 'Elliot Meyerson', 'Tarek El-Gaaly', 'Kenneth O. Stanley', 'Tarin Ziyaee']
- **Abstrat**: This paper claims that machine learning (ML) largely overlooks an important facet of general intelligence: robustness to a qualitatively unknown future in an open world. Such robustness relates to Knightian uncertainty (KU) in economics, i.e. uncertainty that cannot be quantified, which is excluded from consideration in ML's key formalisms. This paper aims to identify this blind spot, argue its importance, and catalyze research into addressing it, which we believe is necessary to create truly robust open-world AI. To help illuminate the blind spot, we contrast one area of ML, reinforcement learning (RL), with the process of biological evolution. Despite staggering ongoing progress, RL still struggles in open-world situations, often failing under unforeseen situations. For example, the idea of zero-shot transferring a self-driving car policy trained only in the US to the UK currently seems exceedingly ambitious. In dramatic contrast, biological evolution routinely produces agents that thrive within an open world, sometimes even to situations that are remarkably out-of-distribution (e.g. invasive species; or humans, who do undertake such zero-shot international driving). Interestingly, evolution achieves such robustness without explicit theory, formalisms, or mathematical gradients. We explore the assumptions underlying RL's typical formalisms, showing how they limit RL's engagement with the unknown unknowns characteristic of an ever-changing complex world. Further, we identify mechanisms through which evolutionary processes foster robustness to novel and unpredictable challenges, and discuss potential pathways to algorithmically embody them. The conclusion is that the intriguing remaining fragility of ML may result from blind spots in its formalisms, and that significant gains may result from direct confrontation with the challenge of KU.





## AdaWM: Adaptive World Model based Planning for Autonomous Driving
- **Url**: http://arxiv.org/abs/2501.13072v1
- **Authors**: ['Hang Wang', 'Xin Ye', 'Feng Tao', 'Abhirup Mallik', 'Burhaneddin Yaman', 'Liu Ren', 'Junshan Zhang']
- **Abstrat**: World model based reinforcement learning (RL) has emerged as a promising approach for autonomous driving, which learns a latent dynamics model and uses it to train a planning policy. To speed up the learning process, the pretrain-finetune paradigm is often used, where online RL is initialized by a pretrained model and a policy learned offline. However, naively performing such initialization in RL may result in dramatic performance degradation during the online interactions in the new task. To tackle this challenge, we first analyze the performance degradation and identify two primary root causes therein: the mismatch of the planning policy and the mismatch of the dynamics model, due to distribution shift. We further analyze the effects of these factors on performance degradation during finetuning, and our findings reveal that the choice of finetuning strategies plays a pivotal role in mitigating these effects. We then introduce AdaWM, an Adaptive World Model based planning method, featuring two key steps: (a) mismatch identification, which quantifies the mismatches and informs the finetuning strategy, and (b) alignment-driven finetuning, which selectively updates either the policy or the model as needed using efficient low-rank updates. Extensive experiments on the challenging CARLA driving tasks demonstrate that AdaWM significantly improves the finetuning process, resulting in more robust and efficient performance in autonomous driving systems.





## Reasoning Language Models: A Blueprint
- **Url**: http://arxiv.org/abs/2501.11223v2
- **Authors**: ['Maciej Besta', 'Julia Barth', 'Eric Schreiber', 'Ales Kubicek', 'Afonso Catarino', 'Robert Gerstenberger', 'Piotr Nyczyk', 'Patrick Iff', 'Yueling Li', 'Sam Houliston', 'Tomasz Sternal', 'Marcin Copik', 'Grzegorz Kwaśniewski', 'Jürgen Müller', 'Łukasz Flis', 'Hannes Eberhard', 'Hubert Niewiadomski', 'Torsten Hoefler']
- **Abstrat**: Reasoning language models (RLMs), also known as Large Reasoning Models (LRMs), such as OpenAI's o1 and o3, DeepSeek-V3, and Alibaba's QwQ, have redefined AI's problem-solving capabilities by extending LLMs with advanced reasoning mechanisms. Yet, their high costs, proprietary nature, and complex architectures - uniquely combining Reinforcement Learning (RL), search heuristics, and LLMs - present accessibility and scalability challenges. To address these, we propose a comprehensive blueprint that organizes RLM components into a modular framework, based on a survey and analysis of all RLM works. This blueprint incorporates diverse reasoning structures (chains, trees, graphs, and nested forms), reasoning strategies (e.g., Monte Carlo Tree Search, Beam Search), RL concepts (policy, value models and others), supervision schemes (Outcome-Based and Process-Based Supervision), and other related concepts (e.g., Test-Time Compute, Retrieval-Augmented Generation, agent tools). We provide detailed mathematical formulations and algorithmic specifications to simplify RLM implementation. By showing how schemes like LLaMA-Berry, QwQ, Journey Learning, and Graph of Thoughts fit as special cases, we demonstrate the blueprint's versatility and unifying potential. To illustrate its utility, we introduce x1, a modular implementation for rapid RLM prototyping and experimentation. Using x1 and a literature review, we provide key insights, such as multi-phase training for policy and value models, and the importance of familiar training distributions. Finally, we discuss scalable RLM cloud deployments and we outline how RLMs can integrate with a broader LLM ecosystem. Our work demystifies RLM construction, democratizes advanced reasoning capabilities, and fosters innovation, aiming to mitigate the gap between "rich AI" and "poor AI" by lowering barriers to RLM development and experimentation.





## Optimizing Return Distributions with Distributional Dynamic Programming
- **Url**: http://arxiv.org/abs/2501.13028v1
- **Authors**: ['Bernardo Ávila Pires', 'Mark Rowland', 'Diana Borsa', 'Zhaohan Daniel Guo', 'Khimya Khetarpal', 'André Barreto', 'David Abel', 'Rémi Munos', 'Will Dabney']
- **Abstrat**: We introduce distributional dynamic programming (DP) methods for optimizing statistical functionals of the return distribution, with standard reinforcement learning as a special case. Previous distributional DP methods could optimize the same class of expected utilities as classic DP. To go beyond expected utilities, we combine distributional DP with stock augmentation, a technique previously introduced for classic DP in the context of risk-sensitive RL, where the MDP state is augmented with a statistic of the rewards obtained so far (since the first time step). We find that a number of recently studied problems can be formulated as stock-augmented return distribution optimization, and we show that we can use distributional DP to solve them. We analyze distributional value and policy iteration, with bounds and a study of what objectives these distributional DP methods can or cannot optimize. We describe a number of applications outlining how to use distributional DP to solve different stock-augmented return distribution optimization problems, for example maximizing conditional value-at-risk, and homeostatic regulation. To highlight the practical potential of stock-augmented return distribution optimization and distributional DP, we combine the core ideas of distributional value iteration with the deep RL agent DQN, and empirically evaluate it for solving instances of the applications discussed.





## MONA: Myopic Optimization with Non-myopic Approval Can Mitigate Multi-step Reward Hacking
- **Url**: http://arxiv.org/abs/2501.13011v1
- **Authors**: ['Sebastian Farquhar', 'Vikrant Varma', 'David Lindner', 'David Elson', 'Caleb Biddulph', 'Ian Goodfellow', 'Rohin Shah']
- **Abstrat**: Future advanced AI systems may learn sophisticated strategies through reinforcement learning (RL) that humans cannot understand well enough to safely evaluate. We propose a training method which avoids agents learning undesired multi-step plans that receive high reward (multi-step "reward hacks") even if humans are not able to detect that the behaviour is undesired. The method, Myopic Optimization with Non-myopic Approval (MONA), works by combining short-sighted optimization with far-sighted reward. We demonstrate that MONA can prevent multi-step reward hacking that ordinary RL causes, even without being able to detect the reward hacking and without any extra information that ordinary RL does not get access to. We study MONA empirically in three settings which model different misalignment failure modes including 2-step environments with LLMs representing delegated oversight and encoded reasoning and longer-horizon gridworld environments representing sensor tampering.





## An Offline Multi-Agent Reinforcement Learning Framework for Radio Resource Management
- **Url**: http://arxiv.org/abs/2501.12991v1
- **Authors**: ['Eslam Eldeeb', 'Hirley Alves']
- **Abstrat**: Offline multi-agent reinforcement learning (MARL) addresses key limitations of online MARL, such as safety concerns, expensive data collection, extended training intervals, and high signaling overhead caused by online interactions with the environment. In this work, we propose an offline MARL algorithm for radio resource management (RRM), focusing on optimizing scheduling policies for multiple access points (APs) to jointly maximize the sum and tail rates of user equipment (UEs). We evaluate three training paradigms: centralized, independent, and centralized training with decentralized execution (CTDE). Our simulation results demonstrate that the proposed offline MARL framework outperforms conventional baseline approaches, achieving over a 15\% improvement in a weighted combination of sum and tail rates. Additionally, the CTDE framework strikes an effective balance, reducing the computational complexity of centralized methods while addressing the inefficiencies of independent training. These results underscore the potential of offline MARL to deliver scalable, robust, and efficient solutions for resource management in dynamic wireless networks.





## Decision Mamba: A Multi-Grained State Space Model with Self-Evolution Regularization for Offline RL
- **Url**: http://arxiv.org/abs/2406.05427v3
- **Authors**: ['Qi Lv', 'Xiang Deng', 'Gongwei Chen', 'Michael Yu Wang', 'Liqiang Nie']
- **Abstrat**: While the conditional sequence modeling with the transformer architecture has demonstrated its effectiveness in dealing with offline reinforcement learning (RL) tasks, it is struggle to handle out-of-distribution states and actions. Existing work attempts to address this issue by data augmentation with the learned policy or adding extra constraints with the value-based RL algorithm. However, these studies still fail to overcome the following challenges: (1) insufficiently utilizing the historical temporal information among inter-steps, (2) overlooking the local intrastep relationships among return-to-gos (RTGs), states, and actions, (3) overfitting suboptimal trajectories with noisy labels. To address these challenges, we propose Decision Mamba (DM), a novel multi-grained state space model (SSM) with a self-evolving policy learning strategy. DM explicitly models the historical hidden state to extract the temporal information by using the mamba architecture. To capture the relationship among RTG-state-action triplets, a fine-grained SSM module is designed and integrated into the original coarse-grained SSM in mamba, resulting in a novel mamba architecture tailored for offline RL. Finally, to mitigate the overfitting issue on noisy trajectories, a self-evolving policy is proposed by using progressive regularization. The policy evolves by using its own past knowledge to refine the suboptimal actions, thus enhancing its robustness on noisy demonstrations. Extensive experiments on various tasks show that DM outperforms other baselines substantially.





## DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning
- **Url**: http://arxiv.org/abs/2501.12948v1
- **Authors**: ['DeepSeek-AI', 'Daya Guo', 'Dejian Yang', 'Haowei Zhang', 'Junxiao Song', 'Ruoyu Zhang', 'Runxin Xu', 'Qihao Zhu', 'Shirong Ma', 'Peiyi Wang', 'Xiao Bi', 'Xiaokang Zhang', 'Xingkai Yu', 'Yu Wu', 'Z. F. Wu', 'Zhibin Gou', 'Zhihong Shao', 'Zhuoshu Li', 'Ziyi Gao', 'Aixin Liu', 'Bing Xue', 'Bingxuan Wang', 'Bochao Wu', 'Bei Feng', 'Chengda Lu', 'Chenggang Zhao', 'Chengqi Deng', 'Chenyu Zhang', 'Chong Ruan', 'Damai Dai', 'Deli Chen', 'Dongjie Ji', 'Erhang Li', 'Fangyun Lin', 'Fucong Dai', 'Fuli Luo', 'Guangbo Hao', 'Guanting Chen', 'Guowei Li', 'H. Zhang', 'Han Bao', 'Hanwei Xu', 'Haocheng Wang', 'Honghui Ding', 'Huajian Xin', 'Huazuo Gao', 'Hui Qu', 'Hui Li', 'Jianzhong Guo', 'Jiashi Li', 'Jiawei Wang', 'Jingchang Chen', 'Jingyang Yuan', 'Junjie Qiu', 'Junlong Li', 'J. L. Cai', 'Jiaqi Ni', 'Jian Liang', 'Jin Chen', 'Kai Dong', 'Kai Hu', 'Kaige Gao', 'Kang Guan', 'Kexin Huang', 'Kuai Yu', 'Lean Wang', 'Lecong Zhang', 'Liang Zhao', 'Litong Wang', 'Liyue Zhang', 'Lei Xu', 'Leyi Xia', 'Mingchuan Zhang', 'Minghua Zhang', 'Minghui Tang', 'Meng Li', 'Miaojun Wang', 'Mingming Li', 'Ning Tian', 'Panpan Huang', 'Peng Zhang', 'Qiancheng Wang', 'Qinyu Chen', 'Qiushi Du', 'Ruiqi Ge', 'Ruisong Zhang', 'Ruizhe Pan', 'Runji Wang', 'R. J. Chen', 'R. L. Jin', 'Ruyi Chen', 'Shanghao Lu', 'Shangyan Zhou', 'Shanhuang Chen', 'Shengfeng Ye', 'Shiyu Wang', 'Shuiping Yu', 'Shunfeng Zhou', 'Shuting Pan', 'S. S. Li', 'Shuang Zhou', 'Shaoqing Wu', 'Shengfeng Ye', 'Tao Yun', 'Tian Pei', 'Tianyu Sun', 'T. Wang', 'Wangding Zeng', 'Wanjia Zhao', 'Wen Liu', 'Wenfeng Liang', 'Wenjun Gao', 'Wenqin Yu', 'Wentao Zhang', 'W. L. Xiao', 'Wei An', 'Xiaodong Liu', 'Xiaohan Wang', 'Xiaokang Chen', 'Xiaotao Nie', 'Xin Cheng', 'Xin Liu', 'Xin Xie', 'Xingchao Liu', 'Xinyu Yang', 'Xinyuan Li', 'Xuecheng Su', 'Xuheng Lin', 'X. Q. Li', 'Xiangyue Jin', 'Xiaojin Shen', 'Xiaosha Chen', 'Xiaowen Sun', 'Xiaoxiang Wang', 'Xinnan Song', 'Xinyi Zhou', 'Xianzu Wang', 'Xinxia Shan', 'Y. K. Li', 'Y. Q. Wang', 'Y. X. Wei', 'Yang Zhang', 'Yanhong Xu', 'Yao Li', 'Yao Zhao', 'Yaofeng Sun', 'Yaohui Wang', 'Yi Yu', 'Yichao Zhang', 'Yifan Shi', 'Yiliang Xiong', 'Ying He', 'Yishi Piao', 'Yisong Wang', 'Yixuan Tan', 'Yiyang Ma', 'Yiyuan Liu', 'Yongqiang Guo', 'Yuan Ou', 'Yuduan Wang', 'Yue Gong', 'Yuheng Zou', 'Yujia He', 'Yunfan Xiong', 'Yuxiang Luo', 'Yuxiang You', 'Yuxuan Liu', 'Yuyang Zhou', 'Y. X. Zhu', 'Yanhong Xu', 'Yanping Huang', 'Yaohui Li', 'Yi Zheng', 'Yuchen Zhu', 'Yunxian Ma', 'Ying Tang', 'Yukun Zha', 'Yuting Yan', 'Z. Z. Ren', 'Zehui Ren', 'Zhangli Sha', 'Zhe Fu', 'Zhean Xu', 'Zhenda Xie', 'Zhengyan Zhang', 'Zhewen Hao', 'Zhicheng Ma', 'Zhigang Yan', 'Zhiyu Wu', 'Zihui Gu', 'Zijia Zhu', 'Zijun Liu', 'Zilin Li', 'Ziwei Xie', 'Ziyang Song', 'Zizheng Pan', 'Zhen Huang', 'Zhipeng Xu', 'Zhongyu Zhang', 'Zhen Zhang']
- **Abstrat**: We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.





## Offline Critic-Guided Diffusion Policy for Multi-User Delay-Constrained Scheduling
- **Url**: http://arxiv.org/abs/2501.12942v1
- **Authors**: ['Zhuoran Li', 'Ruishuo Chen', 'Hai Zhong', 'Longbo Huang']
- **Abstrat**: Effective multi-user delay-constrained scheduling is crucial in various real-world applications, such as instant messaging, live streaming, and data center management. In these scenarios, schedulers must make real-time decisions to satisfy both delay and resource constraints without prior knowledge of system dynamics, which are often time-varying and challenging to estimate. Current learning-based methods typically require interactions with actual systems during the training stage, which can be difficult or impractical, as it is capable of significantly degrading system performance and incurring substantial service costs. To address these challenges, we propose a novel offline reinforcement learning-based algorithm, named \underline{S}cheduling By \underline{O}ffline Learning with \underline{C}ritic Guidance and \underline{D}iffusion Generation (SOCD), to learn efficient scheduling policies purely from pre-collected \emph{offline data}. SOCD innovatively employs a diffusion-based policy network, complemented by a sampling-free critic network for policy guidance. By integrating the Lagrangian multiplier optimization into the offline reinforcement learning, SOCD effectively trains high-quality constraint-aware policies exclusively from available datasets, eliminating the need for online interactions with the system. Experimental results demonstrate that SOCD is resilient to various system dynamics, including partially observable and large-scale environments, and delivers superior performance compared to existing methods.





## Yi-Lightning Technical Report
- **Url**: http://arxiv.org/abs/2412.01253v5
- **Authors**: ['Alan Wake', 'Bei Chen', 'C. X. Lv', 'Chao Li', 'Chengen Huang', 'Chenglin Cai', 'Chujie Zheng', 'Daniel Cooper', 'Fan Zhou', 'Feng Hu', 'Ge Zhang', 'Guoyin Wang', 'Heng Ji', 'Howard Qiu', 'Jiangcheng Zhu', 'Jun Tian', 'Katherine Su', 'Lihuan Zhang', 'Liying Li', 'Ming Song', 'Mou Li', 'Peng Liu', 'Qicheng Hu', 'Shawn Wang', 'Shijun Zhou', 'Shiming Yang', 'Shiyong Li', 'Tianhang Zhu', 'Wen Xie', 'Wenhao Huang', 'Xiang He', 'Xiaobo Chen', 'Xiaohui Hu', 'Xiaoyi Ren', 'Xinyao Niu', 'Yanpeng Li', 'Yongke Zhao', 'Yongzhen Luo', 'Yuchi Xu', 'Yuxuan Sha', 'Zhaodong Yan', 'Zhiyuan Liu', 'Zirui Zhang', 'Zonghong Dai']
- **Abstrat**: This technical report presents Yi-Lightning, our latest flagship large language model (LLM). It achieves exceptional performance, ranking 6th overall on Chatbot Arena, with particularly strong results (2nd to 4th place) in specialized categories including Chinese, Math, Coding, and Hard Prompts. Yi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture, featuring advanced expert segmentation and routing mechanisms coupled with optimized KV-caching techniques. Our development process encompasses comprehensive pre-training, supervised fine-tuning (SFT), and reinforcement learning from human feedback (RLHF), where we devise deliberate strategies for multi-stage training, synthetic data construction, and reward modeling. Furthermore, we implement RAISE (Responsible AI Safety Engine), a four-component framework to address safety issues across pre-training, post-training, and serving phases. Empowered by our scalable super-computing infrastructure, all these innovations substantially reduce training, deployment and inference costs while maintaining high-performance standards. With further evaluations on public academic benchmarks, Yi-Lightning demonstrates competitive performance against top-tier LLMs, while we observe a notable disparity between traditional, static benchmark results and real-world, dynamic human preferences. This observation prompts a critical reassessment of conventional benchmarks' utility in guiding the development of more intelligent and powerful AI systems for practical applications. Yi-Lightning is now available through our developer platform at https://platform.lingyiwanwu.com.





## Reinforcement learning Based Automated Design of Differential Evolution Algorithm for Black-box Optimization
- **Url**: http://arxiv.org/abs/2501.12881v1
- **Authors**: ['Xu Yang', 'Rui Wang', 'Kaiwen Li', 'Ling Wang']
- **Abstrat**: Differential evolution (DE) algorithm is recognized as one of the most effective evolutionary algorithms, demonstrating remarkable efficacy in black-box optimization due to its derivative-free nature. Numerous enhancements to the fundamental DE have been proposed, incorporating innovative mutation strategies and sophisticated parameter tuning techniques to improve performance. However, no single variant has proven universally superior across all problems. To address this challenge, we introduce a novel framework that employs reinforcement learning (RL) to automatically design DE for black-box optimization through meta-learning. RL acts as an advanced meta-optimizer, generating a customized DE configuration that includes an optimal initialization strategy, update rule, and hyperparameters tailored to a specific black-box optimization problem. This process is informed by a detailed analysis of the problem characteristics. In this proof-of-concept study, we utilize a double deep Q-network for implementation, considering a subset of 40 possible strategy combinations and parameter optimizations simultaneously. The framework's performance is evaluated against black-box optimization benchmarks and compared with state-of-the-art algorithms. The experimental results highlight the promising potential of our proposed framework.





## Entropy Regularized Task Representation Learning for Offline Meta-Reinforcement Learning
- **Url**: http://arxiv.org/abs/2412.14834v2
- **Authors**: ['Mohammadreza Nakhaei', 'Aidan Scannell', 'Joni Pajarinen']
- **Abstrat**: Offline meta-reinforcement learning aims to equip agents with the ability to rapidly adapt to new tasks by training on data from a set of different tasks. Context-based approaches utilize a history of state-action-reward transitions -- referred to as the context -- to infer representations of the current task, and then condition the agent, i.e., the policy and value function, on the task representations. Intuitively, the better the task representations capture the underlying tasks, the better the agent can generalize to new tasks. Unfortunately, context-based approaches suffer from distribution mismatch, as the context in the offline data does not match the context at test time, limiting their ability to generalize to the test tasks. This leads to the task representations overfitting to the offline training data. Intuitively, the task representations should be independent of the behavior policy used to collect the offline data. To address this issue, we approximately minimize the mutual information between the distribution over the task representations and behavior policy by maximizing the entropy of behavior policy conditioned on the task representations. We validate our approach in MuJoCo environments, showing that compared to baselines, our task representations more faithfully represent the underlying tasks, leading to outperforming prior methods in both in-distribution and out-of-distribution tasks.





## To Measure or Not: A Cost-Sensitive, Selective Measuring Environment for Agricultural Management Decisions with Reinforcement Learning
- **Url**: http://arxiv.org/abs/2501.12823v1
- **Authors**: ['Hilmy Baja', 'Michiel Kallenberg', 'Ioannis N. Athanasiadis']
- **Abstrat**: Farmers rely on in-field observations to make well-informed crop management decisions to maximize profit and minimize adverse environmental impact. However, obtaining real-world crop state measurements is labor-intensive, time-consuming and expensive. In most cases, it is not feasible to gather crop state measurements before every decision moment. Moreover, in previous research pertaining to farm management optimization, these observations are often assumed to be readily available without any cost, which is unrealistic. Hence, enabling optimization without the need to have temporally complete crop state observations is important. An approach to that problem is to include measuring as part of decision making. As a solution, we apply reinforcement learning (RL) to recommend opportune moments to simultaneously measure crop features and apply nitrogen fertilizer. With realistic considerations, we design an RL environment with explicit crop feature measuring costs. While balancing costs, we find that an RL agent, trained with recurrent PPO, discovers adaptive measuring policies that follow critical crop development stages, with results aligned by what domain experts would consider a sensible approach. Our results highlight the importance of measuring when crop feature measurements are not readily available.





## On Generalization and Distributional Update for Mimicking Observations with Adequate Exploration
- **Url**: http://arxiv.org/abs/2501.12785v1
- **Authors**: ['Yirui Zhou', 'Xiaowei Liu', 'Xiaofeng Zhang', 'Yangchun Zhang']
- **Abstrat**: This paper tackles the efficiency and stability issues in learning from observations (LfO). We commence by investigating how reward functions and policies generalize in LfO. Subsequently, the built-in reinforcement learning (RL) approach in generative adversarial imitation from observation (GAIfO) is replaced with distributional soft actor-critic (DSAC). This change results in a novel algorithm called Mimicking Observations through Distributional Update Learning with adequate Exploration (MODULE), which combines soft actor-critic's superior efficiency with distributional RL's robust stability.





## Cost Optimization for Serverless Edge Computing with Budget Constraints using Deep Reinforcement Learning
- **Url**: http://arxiv.org/abs/2501.12783v1
- **Authors**: ['Chen Chen', 'Peiyuan Guan', 'Ziru Chen', 'Amir Taherkordi', 'Fen Hou', 'Lin X. Cai']
- **Abstrat**: Serverless computing adopts a pay-as-you-go billing model where applications are executed in stateless and shortlived containers triggered by events, resulting in a reduction of monetary costs and resource utilization. However, existing platforms do not provide an upper bound for the billing model which makes the overall cost unpredictable, precluding many organizations from managing their budgets. Due to the diverse ranges of serverless functions and the heterogeneous capacity of edge devices, it is challenging to receive near-optimal solutions for deployment cost in a polynomial time. In this paper, we investigated the function scheduling problem with a budget constraint for serverless computing in wireless networks. Users and IoT devices are sending requests to edge nodes, improving the latency perceived by users. We propose two online scheduling algorithms based on reinforcement learning, incorporating several important characteristics of serverless functions. Via extensive simulations, we justify the superiority of the proposed algorithm by comparing with an ILP solver (Midaco). Our results indicate that the proposed algorithms efficiently approximate the results of Midaco within a factor of 1.03 while our decision-making time is 5 orders of magnitude less than that of Midaco.





## Optimal Sequential Decision-Making in Geosteering: A Reinforcement Learning Approach
- **Url**: http://arxiv.org/abs/2310.04772v2
- **Authors**: ['Ressi Bonti Muhammad', 'Sergey Alyaev', 'Reidar Brumer Bratvold']
- **Abstrat**: Trajectory adjustment decisions throughout the drilling process, called geosteering, affect subsequent choices and information gathering, thus resulting in a coupled sequential decision problem. Previous works on applying decision optimization methods in geosteering rely on greedy optimization or approximate dynamic programming (ADP). Either decision optimization method requires explicit uncertainty and objective function models, making developing decision optimization methods for complex and realistic geosteering environments challenging to impossible. We use the Deep Q-Network (DQN) method, a model-free reinforcement learning (RL) method that learns directly from the decision environment, to optimize geosteering decisions. The expensive computations for RL are handled during the offline training stage. Evaluating DQN needed for real-time decision support takes milliseconds and is faster than the traditional alternatives. Moreover, for two previously published synthetic geosteering scenarios, our results show that RL achieves high-quality outcomes comparable to the quasi-optimal ADP. Yet, the model-free nature of RL means that by replacing the training environment, we can extend it to problems where the solution to ADP is prohibitively expensive to compute. This flexibility will allow applying it to more complex environments and make hybrid versions trained with real data in the future.





## Bad-PFL: Exploring Backdoor Attacks against Personalized Federated Learning
- **Url**: http://arxiv.org/abs/2501.12736v1
- **Authors**: ['Mingyuan Fan', 'Zhanyi Hu', 'Fuyi Wang', 'Cen Chen']
- **Abstrat**: Data heterogeneity and backdoor attacks rank among the most significant challenges facing federated learning (FL). For data heterogeneity, personalized federated learning (PFL) enables each client to maintain a private personalized model to cater to client-specific knowledge. Meanwhile, vanilla FL has proven vulnerable to backdoor attacks. However, recent advancements in PFL community have demonstrated a potential immunity against such attacks. This paper explores this intersection further, revealing that existing federated backdoor attacks fail in PFL because backdoors about manually designed triggers struggle to survive in personalized models. To tackle this, we design Bad-PFL, which employs features from natural data as our trigger. As long as the model is trained on natural data, it inevitably embeds the backdoor associated with our trigger, ensuring its longevity in personalized models. Moreover, our trigger undergoes mutual reinforcement training with the model, further solidifying the backdoor's durability and enhancing attack effectiveness. The large-scale experiments across three benchmark datasets demonstrate the superior performance of our attack against various PFL methods, even when equipped with state-of-the-art defense mechanisms.





# TD3
# Prioritized Experience Replay
# path planning
## Int2Planner: An Intention-based Multi-modal Motion Planner for Integrated Prediction and Planning
- **Url**: http://arxiv.org/abs/2501.12799v1
- **Authors**: ['Xiaolei Chen', 'Junchi Yan', 'Wenlong Liao', 'Tao He', 'Pai Peng']
- **Abstrat**: Motion planning is a critical module in autonomous driving, with the primary challenge of uncertainty caused by interactions with other participants. As most previous methods treat prediction and planning as separate tasks, it is difficult to model these interactions. Furthermore, since the route path navigates ego vehicles to a predefined destination, it provides relatively stable intentions for ego vehicles and helps constrain uncertainty. On this basis, we construct Int2Planner, an \textbf{Int}ention-based \textbf{Int}egrated motion \textbf{Planner} achieves multi-modal planning and prediction. Instead of static intention points, Int2Planner utilizes route intention points for ego vehicles and generates corresponding planning trajectories for each intention point to facilitate multi-modal planning. The experiments on the private dataset and the public nuPlan benchmark show the effectiveness of route intention points, and Int2Planner achieves state-of-the-art performance. We also deploy it in real-world vehicles and have conducted autonomous driving for hundreds of kilometers in urban areas. It further verifies that Int2Planner can continuously interact with the traffic environment. Code will be avaliable at https://github.com/cxlz/Int2Planner.




