# reinforcement learning
## Coarse-to-fine Q-Network with Action Sequence for Data-Efficient Robot Learning
- **Url**: http://arxiv.org/abs/2411.12155v2
- **Authors**: ['Younggyo Seo', 'Pieter Abbeel']
- **Abstrat**: In reinforcement learning (RL), we train a value function to understand the long-term consequence of executing a single action. However, the value of taking each action can be ambiguous in robotics as robot movements are typically the aggregate result of executing multiple small actions. Moreover, robotic training data often consists of noisy trajectories, in which each action is noisy but executing a series of actions results in a meaningful robot movement. This further makes it difficult for the value function to understand the effect of individual actions. To address this, we introduce Coarse-to-fine Q-Network with Action Sequence (CQN-AS), a novel value-based RL algorithm that learns a critic network that outputs Q-values over a sequence of actions, i.e., explicitly training the value function to learn the consequence of executing action sequences. We study our algorithm on 53 robotic tasks with sparse and dense rewards, as well as with and without demonstrations, from BiGym, HumanoidBench, and RLBench. We find that CQN-AS outperforms various baselines, in particular on humanoid control tasks.





## Tulu 3: Pushing Frontiers in Open Language Model Post-Training
- **Url**: http://arxiv.org/abs/2411.15124v3
- **Authors**: ['Nathan Lambert', 'Jacob Morrison', 'Valentina Pyatkin', 'Shengyi Huang', 'Hamish Ivison', 'Faeze Brahman', 'Lester James V. Miranda', 'Alisa Liu', 'Nouha Dziri', 'Shane Lyu', 'Yuling Gu', 'Saumya Malik', 'Victoria Graf', 'Jena D. Hwang', 'Jiangjiang Yang', 'Ronan Le Bras', 'Oyvind Tafjord', 'Chris Wilhelm', 'Luca Soldaini', 'Noah A. Smith', 'Yizhong Wang', 'Pradeep Dasigi', 'Hannaneh Hajishirzi']
- **Abstrat**: Language model post-training is applied to refine behaviors and unlock new skills across a wide range of recent language models, but open recipes for applying these techniques lag behind proprietary ones. The underlying training data and recipes for post-training are simultaneously the most important pieces of the puzzle and the portion with the least transparency. To bridge this gap, we introduce Tulu 3, a family of fully-open state-of-the-art post-trained models, alongside its data, code, and training recipes, serving as a comprehensive guide for modern post-training techniques. Tulu 3, which builds on Llama 3.1 base models, achieves results surpassing the instruct versions of Llama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini and Claude 3.5-Haiku. The training algorithms for our models include supervised finetuning (SFT), Direct Preference Optimization (DPO), and a novel method we call Reinforcement Learning with Verifiable Rewards (RLVR). With Tulu 3, we introduce a multi-task evaluation scheme for post-training recipes with development and unseen evaluations, standard benchmark implementations, and substantial decontamination of existing open datasets on said benchmarks. We conclude with analysis and discussion of training methods that did not reliably improve performance.   In addition to the Tulu 3 model weights and demo, we release the complete recipe -- including datasets for diverse core skills, a robust toolkit for data curation and evaluation, the training code and infrastructure, and, most importantly, a detailed report for reproducing and further adapting the Tulu 3 approach to more domains.





## From Sparse to Dense: Toddler-inspired Reward Transition in Goal-Oriented Reinforcement Learning
- **Url**: http://arxiv.org/abs/2501.17842v1
- **Authors**: ['Junseok Park', 'Hyeonseo Yang', 'Min Whoo Lee', 'Won-Seok Choi', 'Minsu Lee', 'Byoung-Tak Zhang']
- **Abstrat**: Reinforcement learning (RL) agents often face challenges in balancing exploration and exploitation, particularly in environments where sparse or dense rewards bias learning. Biological systems, such as human toddlers, naturally navigate this balance by transitioning from free exploration with sparse rewards to goal-directed behavior guided by increasingly dense rewards. Inspired by this natural progression, we investigate the Toddler-Inspired Reward Transition in goal-oriented RL tasks. Our study focuses on transitioning from sparse to potential-based dense (S2D) rewards while preserving optimal strategies. Through experiments on dynamic robotic arm manipulation and egocentric 3D navigation tasks, we demonstrate that effective S2D reward transitions significantly enhance learning performance and sample efficiency. Additionally, using a Cross-Density Visualizer, we show that S2D transitions smooth the policy loss landscape, resulting in wider minima that improve generalization in RL models. In addition, we reinterpret Tolman's maze experiments, underscoring the critical role of early free exploratory learning in the context of S2D rewards.





## Look-ahead Search on Top of Policy Networks in Imperfect Information Games
- **Url**: http://arxiv.org/abs/2312.15220v3
- **Authors**: ['Ondrej Kubicek', 'Neil Burch', 'Viliam Lisy']
- **Abstrat**: Search in test time is often used to improve the performance of reinforcement learning algorithms. Performing theoretically sound search in fully adversarial two-player games with imperfect information is notoriously difficult and requires a complicated training process. We present a method for adding test-time search to an arbitrary policy-gradient algorithm that learns from sampled trajectories. Besides the policy network, the algorithm trains an additional critic network, which estimates the expected values of players following various transformations of the policies given by the policy network. These values are then used for depth-limited search. We show how the values from this critic can create a value function for imperfect information games. Moreover, they can be used to compute the summary statistics necessary to start the search from an arbitrary decision point in the game. The presented algorithm is scalable to very large games since it does not require any search during train time. We evaluate the algorithm's performance when trained along Regularized Nash Dynamics, and we evaluate the benefit of using the search in the standard benchmark game of Leduc hold'em, multiple variants of imperfect information Goofspiel, and Battleships.





## Langevin Soft Actor-Critic: Efficient Exploration through Uncertainty-Driven Critic Learning
- **Url**: http://arxiv.org/abs/2501.17827v1
- **Authors**: ['Haque Ishfaq', 'Guangyuan Wang', 'Sami Nur Islam', 'Doina Precup']
- **Abstrat**: Existing actor-critic algorithms, which are popular for continuous control reinforcement learning (RL) tasks, suffer from poor sample efficiency due to lack of principled exploration mechanism within them. Motivated by the success of Thompson sampling for efficient exploration in RL, we propose a novel model-free RL algorithm, Langevin Soft Actor Critic (LSAC), which prioritizes enhancing critic learning through uncertainty estimation over policy optimization. LSAC employs three key innovations: approximate Thompson sampling through distributional Langevin Monte Carlo (LMC) based $Q$ updates, parallel tempering for exploring multiple modes of the posterior of the $Q$ function, and diffusion synthesized state-action samples regularized with $Q$ action gradients. Our extensive experiments demonstrate that LSAC outperforms or matches the performance of mainstream model-free RL algorithms for continuous control tasks. Notably, LSAC marks the first successful application of an LMC based Thompson sampling in continuous control tasks with continuous action spaces.





## Consensus Based Stochastic Control
- **Url**: http://arxiv.org/abs/2501.17801v1
- **Authors**: ['Liyao Lyu', 'Jingrun Chen']
- **Abstrat**: We propose a gradient-free deep reinforcement learning algorithm to solve high-dimensional, finite-horizon stochastic control problems. Although the recently developed deep reinforcement learning framework has achieved great success in solving these problems, direct estimation of policy gradients from Monte Carlo sampling often suffers from high variance. To address this, we introduce the Momentum Consensus-Based Optimization (M-CBO) and Adaptive Momentum Consensus-Based Optimization (Adam-CBO) frameworks. These methods optimize policies using Monte Carlo estimates of the value function, rather than its gradients. Adjustable Gaussian noise supports efficient exploration, helping the algorithm converge to optimal policies in complex, nonconvex environments. Numerical results confirm the accuracy and scalability of our approach across various problem dimensions and show the potential for extension to mean-field control problems. Theoretically, we prove that M-CBO can converge to the optimal policy under some assumptions.





## Attention when you need
- **Url**: http://arxiv.org/abs/2501.07440v2
- **Authors**: ['Lokesh Boominathan', 'Yizhou Chen', 'Matthew McGinley', 'Xaq Pitkow']
- **Abstrat**: Being attentive to task-relevant features can improve task performance, but paying attention comes with its own metabolic cost. Therefore, strategic allocation of attention is crucial in performing the task efficiently. This work aims to understand this strategy. Recently, de Gee et al. conducted experiments involving mice performing an auditory sustained attention-value task. This task required the mice to exert attention to identify whether a high-order acoustic feature was present amid the noise. By varying the trial duration and reward magnitude, the task allows us to investigate how an agent should strategically deploy their attention to maximize their benefits and minimize their costs. In our work, we develop a reinforcement learning-based normative model of the mice to understand how it balances attention cost against its benefits. The model is such that at each moment the mice can choose between two levels of attention and decide when to take costly actions that could obtain rewards. Our model suggests that efficient use of attentional resources involves alternating blocks of high attention with blocks of low attention. In the extreme case where the agent disregards sensory input during low attention states, we see that high attention is used rhythmically. Our model provides evidence about how one should deploy attention as a function of task utility, signal statistics, and how attention affects sensory evidence.





## Segmentation-Aware Generative Reinforcement Network (GRN) for Tissue Layer Segmentation in 3-D Ultrasound Images for Chronic Low-back Pain (cLBP) Assessment
- **Url**: http://arxiv.org/abs/2501.17690v1
- **Authors**: ['Zixue Zeng', 'Xiaoyan Zhao', 'Matthew Cartier', 'Tong Yu', 'Jing Wang', 'Xin Meng', 'Zhiyu Sheng', 'Maryam Satarpour', 'John M Cormack', 'Allison Bean', 'Ryan Nussbaum', 'Maya Maurer', 'Emily Landis-Walkenhorst', 'Dinesh Kumbhare', 'Kang Kim', 'Ajay Wasan', 'Jiantao Pu']
- **Abstrat**: We introduce a novel segmentation-aware joint training framework called generative reinforcement network (GRN) that integrates segmentation loss feedback to optimize both image generation and segmentation performance in a single stage. An image enhancement technique called segmentation-guided enhancement (SGE) is also developed, where the generator produces images tailored specifically for the segmentation model. Two variants of GRN were also developed, including GRN for sample-efficient learning (GRN-SEL) and GRN for semi-supervised learning (GRN-SSL). GRN's performance was evaluated using a dataset of 69 fully annotated 3D ultrasound scans from 29 subjects. The annotations included six anatomical structures: dermis, superficial fat, superficial fascial membrane (SFM), deep fat, deep fascial membrane (DFM), and muscle. Our results show that GRN-SEL with SGE reduces labeling efforts by up to 70% while achieving a 1.98% improvement in the Dice Similarity Coefficient (DSC) compared to models trained on fully labeled datasets. GRN-SEL alone reduces labeling efforts by 60%, GRN-SSL with SGE decreases labeling requirements by 70%, and GRN-SSL alone by 60%, all while maintaining performance comparable to fully supervised models. These findings suggest the effectiveness of the GRN framework in optimizing segmentation performance with significantly less labeled data, offering a scalable and efficient solution for ultrasound image analysis and reducing the burdens associated with data annotation.





## CLIP-Motion: Learning Reward Functions for Robotic Actions Using Consecutive Observations
- **Url**: http://arxiv.org/abs/2311.03485v2
- **Authors**: ['Xuzhe Dang', 'Stefan Edelkamp']
- **Abstrat**: This paper presents a novel method for learning reward functions for robotic motions by harnessing the power of a CLIP-based model. Traditional reward function design often hinges on manual feature engineering, which can struggle to generalize across an array of tasks. Our approach circumvents this challenge by capitalizing on CLIP's capability to process both state features and image inputs effectively. Given a pair of consecutive observations, our model excels in identifying the motion executed between them. We showcase results spanning various robotic activities, such as directing a gripper to a designated target and adjusting the position of a cube. Through experimental evaluations, we underline the proficiency of our method in precisely deducing motion and its promise to enhance reinforcement learning training in the realm of robotics.





## FUSION: A Flexible Unified Simulator for Intelligent Optical Networking
- **Url**: http://arxiv.org/abs/2410.13999v2
- **Authors**: ['Ryan McCann', 'Arash Rezaee', 'Vinod M. Vokkarane']
- **Abstrat**: The increasing demand for flexible and efficient optical networks has led to the development of Software-Defined Elastic Optical Networks (SD-EONs). These networks leverage the programmability of Software-Defined Networking (SDN) and the adaptability of Elastic Optical Networks (EONs) to optimize network performance under dynamic traffic conditions. However, existing simulation tools often fall short in terms of transparency, flexibility, and advanced functionality, limiting their utility in cutting-edge research. In this paper, we present a Flexible Unified Simulator for Intelligent Optical Networking (FUSION), a fully open-source simulator designed to address these limitations and provide a comprehensive platform for SD-EON research. FUSION integrates traditional routing and spectrum assignment algorithms with advanced machine learning and reinforcement learning techniques, including support for the Stable Baselines 3 library. The simulator also offers robust unit testing, a fully functional Graphical User Interface (GUI), and extensive documentation to ensure usability and reliability. Performance evaluations demonstrate the effectiveness of FUSION in modeling complex network scenarios, showcasing its potential as a powerful tool for advancing SD-EON research.





## CAMP in the Odyssey: Provably Robust Reinforcement Learning with Certified Radius Maximization
- **Url**: http://arxiv.org/abs/2501.17667v1
- **Authors**: ['Derui Wang', 'Kristen Moore', 'Diksha Goel', 'Minjune Kim', 'Gang Li', 'Yang Li', 'Robin Doss', 'Minhui Xue', 'Bo Li', 'Seyit Camtepe', 'Liming Zhu']
- **Abstrat**: Deep reinforcement learning (DRL) has gained widespread adoption in control and decision-making tasks due to its strong performance in dynamic environments. However, DRL agents are vulnerable to noisy observations and adversarial attacks, and concerns about the adversarial robustness of DRL systems have emerged. Recent efforts have focused on addressing these robustness issues by establishing rigorous theoretical guarantees for the returns achieved by DRL agents in adversarial settings. Among these approaches, policy smoothing has proven to be an effective and scalable method for certifying the robustness of DRL agents. Nevertheless, existing certifiably robust DRL relies on policies trained with simple Gaussian augmentations, resulting in a suboptimal trade-off between certified robustness and certified return. To address this issue, we introduce a novel paradigm dubbed \texttt{C}ertified-r\texttt{A}dius-\texttt{M}aximizing \texttt{P}olicy (\texttt{CAMP}) training. \texttt{CAMP} is designed to enhance DRL policies, achieving better utility without compromising provable robustness. By leveraging the insight that the global certified radius can be derived from local certified radii based on training-time statistics, \texttt{CAMP} formulates a surrogate loss related to the local certified radius and optimizes the policy guided by this surrogate loss. We also introduce \textit{policy imitation} as a novel technique to stabilize \texttt{CAMP} training. Experimental results demonstrate that \texttt{CAMP} significantly improves the robustness-return trade-off across various tasks. Based on the results, \texttt{CAMP} can achieve up to twice the certified expected return compared to that of baselines. Our code is available at https://github.com/NeuralSec/camp-robust-rl.





## AdaSociety: An Adaptive Environment with Social Structures for Multi-Agent Decision-Making
- **Url**: http://arxiv.org/abs/2411.03865v5
- **Authors**: ['Yizhe Huang', 'Xingbo Wang', 'Hao Liu', 'Fanqi Kong', 'Aoyang Qin', 'Min Tang', 'Song-Chun Zhu', 'Mingjie Bi', 'Siyuan Qi', 'Xue Feng']
- **Abstrat**: Traditional interactive environments limit agents' intelligence growth with fixed tasks. Recently, single-agent environments address this by generating new tasks based on agent actions, enhancing task diversity. We consider the decision-making problem in multi-agent settings, where tasks are further influenced by social connections, affecting rewards and information access. However, existing multi-agent environments lack a combination of adaptive physical surroundings and social connections, hindering the learning of intelligent behaviors. To address this, we introduce AdaSociety, a customizable multi-agent environment featuring expanding state and action spaces, alongside explicit and alterable social structures. As agents progress, the environment adaptively generates new tasks with social structures for agents to undertake. In AdaSociety, we develop three mini-games showcasing distinct social structures and tasks. Initial results demonstrate that specific social structures can promote both individual and collective benefits, though current reinforcement learning and LLM-based algorithms show limited effectiveness in leveraging social structures to enhance performance. Overall, AdaSociety serves as a valuable research platform for exploring intelligence in diverse physical and social settings. The code is available at https://github.com/bigai-ai/AdaSociety.





## Accelerated DC loadflow solver for topology optimization
- **Url**: http://arxiv.org/abs/2501.17529v1
- **Authors**: ['Nico Westerbeck', 'Joost van Dijk', 'Jan Viebahn', 'Christian Merz', 'Dirk Witthaut']
- **Abstrat**: We present a massively parallel solver that accelerates DC loadflow computations for power grid topology optimization tasks. Our approach leverages low-rank updates of the Power Transfer Distribution Factors (PTDFs) to represent substation splits, line outages, and reconfigurations without ever refactorizing the system. Furthermore, we implement the core routines on Graphics Processing Units (GPUs), thereby exploiting their high-throughput architecture for linear algebra. A two-level decomposition separates changes in branch topology from changes in nodal injections, enabling additional speed-ups by an in-the-loop brute force search over injection variations at minimal additional cost. We demonstrate billion-loadflow-per-second performance on power grids of varying sizes in workload settings which are typical for gradient-free topology optimization such as Reinforcement Learning or Quality Diversity methods. While adopting the DC approximation sacrifices some accuracy and prohibits the computation of voltage magnitudes, we show that this sacrifice unlocks new scales of computational feasibility, offering a powerful tool for large-scale grid planning and operational topology optimization.





## The impact of behavioral diversity in multi-agent reinforcement learning
- **Url**: http://arxiv.org/abs/2412.16244v2
- **Authors**: ['Matteo Bettini', 'Ryan Kortvelesy', 'Amanda Prorok']
- **Abstrat**: Many of the world's most pressing issues, such as climate change and global peace, require complex collective problem-solving skills. Recent studies indicate that diversity in individuals' behaviors is key to developing such skills and increasing collective performance. Yet behavioral diversity in collective artificial learning is understudied, with today's machine learning paradigms commonly favoring homogeneous agent strategies over heterogeneous ones, mainly due to computational considerations. In this work, we employ diversity measurement and control paradigms to study the impact of behavioral heterogeneity in several facets of multi-agent reinforcement learning. Through experiments in team play and other cooperative tasks, we show the emergence of unbiased behavioral roles that improve team outcomes; how behavioral diversity synergizes with morphological diversity; how diverse agents are more effective at finding cooperative solutions in sparse reward settings; and how behaviorally heterogeneous teams learn and retain latent skills to overcome repeated disruptions. Overall, our results indicate that, by controlling diversity, we can obtain non-trivial benefits over homogeneous training paradigms, demonstrating that diversity is a fundamental component of collective artificial learning, an insight thus far overlooked.





## Consistent time travel for realistic interactions with historical data: reinforcement learning for market making
- **Url**: http://arxiv.org/abs/2408.02322v2
- **Authors**: ['Vincent Ragel', 'Damien Challet']
- **Abstrat**: Reinforcement learning works best when the impact of the agent's actions on its environment can be perfectly simulated or fully appraised from available data. Some systems are however both hard to simulate and very sensitive to small perturbations. An additional difficulty arises when a RL agent is trained offline to be part of a multi-agent system using only anonymous data, which makes it impossible to infer the state of each agent, thus to use data directly. Typical examples are competitive systems without agent-resolved data such as financial markets. We introduce consistent data time travel for offline RL as a remedy for these problems: instead of using historical data in a sequential way, we argue that one needs to perform time travel in historical data, i.e., to adjust the time index so that both the past state and the influence of the RL agent's action on the system coincide with real data. This both alleviates the need to resort to imperfect models and consistently accounts for both the immediate and long-term reactions of the system when using anonymous historical data. We apply this idea to market making in limit order books, a notoriously difficult task for RL; it turns out that the gain of the agent is significantly higher with data time travel than with naive sequential data, which suggests that the difficulty of this task for RL may have been overestimated.





# TD3
# Prioritized Experience Replay
# path planning
## Multi-Agent Path Finding Using Conflict-Based Search and Structural-Semantic Topometric Maps
- **Url**: http://arxiv.org/abs/2501.17661v1
- **Authors**: ['Scott Fredriksson', 'Yifan Bai', 'Akshit Saradagi', 'George Nikolakopoulos']
- **Abstrat**: As industries increasingly adopt large robotic fleets, there is a pressing need for computationally efficient, practical, and optimal conflict-free path planning for multiple robots. Conflict-Based Search (CBS) is a popular method for multi-agent path finding (MAPF) due to its completeness and optimality; however, it is often impractical for real-world applications, as it is computationally intensive to solve and relies on assumptions about agents and operating environments that are difficult to realize. This article proposes a solution to overcome computational challenges and practicality issues of CBS by utilizing structural-semantic topometric maps. Instead of running CBS over large grid-based maps, the proposed solution runs CBS over a sparse topometric map containing structural-semantic cells representing intersections, pathways, and dead ends. This approach significantly accelerates the MAPF process and reduces the number of conflict resolutions handled by CBS while operating in continuous time. In the proposed method, robots are assigned time ranges to move between topometric regions, departing from the traditional CBS assumption that a robot can move to any connected cell in a single time step. The approach is validated through real-world multi-robot path-finding experiments and benchmarking simulations. The results demonstrate that the proposed MAPF method can be applied to real-world non-holonomic robots and yields significant improvement in computational efficiency compared to traditional CBS methods while improving conflict detection and resolution in cases of corridor symmetries.





## Multimodal urban transportation network equilibrium including intermodality and shared mobility services
- **Url**: http://arxiv.org/abs/2402.00735v3
- **Authors**: ['Khadidja Kadem', 'Mostafa Ameli', 'Mahdi Zargayouna', 'Latifa Oukhellou']
- **Abstrat**: Shared Mobility Services (SMSs) are reshaping urban transportation systems by providing flexible mobility options. With their ability to decrease the number of cars on the roads, these services can potentially improve the transportation system's performance in terms of travel times and emissions. This emphasizes the importance of analyzing and understanding their impacts on the system and users' choices, especially when integrated into a complex multi-modal system, including public transport (PT). Many studies overlook the synergies between SMSs and PT, leading to inaccurate traffic estimations and planning. This research offers an extensive review of multi-modal transportation system models involving SMSs. We then introduce a traffic assignment analytical model valid in both continuous and integer settings, leading to a Mixed-Integer Bilinear Programming (MIBLP) formulation. This model comprises diverse travel possibilities, including SMSs, and handles intermodality by allowing commuters to combine modes to optimize time and monetary expense. An in-depth examination of commuters' mode and path choices on two test cases and an analysis of the price of anarchy highlights the disparities between user equilibrium and system optimum in such intricate systems.




