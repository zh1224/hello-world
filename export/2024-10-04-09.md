# reinforcement learning
## ReLIC: A Recipe for 64k Steps of In-Context Reinforcement Learning for Embodied AI
- **Url**: http://arxiv.org/abs/2410.02751v1
- **Authors**: ['Ahmad Elawady', 'Gunjan Chhablani', 'Ram Ramrakhya', 'Karmesh Yadav', 'Dhruv Batra', 'Zsolt Kira', 'Andrew Szot']
- **Abstrat**: Intelligent embodied agents need to quickly adapt to new scenarios by integrating long histories of experience into decision-making. For instance, a robot in an unfamiliar house initially wouldn't know the locations of objects needed for tasks and might perform inefficiently. However, as it gathers more experience, it should learn the layout of its environment and remember where objects are, allowing it to complete new tasks more efficiently. To enable such rapid adaptation to new tasks, we present ReLIC, a new approach for in-context reinforcement learning (RL) for embodied agents. With ReLIC, agents are capable of adapting to new environments using 64,000 steps of in-context experience with full attention while being trained through self-generated experience via RL. We achieve this by proposing a novel policy update scheme for on-policy RL called "partial updates'' as well as a Sink-KV mechanism that enables effective utilization of a long observation history for embodied agents. Our method outperforms a variety of meta-RL baselines in adapting to unseen houses in an embodied multi-object navigation task. In addition, we find that ReLIC is capable of few-shot imitation learning despite never being trained with expert demonstrations. We also provide a comprehensive analysis of ReLIC, highlighting that the combination of large-scale RL training, the proposed partial updates scheme, and the Sink-KV are essential for effective in-context learning. The code for ReLIC and all our experiments is at https://github.com/aielawady/relic





## MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions
- **Url**: http://arxiv.org/abs/2410.02743v1
- **Authors**: ['Yekun Chai', 'Haoran Sun', 'Huang Fang', 'Shuohuan Wang', 'Yu Sun', 'Hua Wu']
- **Abstrat**: Reinforcement learning from human feedback (RLHF) has demonstrated effectiveness in aligning large language models (LLMs) with human preferences. However, token-level RLHF suffers from the credit assignment problem over long sequences, where delayed rewards make it challenging for the model to discern which actions contributed to successful outcomes. This hinders learning efficiency and slows convergence. In this paper, we propose MA-RLHF, a simple yet effective RLHF framework that incorporates macro actions -- sequences of tokens or higher-level language constructs -- into the learning process. By operating at this higher level of abstraction, our approach reduces the temporal distance between actions and rewards, facilitating faster and more accurate credit assignment. This results in more stable policy gradient estimates and enhances learning efficiency within each episode, all without increasing computational complexity during training or inference. We validate our approach through extensive experiments across various model sizes and tasks, including text summarization, dialogue generation, question answering, and program synthesis. Our method achieves substantial performance improvements over standard RLHF, with performance gains of up to 30% in text summarization and code generation, 18% in dialogue, and 8% in question answering tasks. Notably, our approach reaches parity with vanilla RLHF 1.7x to 2x faster in terms of training time and continues to outperform it with further training. We will make our code and data publicly available at https://github.com/ernie-research/MA-RLHF .





## On the Limited Generalization Capability of the Implicit Reward Model Induced by Direct Preference Optimization
- **Url**: http://arxiv.org/abs/2409.03650v2
- **Authors**: ['Yong Lin', 'Skyler Seto', 'Maartje ter Hoeve', 'Katherine Metcalf', 'Barry-John Theobald', 'Xuan Wang', 'Yizhe Zhang', 'Chen Huang', 'Tong Zhang']
- **Abstrat**: Reinforcement Learning from Human Feedback (RLHF) is an effective approach for aligning language models to human preferences. Central to RLHF is learning a reward function for scoring human preferences. Two main approaches for learning a reward model are 1) training an EXplicit Reward Model (EXRM) as in RLHF, and 2) using an implicit reward learned from preference data through methods such as Direct Preference Optimization (DPO). Prior work has shown that the implicit reward model of DPO (denoted as DPORM) can approximate an EXRM in the limit. DPORM's effectiveness directly implies the optimality of the learned policy, and also has practical implication for LLM alignment methods including iterative DPO. However, it is unclear how well DPORM empirically matches the performance of EXRM. This work studies the accuracy at distinguishing preferred and rejected answers for both DPORM and EXRM. Our findings indicate that even though DPORM fits the training dataset comparably, it generalizes less effectively than EXRM, especially when the validation datasets contain distribution shifts. Across five out-of-distribution settings, DPORM has a mean drop in accuracy of 3% and a maximum drop of 7%. These findings highlight that DPORM has limited generalization ability and substantiates the integration of an explicit reward model in iterative DPO approaches.





## Grounded Answers for Multi-agent Decision-making Problem through Generative World Model
- **Url**: http://arxiv.org/abs/2410.02664v1
- **Authors**: ['Zeyang Liu', 'Xinrui Yang', 'Shiguang Sun', 'Long Qian', 'Lipeng Wan', 'Xingyu Chen', 'Xuguang Lan']
- **Abstrat**: Recent progress in generative models has stimulated significant innovations in many fields, such as image generation and chatbots. Despite their success, these models often produce sketchy and misleading solutions for complex multi-agent decision-making problems because they miss the trial-and-error experience and reasoning as humans. To address this limitation, we explore a paradigm that integrates a language-guided simulator into the multi-agent reinforcement learning pipeline to enhance the generated answer. The simulator is a world model that separately learns dynamics and reward, where the dynamics model comprises an image tokenizer as well as a causal transformer to generate interaction transitions autoregressively, and the reward model is a bidirectional transformer learned by maximizing the likelihood of trajectories in the expert demonstrations under language guidance. Given an image of the current state and the task description, we use the world model to train the joint policy and produce the image sequence as the answer by running the converged policy on the dynamics model. The empirical results demonstrate that this framework can improve the answers for multi-agent decision-making problems by showing superior performance on the training and unseen tasks of the StarCraft Multi-Agent Challenge benchmark. In particular, it can generate consistent interaction sequences and explainable reward functions at interaction states, opening the path for training generative models of the future.





## Sample and Oracle Efficient Reinforcement Learning for MDPs with Linearly-Realizable Value Functions
- **Url**: http://arxiv.org/abs/2409.04840v2
- **Authors**: ['Zakaria Mhammedi']
- **Abstrat**: Designing sample-efficient and computationally feasible reinforcement learning (RL) algorithms is particularly challenging in environments with large or infinite state and action spaces. In this paper, we advance this effort by presenting an efficient algorithm for Markov Decision Processes (MDPs) where the state-action value function of any policy is linear in a given feature map. This challenging setting can model environments with infinite states and actions, strictly generalizes classic linear MDPs, and currently lacks a computationally efficient algorithm under online access to the MDP. Specifically, we introduce a new RL algorithm that efficiently finds a near-optimal policy in this setting, using a number of episodes and calls to a cost-sensitive classification (CSC) oracle that are both polynomial in the problem parameters. Notably, our CSC oracle can be efficiently implemented when the feature dimension is constant, representing a clear improvement over state-of-the-art methods, which require solving non-convex problems with horizon-many variables and can incur computational costs that are exponential in the horizon.





## Advantage Alignment Algorithms
- **Url**: http://arxiv.org/abs/2406.14662v2
- **Authors**: ['Juan Agustin Duque', 'Milad Aghajohari', 'Tim Cooijmans', 'Razvan Ciuca', 'Tianyu Zhang', 'Gauthier Gidel', 'Aaron Courville']
- **Abstrat**: Artificially intelligent agents are increasingly being integrated into human decision-making: from large language model (LLM) assistants to autonomous vehicles. These systems often optimize their individual objective, leading to conflicts, particularly in general-sum games where naive reinforcement learning agents empirically converge to Pareto-suboptimal Nash equilibria. To address this issue, opponent shaping has emerged as a paradigm for finding socially beneficial equilibria in general-sum games. In this work, we introduce Advantage Alignment, a family of algorithms derived from first principles that perform opponent shaping efficiently and intuitively. We achieve this by aligning the advantages of interacting agents, increasing the probability of mutually beneficial actions when their interaction has been positive. We prove that existing opponent shaping methods implicitly perform Advantage Alignment. Compared to these methods, Advantage Alignment simplifies the mathematical formulation of opponent shaping, reduces the computational burden and extends to continuous action domains. We demonstrate the effectiveness of our algorithms across a range of social dilemmas, achieving state-of-the-art cooperation and robustness against exploitation.





## Beyond Expected Returns: A Policy Gradient Algorithm for Cumulative Prospect Theoretic Reinforcement Learning
- **Url**: http://arxiv.org/abs/2410.02605v1
- **Authors**: ['Olivier Lepel', 'Anas Barakat']
- **Abstrat**: The widely used expected utility theory has been shown to be empirically inconsistent with human preferences in the psychology and behavioral economy literatures. Cumulative Prospect Theory (CPT) has been developed to fill in this gap and provide a better model for human-based decision-making supported by empirical evidence. It allows to express a wide range of attitudes and perceptions towards risk, gains and losses. A few years ago, CPT has been combined with Reinforcement Learning (RL) to formulate a CPT policy optimization problem where the goal of the agent is to search for a policy generating long-term returns which are aligned with their preferences. In this work, we revisit this policy optimization problem and provide new insights on optimal policies and their nature depending on the utility function under consideration. We further derive a novel policy gradient theorem for the CPT policy optimization objective generalizing the seminal corresponding result in standard RL. This result enables us to design a model-free policy gradient algorithm to solve the CPT-RL problem. We illustrate the performance of our algorithm in simple examples motivated by traffic control and electricity management applications. We also demonstrate that our policy gradient algorithm scales better to larger state spaces compared to the existing zeroth order algorithm for solving the same problem.





## Boosting Sample Efficiency and Generalization in Multi-agent Reinforcement Learning via Equivariance
- **Url**: http://arxiv.org/abs/2410.02581v1
- **Authors**: ['Joshua McClellan', 'Naveed Haghani', 'John Winder', 'Furong Huang', 'Pratap Tokekar']
- **Abstrat**: Multi-Agent Reinforcement Learning (MARL) struggles with sample inefficiency and poor generalization [1]. These challenges are partially due to a lack of structure or inductive bias in the neural networks typically used in learning the policy. One such form of structure that is commonly observed in multi-agent scenarios is symmetry. The field of Geometric Deep Learning has developed Equivariant Graph Neural Networks (EGNN) that are equivariant (or symmetric) to rotations, translations, and reflections of nodes. Incorporating equivariance has been shown to improve learning efficiency and decrease error [ 2 ]. In this paper, we demonstrate that EGNNs improve the sample efficiency and generalization in MARL. However, we also show that a naive application of EGNNs to MARL results in poor early exploration due to a bias in the EGNN structure. To mitigate this bias, we present Exploration-enhanced Equivariant Graph Neural Networks or E2GN2. We compare E2GN2 to other common function approximators using common MARL benchmarks MPE and SMACv2. E2GN2 demonstrates a significant improvement in sample efficiency, greater final reward convergence, and a 2x-5x gain in over standard GNNs in our generalization tests. These results pave the way for more reliable and effective solutions in complex multi-agent systems.





## Machine Learning Approaches for Active Queue Management: A Survey, Taxonomy, and Future Directions
- **Url**: http://arxiv.org/abs/2410.02563v1
- **Authors**: ['Mohammad Parsa Toopchinezhad', 'Mahmood Ahmadi']
- **Abstrat**: Active Queue Management (AQM), a network-layer congestion control technique endorsed by the Internet Engineering Task Force (IETF), encourages routers to discard packets before the occurrence of buffer overflow. Traditional AQM techniques often employ heuristic approaches that require meticulous parameter adjustments, limiting their real-world applicability. In contrast, Machine Learning (ML) approaches offer highly adaptive, data-driven solutions custom to dynamic network conditions. Consequently, many researchers have adapted ML for AQM throughout the years, resulting in a wide variety of algorithms ranging from predicting congestion via supervised learning to discovering optimal packet-dropping policies with reinforcement learning. Despite these remarkable advancements, no previous work has compiled these methods in the form of a survey article. This paper presents the first thorough documentation and analysis of ML-based algorithms for AQM, in which the strengths and limitations of each proposed method are evaluated and compared. In addition, a novel taxonomy of ML approaches based on methodology is also established. The review is concluded by discussing unexplored research gaps and potential new directions for more robust ML-AQM methods.





## Semantic-Guided RL for Interpretable Feature Engineering
- **Url**: http://arxiv.org/abs/2410.02519v1
- **Authors**: ['Mohamed Bouadi', 'Arta Alavi', 'Salima Benbernou', 'Mourad Ouziri']
- **Abstrat**: The quality of Machine Learning (ML) models strongly depends on the input data, as such generating high-quality features is often required to improve the predictive accuracy. This process is referred to as Feature Engineering (FE). However, since manual feature engineering is time-consuming and requires case-by-case domain knowledge, Automated Feature Engineering (AutoFE) is crucial. A major challenge that remains is to generate interpretable features. To tackle this problem, we introduce SMART, a hybrid approach that uses semantic technologies to guide the generation of interpretable features through a two-step process: Exploitation and Exploration. The former uses Description Logics (DL) to reason on the semantics embedded in Knowledge Graphs (KG) to infer domain-specific features, while the latter exploits the knowledge graph to conduct a guided exploration of the search space through Deep Reinforcement Learning (DRL). Our experiments on public datasets demonstrate that SMART significantly improves prediction accuracy while ensuring a high level of interpretability.





## Learning Emergence of Interaction Patterns across Independent RL Agents in Multi-Agent Environments
- **Url**: http://arxiv.org/abs/2410.02516v1
- **Authors**: ['Vasanth Reddy Baddam', 'Suat Gumussoy', 'Almuatazbellah Boker', 'Hoda Eldardiry']
- **Abstrat**: Many real-world problems, such as controlling swarms of drones and urban traffic, naturally lend themselves to modeling as multi-agent reinforcement learning (RL) problems. However, existing multi-agent RL methods often suffer from scalability challenges, primarily due to the introduction of communication among agents. Consequently, a key challenge lies in adapting the success of deep learning in single-agent RL to the multi-agent setting. In response to this challenge, we propose an approach that fundamentally reimagines multi-agent environments. Unlike conventional methods that model each agent individually with separate networks, our approach, the Bottom Up Network (BUN), adopts a unique perspective. BUN treats the collective of multi-agents as a unified entity while employing a specialized weight initialization strategy that promotes independent learning. Furthermore, we dynamically establish connections among agents using gradient information, enabling coordination when necessary while maintaining these connections as limited and sparse to effectively manage the computational budget. Our extensive empirical evaluations across a variety of cooperative multi-agent scenarios, including tasks such as cooperative navigation and traffic control, consistently demonstrate BUN's superiority over baseline methods with substantially reduced computational costs.





## A Hitchhiker's Guide To Active Motion
- **Url**: http://arxiv.org/abs/2410.02515v1
- **Authors**: ['Tobias Plasczyk', 'Paul A. Monderkamp', 'Hartmut Löwen', 'René Wittmann']
- **Abstrat**: Intelligent decisions in response to external informative input can allow organisms to achieve their biological goals while spending very little of their own resources. In this paper, we develop and study a minimal model for a navigational task, performed by an otherwise completely motorless particle that possesses the ability of \textit{hitchhiking} in a bath of active Brownian particles (ABPs). Hitchhiking refers to identifying and attaching to suitable surrounding bath particles. Using a reinforcement learning algorithm, such an agent, which we refer to as intelligent hitchhiking particle (IHP), is enabled to persistently navigate in the desired direction. This relatively simple IHP can also anticipate and react to characteristic motion patterns of their hosts, which we exemplify for a bath of chiral ABPs (cABPs). To demonstrate that the persistent motion of the IHP will outperform that of the bath particles in view of long-time ballistic motion, we calculate the mean-squared displacement and discuss its dependence on the density and persistence time of the bath ABPs by means of an analytic model.





## Symbolic State Partitioning for Reinforcement Learning
- **Url**: http://arxiv.org/abs/2409.16791v2
- **Authors**: ['Mohsen Ghaffari', 'Mahsa Varshosaz', 'Einar Broch Johnsen', 'Andrzej Wąsowski']
- **Abstrat**: Tabular reinforcement learning methods cannot operate directly on continuous state spaces. One solution for this problem is to partition the state space. A good partitioning enables generalization during learning and more efficient exploitation of prior experiences. Consequently, the learning process becomes faster and produces more reliable policies. However, partitioning introduces approximation, which is particularly harmful in the presence of nonlinear relations between state components. An ideal partition should be as coarse as possible, while capturing the key structure of the state space for the given problem. This work extracts partitions from the environment dynamics by symbolic execution. We show that symbolic partitioning improves state space coverage with respect to environmental behavior and allows reinforcement learning to perform better for sparse rewards. We evaluate symbolic state space partitioning with respect to precision, scalability, learning agent performance and state space coverage for the learnt policies.





## Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration
- **Url**: http://arxiv.org/abs/2410.02511v1
- **Authors**: ['Yun Qu', 'Boyuan Wang', 'Yuhang Jiang', 'Jianzhun Shao', 'Yixiu Mao', 'Cheems Wang', 'Chang Liu', 'Xiangyang Ji']
- **Abstrat**: With expansive state-action spaces, efficient multi-agent exploration remains a longstanding challenge in reinforcement learning. Although pursuing novelty, diversity, or uncertainty attracts increasing attention, redundant efforts brought by exploration without proper guidance choices poses a practical issue for the community. This paper introduces a systematic approach, termed LEMAE, choosing to channel informative task-relevant guidance from a knowledgeable Large Language Model (LLM) for Efficient Multi-Agent Exploration. Specifically, we ground linguistic knowledge from LLM into symbolic key states, that are critical for task fulfillment, in a discriminative manner at low LLM inference costs. To unleash the power of key states, we design Subspace-based Hindsight Intrinsic Reward (SHIR) to guide agents toward key states by increasing reward density. Additionally, we build the Key State Memory Tree (KSMT) to track transitions between key states in a specific task for organized exploration. Benefiting from diminishing redundant explorations, LEMAE outperforms existing SOTA approaches on the challenging benchmarks (e.g., SMAC and MPE) by a large margin, achieving a 10x acceleration in certain scenarios.





## Dual Active Learning for Reinforcement Learning from Human Feedback
- **Url**: http://arxiv.org/abs/2410.02504v1
- **Authors**: ['Pangpang Liu', 'Chengchun Shi', 'Will Wei Sun']
- **Abstrat**: Aligning large language models (LLMs) with human preferences is critical to recent advances in generative artificial intelligence. Reinforcement learning from human feedback (RLHF) is widely applied to achieve this objective. A key step in RLHF is to learn the reward function from human feedback. However, human feedback is costly and time-consuming, making it essential to collect high-quality conversation data for human teachers to label. Additionally, different human teachers have different levels of expertise. It is thus critical to query the most appropriate teacher for their opinions. In this paper, we use offline reinforcement learning (RL) to formulate the alignment problem. Motivated by the idea of $D$-optimal design, we first propose a dual active reward learning algorithm for the simultaneous selection of conversations and teachers. Next, we apply pessimistic RL to solve the alignment problem, based on the learned reward estimator. Theoretically, we show that the reward estimator obtained through our proposed adaptive selection strategy achieves minimal generalized variance asymptotically, and prove that the sub-optimality of our pessimistic policy scales as $O(1/\sqrt{T})$ with a given sample budget $T$. Through simulations and experiments on LLMs, we demonstrate the effectiveness of our algorithm and its superiority over state-of-the-arts.





## Cross-Embodiment Dexterous Grasping with Reinforcement Learning
- **Url**: http://arxiv.org/abs/2410.02479v1
- **Authors**: ['Haoqi Yuan', 'Bohan Zhou', 'Yuhui Fu', 'Zongqing Lu']
- **Abstrat**: Dexterous hands exhibit significant potential for complex real-world grasping tasks. While recent studies have primarily focused on learning policies for specific robotic hands, the development of a universal policy that controls diverse dexterous hands remains largely unexplored. In this work, we study the learning of cross-embodiment dexterous grasping policies using reinforcement learning (RL). Inspired by the capability of human hands to control various dexterous hands through teleoperation, we propose a universal action space based on the human hand's eigengrasps. The policy outputs eigengrasp actions that are then converted into specific joint actions for each robot hand through a retargeting mapping. We simplify the robot hand's proprioception to include only the positions of fingertips and the palm, offering a unified observation space across different robot hands. Our approach demonstrates an 80% success rate in grasping objects from the YCB dataset across four distinct embodiments using a single vision-based policy. Additionally, our policy exhibits zero-shot generalization to two previously unseen embodiments and significant improvement in efficient finetuning. For further details and videos, visit our project page https://sites.google.com/view/crossdex.





## Learning Diverse Bimanual Dexterous Manipulation Skills from Human Demonstrations
- **Url**: http://arxiv.org/abs/2410.02477v1
- **Authors**: ['Bohan Zhou', 'Haoqi Yuan', 'Yuhui Fu', 'Zongqing Lu']
- **Abstrat**: Bimanual dexterous manipulation is a critical yet underexplored area in robotics. Its high-dimensional action space and inherent task complexity present significant challenges for policy learning, and the limited task diversity in existing benchmarks hinders general-purpose skill development. Existing approaches largely depend on reinforcement learning, often constrained by intricately designed reward functions tailored to a narrow set of tasks. In this work, we present a novel approach for efficiently learning diverse bimanual dexterous skills from abundant human demonstrations. Specifically, we introduce BiDexHD, a framework that unifies task construction from existing bimanual datasets and employs teacher-student policy learning to address all tasks. The teacher learns state-based policies using a general two-stage reward function across tasks with shared behaviors, while the student distills the learned multi-task policies into a vision-based policy. With BiDexHD, scalable learning of numerous bimanual dexterous skills from auto-constructed tasks becomes feasible, offering promising advances toward universal bimanual dexterous manipulation. Our empirical evaluation on the TACO dataset, spanning 141 tasks across six categories, demonstrates a task fulfillment rate of 74.59% on trained tasks and 51.07% on unseen tasks, showcasing the effectiveness and competitive zero-shot generalization capabilities of BiDexHD. For videos and more information, visit our project page https://sites.google.com/view/bidexhd.





## Efficient Residual Learning with Mixture-of-Experts for Universal Dexterous Grasping
- **Url**: http://arxiv.org/abs/2410.02475v1
- **Authors**: ['Ziye Huang', 'Haoqi Yuan', 'Yuhui Fu', 'Zongqing Lu']
- **Abstrat**: Universal dexterous grasping across diverse objects presents a fundamental yet formidable challenge in robot learning. Existing approaches using reinforcement learning (RL) to develop policies on extensive object datasets face critical limitations, including complex curriculum design for multi-task learning and limited generalization to unseen objects. To overcome these challenges, we introduce ResDex, a novel approach that integrates residual policy learning with a mixture-of-experts (MoE) framework. ResDex is distinguished by its use of geometry-unaware base policies that are efficiently acquired on individual objects and capable of generalizing across a wide range of unseen objects. Our MoE framework incorporates several base policies to facilitate diverse grasping styles suitable for various objects. By learning residual actions alongside weights that combine these base policies, ResDex enables efficient multi-task RL for universal dexterous grasping. ResDex achieves state-of-the-art performance on the DexGraspNet dataset comprising 3,200 objects with an 88.8% success rate. It exhibits no generalization gap with unseen objects and demonstrates superior training efficiency, mastering all tasks within only 12 hours on a single GPU.





## Optimised Hybrid Classical-Quantum Algorithm for Accelerated Solution of Sparse Linear Systems
- **Url**: http://arxiv.org/abs/2410.02408v1
- **Authors**: ['Hakikat Singh']
- **Abstrat**: Efficiently solving large-scale sparse linear systems poses a significant challenge in computational science, especially in fields such as physics, engineering, machine learning, and finance. Traditional classical algorithms face scalability issues as the size of these systems increases, leading to performance degradation. On the other hand, quantum algorithms, like the Harrow-Hassidim-Lloyd (HHL) algorithm, offer exponential speedups for solving linear systems, yet they are constrained by the current state of quantum hardware and sensitivity to matrix condition numbers. This paper introduces a hybrid classical-quantum algorithm that combines CUDA-accelerated preconditioning techniques with the HHL algorithm to solve sparse linear systems more efficiently. The classical GPU parallelism is utilised to preprocess and precondition the matrix, reducing its condition number, while quantum computing is employed to solve the preconditioned system using the HHL algorithm. Additionally, the algorithm integrates machine learning models, particularly reinforcement learning, to dynamically optimise system parameters, such as block sizes and preconditioning stratgies, based on real-time performance data. Our experimental results show that the proposed approach not only surpasses traditional methods in speed and scalability but also mitigates some of the inherent limitations of quantum algorithms. This work pushes the boundaries of efficient computing and provides a foundation for future advancements in hybrid computational frameworks.





## Diffusion Meets Options: Hierarchical Generative Skill Composition for Temporally-Extended Tasks
- **Url**: http://arxiv.org/abs/2410.02389v1
- **Authors**: ['Zeyu Feng', 'Hao Luan', 'Kevin Yuchen Ma', 'Harold Soh']
- **Abstrat**: Safe and successful deployment of robots requires not only the ability to generate complex plans but also the capacity to frequently replan and correct execution errors. This paper addresses the challenge of long-horizon trajectory planning under temporally extended objectives in a receding horizon manner. To this end, we propose DOPPLER, a data-driven hierarchical framework that generates and updates plans based on instruction specified by linear temporal logic (LTL). Our method decomposes temporal tasks into chain of options with hierarchical reinforcement learning from offline non-expert datasets. It leverages diffusion models to generate options with low-level actions. We devise a determinantal-guided posterior sampling technique during batch generation, which improves the speed and diversity of diffusion generated options, leading to more efficient querying. Experiments on robot navigation and manipulation tasks demonstrate that DOPPLER can generate sequences of trajectories that progressively satisfy the specified formulae for obstacle avoidance and sequential visitation. Demonstration videos are available online at: https://philiptheother.github.io/doppler/.





# TD3
# Prioritized Experience Replay
# path planning
## A Schema-aware Logic Reformulation for Graph Reachability
- **Url**: http://arxiv.org/abs/2410.02533v1
- **Authors**: ['Davide Di Pierro', 'Stefano Ferilli']
- **Abstrat**: Graph reachability is the task of understanding whether two distinct points in a graph are interconnected by arcs to which in general a semantic is attached. Reachability has plenty of applications, ranging from motion planning to routing. Improving reachability requires structural knowledge of relations so as to avoid the complexity of traditional depth-first and breadth-first strategies, implemented in logic languages. In some contexts, graphs are enriched with their schema definitions establishing domain and range for every arc. The introduction of a schema-aware formalization for guiding the search may result in a sensitive improvement by cutting out unuseful paths and prioritising those that, in principle, reach the target earlier. In this work, we propose a strategy to automatically exclude and sort certain graph paths by exploiting the higher-level conceptualization of instances. The aim is to obtain a new first-order logic reformulation of the graph reachability scenario, capable of improving the traditional algorithms in terms of time, space requirements, and number of backtracks. The experiments exhibit the expected advantages of the approach in reducing the number of backtracks during the search strategy, resulting in saving time and space as well.





## Theory and Explicit Design of a Path Planner for an SE(3) Robot
- **Url**: http://arxiv.org/abs/2407.05135v2
- **Authors**: ['Zhaoqi Zhang', 'Yi-Jen Chiang', 'Chee Yap']
- **Abstrat**: We consider path planning for a rigid spatial robot with 6 degrees of freedom (6 DOFs), moving amidst polyhedral obstacles. A correct, complete and practical path planner for such a robot has never been achieved, although this is widely recognized as a key challenge in robotics. This paper provides a complete "explicit" design, down to explicit geometric primitives that are easily implementable.   Our design is within an algorithmic framework for path planners, called Soft Subdivision Search (SSS). The framework is based on the twin foundations of $\epsilon$-exactness and soft predicates, which are critical for rigorous numerical implementations. The practicality of SSS has been previously demonstrated for various robots including 5-DOF spatial robots.   In this paper, we solve several significant technical challenges for SE(3) robots: (1) We first ensure the correct theory by proving a general form of the Fundamental Theorem of the SSS theory. We prove this within an axiomatic framework, thus making it easy for future applications of this theory. (2) One component of $SE(3) = R^3 \times SO(3)$ is the non-Euclidean space SO(3). We design a novel topologically correct data structure for SO(3). Using the concept of subdivision charts and atlases for SO(3), we can now carry out subdivision of SO(3). (3) The geometric problem of collision detection takes place in $R^3$, via the footprint map. Unlike sampling-based approaches, we must reason with the notion of footprints of configuration boxes, which is much harder to characterize. Exploiting the theory of soft predicates, we design suitable approximate footprints which, when combined with the highly effective feature-set technique, lead to soft predicates. (4) Finally, we make the underlying geometric computation "explicit", i.e., avoiding a general solver of polynomial systems, in order to allow a direct implementation.




