# reinforcement learning
## Robust Offline Reinforcement Learning with Linearly Structured $f$-Divergence Regularization
- **Url**: http://arxiv.org/abs/2411.18612v1
- **Authors**: ['Cheng Tang', 'Zhishuai Liu', 'Pan Xu']
- **Abstrat**: The Distributionally Robust Markov Decision Process (DRMDP) is a popular framework for addressing dynamics shift in reinforcement learning by learning policies robust to the worst-case transition dynamics within a constrained set. However, solving its dual optimization oracle poses significant challenges, limiting theoretical analysis and computational efficiency. The recently proposed Robust Regularized Markov Decision Process (RRMDP) replaces the uncertainty set constraint with a regularization term on the value function, offering improved scalability and theoretical insights. Yet, existing RRMDP methods rely on unstructured regularization, often leading to overly conservative policies by considering transitions that are unrealistic. To address these issues, we propose a novel framework, the $d$-rectangular linear robust regularized Markov decision process ($d$-RRMDP), which introduces a linear latent structure into both transition kernels and regularization. For the offline RL setting, where an agent learns robust policies from a pre-collected dataset in the nominal environment, we develop a family of algorithms, Robust Regularized Pessimistic Value Iteration (R2PVI), employing linear function approximation and $f$-divergence based regularization terms on transition kernels. We provide instance-dependent upper bounds on the suboptimality gap of R2PVI policies, showing these bounds depend on how well the dataset covers state-action spaces visited by the optimal robust policy under robustly admissible transitions. This term is further shown to be fundamental to $d$-RRMDPs via information-theoretic lower bounds. Finally, numerical experiments validate that R2PVI learns robust policies and is computationally more efficient than methods for constrained DRMDPs.





## A Talent-infused Policy-gradient Approach to Efficient Co-Design of Morphology and Task Allocation Behavior of Multi-Robot Systems
- **Url**: http://arxiv.org/abs/2411.18519v1
- **Authors**: ['Prajit KrisshnaKumar', 'Steve Paul', 'Souma Chowdhury']
- **Abstrat**: Interesting and efficient collective behavior observed in multi-robot or swarm systems emerges from the individual behavior of the robots. The functional space of individual robot behaviors is in turn shaped or constrained by the robot's morphology or physical design. Thus the full potential of multi-robot systems can be realized by concurrently optimizing the morphology and behavior of individual robots, informed by the environment's feedback about their collective performance, as opposed to treating morphology and behavior choices disparately or in sequence (the classical approach). This paper presents an efficient concurrent design or co-design method to explore this potential and understand how morphology choices impact collective behavior, particularly in an MRTA problem focused on a flood response scenario, where the individual behavior is designed via graph reinforcement learning. Computational efficiency in this case is attributed to a new way of near exact decomposition of the co-design problem into a series of simpler optimization and learning problems. This is achieved through i) the identification and use of the Pareto front of Talent metrics that represent morphology-dependent robot capabilities, and ii) learning the selection of Talent best trade-offs and individual robot policy that jointly maximizes the MRTA performance. Applied to a multi-unmanned aerial vehicle flood response use case, the co-design outcomes are shown to readily outperform sequential design baselines. Significant differences in morphology and learned behavior are also observed when comparing co-designed single robot vs. co-designed multi-robot systems for similar operations.





## Lusifer: LLM-based User SImulated Feedback Environment for online Recommender systems
- **Url**: http://arxiv.org/abs/2405.13362v2
- **Authors**: ['Danial Ebrat', 'Eli Paradalis', 'Luis Rueda']
- **Abstrat**: Training reinforcement learning-based recommender systems is often hindered by the lack of dynamic and realistic user interactions. To address this limitation, we introduce Lusifer, a novel environment leveraging Large Language Models (LLMs) to generate simulated user feedback. Lusifer synthesizes user profiles and interaction histories to simulate responses and behaviors toward recommended items, with profiles updated after each rating to reflect evolving user characteristics. Utilizing the MovieLens dataset as a proof of concept, we limited our implementation to the last 40 interactions for each user, representing approximately 39% and 22% of the training sets, to focus on recent user behavior. For consistency and to gain insights into the performance of traditional methods with limited data, we implemented baseline approaches using the same data subset. Our results demonstrate that Lusifer accurately emulates user behavior and preferences, even with reduced training data having an RMSE of 1.3 across various test sets. This paper presents Lusifer's operational pipeline, including prompt generation and iterative user profile updates, and compares its performance against baseline methods. The findings validate Lusifer's ability to produce realistic dynamic feedback and suggest that it offers a scalable and adjustable framework for user simulation in online reinforcement learning recommender systems for future studies, particularly when training data is limited.





## Dynamic Throwing with Robotic Material Handling Machines
- **Url**: http://arxiv.org/abs/2405.19001v3
- **Authors**: ['Lennart Werner', 'Fang Nan', 'Pol Eyschen', 'Filippo A. Spinelli', 'Hongyi Yang', 'Marco Hutter']
- **Abstrat**: Automation of hydraulic material handling machinery is currently limited to semi-static pick-and-place cycles. Dynamic throwing motions which utilize the passive joints, can greatly improve time efficiency as well as increase the dumping workspace. In this work, we use Reinforcement Learning (RL) to design dynamic controllers for material handlers with underactuated arms as commonly used in logistics. The controllers are tested both in simulation and in real-world experiments on a 12-ton test platform. The method is able to exploit the passive joints of the gripper to perform dynamic throwing motions. With the proposed controllers, the machine is able to throw individual objects to targets outside the static reachability zone with good accuracy for its practical applications. The work demonstrates the possibility of using RL to perform highly dynamic tasks with heavy machinery, suggesting a potential for improving the efficiency and precision of autonomous material handling tasks.





## Synatra: Turning Indirect Knowledge into Direct Demonstrations for Digital Agents at Scale
- **Url**: http://arxiv.org/abs/2409.15637v2
- **Authors**: ['Tianyue Ou', 'Frank F. Xu', 'Aman Madaan', 'Jiarui Liu', 'Robert Lo', 'Abishek Sridhar', 'Sudipta Sengupta', 'Dan Roth', 'Graham Neubig', 'Shuyan Zhou']
- **Abstrat**: LLMs can now act as autonomous agents that interact with digital environments and complete specific objectives (e.g., arranging an online meeting). However, accuracy is still far from satisfactory, partly due to a lack of large-scale, direct demonstrations for digital tasks. Obtaining supervised data from humans is costly, and automatic data collection through exploration or reinforcement learning relies on complex environmental and content setup, resulting in datasets that lack comprehensive coverage of various scenarios. On the other hand, there is abundant knowledge that may indirectly assist task completion, such as online tutorials that were created for human consumption. In this work, we present Synatra, an approach that effectively transforms this indirect knowledge into direct supervision at scale. We define different types of indirect knowledge, and carefully study the available sources to obtain it, methods to encode the structure of direct demonstrations, and finally methods to transform indirect knowledge into direct demonstrations. We use 100k such synthetically-created demonstrations to finetune a 7B CodeLlama, and demonstrate that the resulting agent surpasses all comparably sized models on three web-based task benchmarks Mind2Web, MiniWoB++ and WebArena, as well as surpassing GPT-3.5 on WebArena and Mind2Web. In addition, while synthetic demonstrations prove to be only 3% the cost of human demonstrations (at $0.031 each), we show that the synthetic demonstrations can be more effective than an identical number of human demonstrations collected from limited domains.





## Two-Timescale Digital Twin Assisted Model Interference and Retraining over Wireless Network
- **Url**: http://arxiv.org/abs/2411.18329v1
- **Authors**: ['Jiayi Cong', 'Guoliang Cheng', 'Changsheng You', 'Xinyu Huang', 'Wen Wu']
- **Abstrat**: In this paper, we investigate a resource allocation and model retraining problem for dynamic wireless networks by utilizing incremental learning, in which the digital twin (DT) scheme is employed for decision making. A two-timescale framework is proposed for computation resource allocation, mobile user association, and incremental training of user models. To obtain an optimal resource allocation and incremental learning policy, we propose an efficient two-timescale scheme based on hybrid DT-physical architecture with the objective to minimize long-term system delay. Specifically, in the large-timescale, base stations will update the user association and implement incremental learning decisions based on statistical state information from the DT system. Then, in the short timescale, an effective computation resource allocation and incremental learning data generated from the DT system is designed based on deep reinforcement learning (DRL), thus reducing the network system's delay in data transmission, data computation, and model retraining steps. Simulation results demonstrate the effectiveness of the proposed two-timescale scheme compared with benchmark schemes.





## MENTOR: Guiding Hierarchical Reinforcement Learning with Human Feedback and Dynamic Distance Constraint
- **Url**: http://arxiv.org/abs/2402.14244v2
- **Authors**: ['Xinglin Zhou', 'Yifu Yuan', 'Shaofu Yang', 'Jianye Hao']
- **Abstrat**: Hierarchical reinforcement learning (HRL) provides a promising solution for complex tasks with sparse rewards of intelligent agents, which uses a hierarchical framework that divides tasks into subgoals and completes them sequentially. However, current methods struggle to find suitable subgoals for ensuring a stable learning process. Without additional guidance, it is impractical to rely solely on exploration or heuristics methods to determine subgoals in a large goal space. To address the issue, We propose a general hierarchical reinforcement learning framework incorporating human feedback and dynamic distance constraints (MENTOR). MENTOR acts as a "mentor", incorporating human feedback into high-level policy learning, to find better subgoals. As for low-level policy, MENTOR designs a dual policy for exploration-exploitation decoupling respectively to stabilize the training. Furthermore, although humans can simply break down tasks into subgoals to guide the right learning direction, subgoals that are too difficult or too easy can still hinder downstream learning efficiency. We propose the Dynamic Distance Constraint (DDC) mechanism dynamically adjusting the space of optional subgoals. Thus MENTOR can generate subgoals matching the low-level policy learning process from easy to hard. Extensive experiments demonstrate that MENTOR uses a small amount of human feedback to achieve significant improvement in complex tasks with sparse rewards.





## Application of Soft Actor-Critic Algorithms in Optimizing Wastewater Treatment with Time Delays Integration
- **Url**: http://arxiv.org/abs/2411.18305v1
- **Authors**: ['Esmaeel Mohammadi', 'Daniel Ortiz-Arroyo', 'Aviaja Anna Hansen', 'Mikkel Stokholm-Bjerregaard', 'Sebastien Gros', 'Akhil S Anand', 'Petar Durdevic']
- **Abstrat**: Wastewater treatment plants face unique challenges for process control due to their complex dynamics, slow time constants, and stochastic delays in observations and actions. These characteristics make conventional control methods, such as Proportional-Integral-Derivative controllers, suboptimal for achieving efficient phosphorus removal, a critical component of wastewater treatment to ensure environmental sustainability. This study addresses these challenges using a novel deep reinforcement learning approach based on the Soft Actor-Critic algorithm, integrated with a custom simulator designed to model the delayed feedback inherent in wastewater treatment plants. The simulator incorporates Long Short-Term Memory networks for accurate multi-step state predictions, enabling realistic training scenarios. To account for the stochastic nature of delays, agents were trained under three delay scenarios: no delay, constant delay, and random delay. The results demonstrate that incorporating random delays into the reinforcement learning framework significantly improves phosphorus removal efficiency while reducing operational costs. Specifically, the delay-aware agent achieved 36% reduction in phosphorus emissions, 55% higher reward, 77% lower target deviation from the regulatory limit, and 9% lower total costs than traditional control methods in the simulated environment. These findings underscore the potential of reinforcement learning to overcome the limitations of conventional control strategies in wastewater treatment, providing an adaptive and cost-effective solution for phosphorus removal.





## Reinforcement Learning Enhancing Entanglement for Two-Photon-Driven Rabi Model
- **Url**: http://arxiv.org/abs/2411.15841v2
- **Authors**: ['Tingting Li', 'Yiming Zhao', 'Yong Wang', 'Yanping Liu', 'Yazhuang Miao', 'Xiaolong Zhao']
- **Abstrat**: A control scheme is proposed that leverages reinforcement learning to enhance entanglement by modulating the two-photon-driven amplitude in a Rabi model. The quantum phase diagram versus the amplitude of the two-photon process and the coupling between the cavity field and the atom in the Rabi model, is indicated by the energy spectrum of the hybrid system, the witness of entanglement, second order correlation, and negativity of Wigner function. From a dynamical perspective, the behavior of entanglement can reflect the phase transition and the reinforcement learning agent is employed to produce temporal sequences of control pulses to enhance the entanglement in the presence of dissipation. The entanglement can be enhanced in different parameter regimes and the control scheme exhibits robustness against dissipation. The replaceability of the controlled system and the reinforcement learning module demonstrates the generalization of this scheme. This research paves the way of positively enhancing quantum resources in non-equilibrium systems.





## NeoHebbian Synapses to Accelerate Online Training of Neuromorphic Hardware
- **Url**: http://arxiv.org/abs/2411.18272v1
- **Authors**: ['Shubham Pande', 'Sai Sukruth Bezugam', 'Tinish Bhattacharya', 'Ewelina Wlazlak', 'Anjan Chakaravorty', 'Bhaswar Chakrabarti', 'Dmitri Strukov']
- **Abstrat**: Neuromorphic systems that employ advanced synaptic learning rules, such as the three-factor learning rule, require synaptic devices of increased complexity. Herein, a novel neoHebbian artificial synapse utilizing ReRAM devices has been proposed and experimentally validated to meet this demand. This synapse features two distinct state variables: a neuron coupling weight and an "eligibility trace" that dictates synaptic weight updates. The coupling weight is encoded in the ReRAM conductance, while the "eligibility trace" is encoded in the local temperature of the ReRAM and is modulated by applying voltage pulses to a physically co-located resistive heating element. The utility of the proposed synapse has been investigated using two representative tasks: first, temporal signal classification using Recurrent Spiking Neural Networks (RSNNs) employing the e-prop algorithm, and second, Reinforcement Learning (RL) for path planning tasks in feedforward networks using a modified version of the same learning rule. System-level simulations, accounting for various device and system-level non-idealities, confirm that these synapses offer a robust solution for the fast, compact, and energy-efficient implementation of advanced learning rules in neuromorphic hardware.





## Dynamic Retail Pricing via Q-Learning -- A Reinforcement Learning Framework for Enhanced Revenue Management
- **Url**: http://arxiv.org/abs/2411.18261v1
- **Authors**: ['Mohit Apte', 'Ketan Kale', 'Pranav Datar', 'Pratiksha Deshmukh']
- **Abstrat**: This paper explores the application of a reinforcement learning (RL) framework using the Q-Learning algorithm to enhance dynamic pricing strategies in the retail sector. Unlike traditional pricing methods, which often rely on static demand models, our RL approach continuously adapts to evolving market dynamics, offering a more flexible and responsive pricing strategy. By creating a simulated retail environment, we demonstrate how RL effectively addresses real-time changes in consumer behavior and market conditions, leading to improved revenue outcomes. Our results illustrate that the RL model not only surpasses traditional methods in terms of revenue generation but also provides insights into the complex interplay of price elasticity and consumer demand. This research underlines the significant potential of applying artificial intelligence in economic decision-making, paving the way for more sophisticated, data-driven pricing models in various commercial domains.





## Dependency-Aware CAV Task Scheduling via Diffusion-Based Reinforcement Learning
- **Url**: http://arxiv.org/abs/2411.18230v1
- **Authors**: ['Xiang Cheng', 'Zhi Mao', 'Ying Wang', 'Wen Wu']
- **Abstrat**: In this paper, we propose a novel dependency-aware task scheduling strategy for dynamic unmanned aerial vehicle-assisted connected autonomous vehicles (CAVs). Specifically, different computation tasks of CAVs consisting of multiple dependency subtasks are judiciously assigned to nearby CAVs or the base station for promptly completing tasks. Therefore, we formulate a joint scheduling priority and subtask assignment optimization problem with the objective of minimizing the average task completion time. The problem aims at improving the long-term system performance, which is reformulated as a Markov decision process. To solve the problem, we further propose a diffusion-based reinforcement learning algorithm, named Synthetic DDQN based Subtasks Scheduling, which can make adaptive task scheduling decision in real time. A diffusion model-based synthetic experience replay is integrated into the reinforcement learning framework, which can generate sufficient synthetic data in experience replay buffer, thereby significantly accelerating convergence and improving sample efficiency. Simulation results demonstrate the effectiveness of the proposed algorithm on reducing task completion time, comparing to benchmark schemes.





## Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning
- **Url**: http://arxiv.org/abs/2411.18203v1
- **Authors**: ['Di Zhang', 'Jingdi Lei', 'Junxian Li', 'Xunzhi Wang', 'Yujie Liu', 'Zonglin Yang', 'Jiatong Li', 'Weida Wang', 'Suorong Yang', 'Jianbo Wu', 'Peng Ye', 'Wanli Ouyang', 'Dongzhan Zhou']
- **Abstrat**: Vision-language models~(VLMs) have shown remarkable advancements in multimodal reasoning tasks. However, they still often generate inaccurate or irrelevant responses due to issues like hallucinated image understandings or unrefined reasoning paths. To address these challenges, we introduce Critic-V, a novel framework inspired by the Actor-Critic paradigm to boost the reasoning capability of VLMs. This framework decouples the reasoning process and critic process by integrating two independent components: the Reasoner, which generates reasoning paths based on visual and textual inputs, and the Critic, which provides constructive critique to refine these paths. In this approach, the Reasoner generates reasoning responses according to text prompts, which can evolve iteratively as a policy based on feedback from the Critic. This interaction process was theoretically driven by a reinforcement learning framework where the Critic offers natural language critiques instead of scalar rewards, enabling more nuanced feedback to boost the Reasoner's capability on complex reasoning tasks. The Critic model is trained using Direct Preference Optimization (DPO), leveraging a preference dataset of critiques ranked by Rule-based Reward(RBR) to enhance its critic capabilities. Evaluation results show that the Critic-V framework significantly outperforms existing methods, including GPT-4V, on 5 out of 8 benchmarks, especially regarding reasoning accuracy and efficiency. Combining a dynamic text-based policy for the Reasoner and constructive feedback from the preference-optimized Critic enables a more reliable and context-sensitive multimodal reasoning process. Our approach provides a promising solution to enhance the reliability of VLMs, improving their performance in real-world reasoning-heavy multimodal applications such as autonomous driving and embodied intelligence.





## Scalable Multi-Objective Reinforcement Learning with Fairness Guarantees using Lorenz Dominance
- **Url**: http://arxiv.org/abs/2411.18195v1
- **Authors**: ['Dimitris Michailidis', 'Willem Röpke', 'Diederik M. Roijers', 'Sennay Ghebreab', 'Fernando P. Santos']
- **Abstrat**: Multi-Objective Reinforcement Learning (MORL) aims to learn a set of policies that optimize trade-offs between multiple, often conflicting objectives. MORL is computationally more complex than single-objective RL, particularly as the number of objectives increases. Additionally, when objectives involve the preferences of agents or groups, ensuring fairness is socially desirable. This paper introduces a principled algorithm that incorporates fairness into MORL while improving scalability to many-objective problems. We propose using Lorenz dominance to identify policies with equitable reward distributions and introduce {\lambda}-Lorenz dominance to enable flexible fairness preferences. We release a new, large-scale real-world transport planning environment and demonstrate that our method encourages the discovery of fair policies, showing improved scalability in two large cities (Xi'an and Amsterdam). Our methods outperform common multi-objective approaches, particularly in high-dimensional objective spaces.





## Age-minimal Multicast by Graph Attention Reinforcement Learning
- **Url**: http://arxiv.org/abs/2404.18084v4
- **Authors**: ['Yanning Zhang', 'Guocheng Liao', 'Shengbin Cao', 'Ning Yang', 'Nikolaos Pappas', 'Meng Zhang']
- **Abstrat**: Age of Information (AoI) has emerged as a prominent metric for evaluating the timeliness of information in time-critical applications. Applications, including video streaming, virtual reality, and metaverse platforms, necessitate the use of multicast communication. Optimizing AoI in multicast networks is challenging due to the coupled multicast routing and scheduling decisions, the network dynamics, and the complexity of the multicast. This paper focuses on dynamic multicast networks and aims to minimize the expected average AoI through the integration of multicast routing and scheduling. To address the inherent complexity of the problem, we first propose to apply reinforcement learning (RL) to learn the heuristics of multicast routing, based on which we decompose the original problem into two subtasks that are amenable to hierarchical RL methods. Subsequently, we propose an innovative framework based on graph attention networks (GATs) and prove its contraction mapping property. Such a GAT framework effectively captures graph information used in the hierarchical RL framework with superior generalization capabilities. To validate our framework, we conduct experiments on three datasets, including a real-world dataset called AS-733, and show that our proposed scheme reduces the average weighted AoI by $38.2\%$ and the weighted peak age by $43.4\%$ compared to baselines over all datasets in dynamic networks.





## Actor-Critic Model Predictive Control: Differentiable Optimization meets Reinforcement Learning
- **Url**: http://arxiv.org/abs/2306.09852v6
- **Authors**: ['Angel Romero', 'Elie Aljalbout', 'Yunlong Song', 'Davide Scaramuzza']
- **Abstrat**: An open research question in robotics is how to combine the benefits of model-free reinforcement learning (RL) -- known for its strong task performance and flexibility in optimizing general reward formulations -- with the robustness and online replanning capabilities of model predictive control (MPC). This paper provides an answer by introducing a new framework called Actor-Critic Model Predictive Control. The key idea is to embed a differentiable MPC within an actor-critic RL framework. This integration allows for short-term predictive optimization of control actions through MPC, while leveraging RL for end-to-end learning and exploration over longer horizons. Through various ablation studies, we expose the benefits of the proposed approach: it achieves better out-of-distribution behaviour, better robustness to changes in the dynamics and improved sample efficiency. Additionally, we conduct an empirical analysis that reveals a relationship between the critic's learned value function and the cost function of the differentiable MPC, providing a deeper understanding of the interplay between the critic's value and the MPC cost functions. Finally, we validate our method in the drone racing task in various tracks, in both simulation and the real world. Our method achieves the same superhuman performance as state-of-the-art model-free RL, showcasing speeds of up to 21 m/s. We show that the proposed architecture can achieve real-time control performance, learn complex behaviors via trial and error, and retain the predictive properties of the MPC to better handle out of distribution behavior.





# TD3
# Prioritized Experience Replay
# path planning
## NeoHebbian Synapses to Accelerate Online Training of Neuromorphic Hardware
- **Url**: http://arxiv.org/abs/2411.18272v1
- **Authors**: ['Shubham Pande', 'Sai Sukruth Bezugam', 'Tinish Bhattacharya', 'Ewelina Wlazlak', 'Anjan Chakaravorty', 'Bhaswar Chakrabarti', 'Dmitri Strukov']
- **Abstrat**: Neuromorphic systems that employ advanced synaptic learning rules, such as the three-factor learning rule, require synaptic devices of increased complexity. Herein, a novel neoHebbian artificial synapse utilizing ReRAM devices has been proposed and experimentally validated to meet this demand. This synapse features two distinct state variables: a neuron coupling weight and an "eligibility trace" that dictates synaptic weight updates. The coupling weight is encoded in the ReRAM conductance, while the "eligibility trace" is encoded in the local temperature of the ReRAM and is modulated by applying voltage pulses to a physically co-located resistive heating element. The utility of the proposed synapse has been investigated using two representative tasks: first, temporal signal classification using Recurrent Spiking Neural Networks (RSNNs) employing the e-prop algorithm, and second, Reinforcement Learning (RL) for path planning tasks in feedforward networks using a modified version of the same learning rule. System-level simulations, accounting for various device and system-level non-idealities, confirm that these synapses offer a robust solution for the fast, compact, and energy-efficient implementation of advanced learning rules in neuromorphic hardware.





## SCoTT: Wireless-Aware Path Planning with Vision Language Models and Strategic Chains-of-Thought
- **Url**: http://arxiv.org/abs/2411.18212v1
- **Authors**: ['Aladin Djuhera', 'Vlad C. Andrei', 'Amin Seffo', 'Holger Boche', 'Walid Saad']
- **Abstrat**: Path planning is a complex problem for many practical applications, particularly in robotics. Existing algorithms, however, are exhaustive in nature and become increasingly complex when additional side constraints are incorporated alongside distance minimization. In this paper, a novel approach using vision language models (VLMs) is proposed for enabling path planning in complex wireless-aware environments. To this end, insights from a digital twin (DT) with real-world wireless ray tracing data are explored in order to guarantee an average path gain threshold while minimizing the trajectory length. First, traditional approaches such as A* are compared to several wireless-aware extensions, and an optimal iterative dynamic programming approach (DP-WA*) is derived, which fully takes into account all path gains and distance metrics within the DT. On the basis of these baselines, the role of VLMs as an alternative assistant for path planning is investigated, and a strategic chain-of-thought tasking (SCoTT) approach is proposed. SCoTT divides the complex planning task into several subproblems and solves each with advanced CoT prompting. Results show that SCoTT achieves very close average path gains compared to DP-WA* while at the same time yielding consistently shorter path lengths. The results also show that VLMs can be used to accelerate DP-WA* by efficiently reducing the algorithm's search space and thus saving up to 62\% in execution time. This work underscores the potential of VLMs in future digital systems as capable assistants for solving complex tasks, while enhancing user interaction and accelerating rapid prototyping under diverse wireless constraints.





## SuperFusion: Multilevel LiDAR-Camera Fusion for Long-Range HD Map Generation
- **Url**: http://arxiv.org/abs/2211.15656v4
- **Authors**: ['Hao Dong', 'Weihao Gu', 'Xianjing Zhang', 'Jintao Xu', 'Rui Ai', 'Huimin Lu', 'Juho Kannala', 'Xieyuanli Chen']
- **Abstrat**: High-definition (HD) semantic map generation of the environment is an essential component of autonomous driving. Existing methods have achieved good performance in this task by fusing different sensor modalities, such as LiDAR and camera. However, current works are based on raw data or network feature-level fusion and only consider short-range HD map generation, limiting their deployment to realistic autonomous driving applications. In this paper, we focus on the task of building the HD maps in both short ranges, i.e., within 30 m, and also predicting long-range HD maps up to 90 m, which is required by downstream path planning and control tasks to improve the smoothness and safety of autonomous driving. To this end, we propose a novel network named SuperFusion, exploiting the fusion of LiDAR and camera data at multiple levels. We use LiDAR depth to improve image depth estimation and use image features to guide long-range LiDAR feature prediction. We benchmark our SuperFusion on the nuScenes dataset and a self-recorded dataset and show that it outperforms the state-of-the-art baseline methods with large margins on all intervals. Additionally, we apply the generated HD map to a downstream path planning task, demonstrating that the long-range HD maps predicted by our method can lead to better path planning for autonomous vehicles. Our code has been released at https://github.com/haomo-ai/SuperFusion.





## A Cost-Effective Approach to Smooth A* Path Planning for Autonomous Vehicles
- **Url**: http://arxiv.org/abs/2411.18150v1
- **Authors**: ['Lukas Schichler', 'Karin Festl', 'Selim Solmaz', 'Daniel Watzenig']
- **Abstrat**: Path planning for wheeled mobile robots is a critical component in the field of automation and intelligent transportation systems. Car-like vehicles, which have non-holonomic constraints on their movement capability impose additional requirements on the planned paths. Traditional path planning algorithms, such as A* , are widely used due to their simplicity and effectiveness in finding optimal paths in complex environments. However, these algorithms often do not consider vehicle dynamics, resulting in paths that are infeasible or impractical for actual driving. Specifically, a path that minimizes the number of grid cells may still be too curvy or sharp for a car-like vehicle to navigate smoothly. This paper addresses the need for a path planning solution that not only finds a feasible path but also ensures that the path is smooth and drivable. By adapting the A* algorithm for a curvature constraint and incorporating a cost function that considers the smoothness of possible paths, we aim to bridge the gap between grid based path planning and smooth paths that are drivable by car-like vehicles. The proposed method leverages motion primitives, pre-computed using a ribbon based path planner that produces smooth paths of minimum curvature. The motion primitives guide the A* algorithm in finding paths of minimal length and curvature. With the proposed modification on the A* algorithm, the planned paths can be constraint to have a minimum turning radius much larger than the grid size. We demonstrate the effectiveness of the proposed algorithm in different unstructured environments. In a two-stage planning approach, first the modified A* algorithm finds a grid-based path and the ribbon based path planner creates a smooth path within the area of grid cells. The resulting paths are smooth with small curvatures independent of the orientation of the grid axes and even in presence of sharp obstacles.




