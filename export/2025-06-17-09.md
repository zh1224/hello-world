# reinforcement learning
## Touch begins where vision ends: Generalizable policies for contact-rich manipulation
- **Url**: http://arxiv.org/abs/2506.13762v1
- **Authors**: ['Zifan Zhao', 'Siddhant Haldar', 'Jinda Cui', 'Lerrel Pinto', 'Raunaq Bhirangi']
- **Abstrat**: Data-driven approaches struggle with precise manipulation; imitation learning requires many hard-to-obtain demonstrations, while reinforcement learning yields brittle, non-generalizable policies. We introduce VisuoTactile Local (ViTaL) policy learning, a framework that solves fine-grained manipulation tasks by decomposing them into two phases: a reaching phase, where a vision-language model (VLM) enables scene-level reasoning to localize the object of interest, and a local interaction phase, where a reusable, scene-agnostic ViTaL policy performs contact-rich manipulation using egocentric vision and tactile sensing. This approach is motivated by the observation that while scene context varies, the low-level interaction remains consistent across task instances. By training local policies once in a canonical setting, they can generalize via a localize-then-execute strategy. ViTaL achieves around 90% success on contact-rich tasks in unseen environments and is robust to distractors. ViTaL's effectiveness stems from three key insights: (1) foundation models for segmentation enable training robust visual encoders via behavior cloning; (2) these encoders improve the generalizability of policies learned using residual RL; and (3) tactile sensing significantly boosts performance in contact-rich tasks. Ablation studies validate each of these insights, and we demonstrate that ViTaL integrates well with high-level VLMs, enabling robust, reusable low-level skills. Results and videos are available at https://vitalprecise.github.io.





## MARCO: Hardware-Aware Neural Architecture Search for Edge Devices with Multi-Agent Reinforcement Learning and Conformal Prediction Filtering
- **Url**: http://arxiv.org/abs/2506.13755v1
- **Authors**: ['Arya Fayyazi', 'Mehdi Kamal', 'Massoud Pedram']
- **Abstrat**: This paper introduces MARCO (Multi-Agent Reinforcement learning with Conformal Optimization), a novel hardware-aware framework for efficient neural architecture search (NAS) targeting resource-constrained edge devices. By significantly reducing search time and maintaining accuracy under strict hardware constraints, MARCO bridges the gap between automated DNN design and CAD for edge AI deployment. MARCO's core technical contribution lies in its unique combination of multi-agent reinforcement learning (MARL) with Conformal Prediction (CP) to accelerate the hardware/software co-design process for deploying deep neural networks. Unlike conventional once-for-all (OFA) supernet approaches that require extensive pretraining, MARCO decomposes the NAS task into a hardware configuration agent (HCA) and a Quantization Agent (QA). The HCA optimizes high-level design parameters, while the QA determines per-layer bit-widths under strict memory and latency budgets using a shared reward signal within a centralized-critic, decentralized-execution (CTDE) paradigm. A key innovation is the integration of a calibrated CP surrogate model that provides statistical guarantees (with a user-defined miscoverage rate) to prune unpromising candidate architectures before incurring the high costs of partial training or hardware simulation. This early filtering drastically reduces the search space while ensuring that high-quality designs are retained with a high probability. Extensive experiments on MNIST, CIFAR-10, and CIFAR-100 demonstrate that MARCO achieves a 3-4x reduction in total search time compared to an OFA baseline while maintaining near-baseline accuracy (within 0.3%). Furthermore, MARCO also reduces inference latency. Validation on a MAX78000 evaluation board confirms that simulator trends hold in practice, with simulator estimates deviating from measured values by less than 5%.





## PB$^2$: Preference Space Exploration via Population-Based Methods in Preference-Based Reinforcement Learning
- **Url**: http://arxiv.org/abs/2506.13741v1
- **Authors**: ['Brahim Driss', 'Alex Davey', 'Riad Akrour']
- **Abstrat**: Preference-based reinforcement learning (PbRL) has emerged as a promising approach for learning behaviors from human feedback without predefined reward functions. However, current PbRL methods face a critical challenge in effectively exploring the preference space, often converging prematurely to suboptimal policies that satisfy only a narrow subset of human preferences. In this work, we identify and address this preference exploration problem through population-based methods. We demonstrate that maintaining a diverse population of agents enables more comprehensive exploration of the preference landscape compared to single-agent approaches. Crucially, this diversity improves reward model learning by generating preference queries with clearly distinguishable behaviors, a key factor in real-world scenarios where humans must easily differentiate between options to provide meaningful feedback. Our experiments reveal that current methods may fail by getting stuck in local optima, requiring excessive feedback, or degrading significantly when human evaluators make errors on similar trajectories, a realistic scenario often overlooked by methods relying on perfect oracle teachers. Our population-based approach demonstrates robust performance when teachers mislabel similar trajectory segments and shows significantly enhanced preference exploration capabilities,particularly in environments with complex reward landscapes.





## TimeMaster: Training Time-Series Multimodal LLMs to Reason via Reinforcement Learning
- **Url**: http://arxiv.org/abs/2506.13705v1
- **Authors**: ['Junru Zhang', 'Lang Feng', 'Xu Guo', 'Yuhan Wu', 'Yabo Dong', 'Duanqing Xu']
- **Abstrat**: Time-series reasoning remains a significant challenge in multimodal large language models (MLLMs) due to the dynamic temporal patterns, ambiguous semantics, and lack of temporal priors. In this work, we introduce TimeMaster, a reinforcement learning (RL)-based method that enables time-series MLLMs to perform structured, interpretable reasoning directly over visualized time-series inputs and task prompts. TimeMaster adopts a three-part structured output format, reasoning, classification, and domain-specific extension, and is optimized via a composite reward function that aligns format adherence, prediction accuracy, and open-ended insight quality. The model is trained using a two-stage pipeline: we first apply supervised fine-tuning (SFT) to establish a good initialization, followed by Group Relative Policy Optimization (GRPO) at the token level to enable stable and targeted reward-driven improvement in time-series reasoning. We evaluate TimeMaster on the TimerBed benchmark across six real-world classification tasks based on Qwen2.5-VL-3B-Instruct. TimeMaster achieves state-of-the-art performance, outperforming both classical time-series models and few-shot GPT-4o by over 14.6% and 7.3% performance gain, respectively. Notably, TimeMaster goes beyond time-series classification: it also exhibits expert-like reasoning behavior, generates context-aware explanations, and delivers domain-aligned insights. Our results highlight that reward-driven RL can be a scalable and promising path toward integrating temporal understanding into time-series MLLMs.





## Value-Free Policy Optimization via Reward Partitioning
- **Url**: http://arxiv.org/abs/2506.13702v1
- **Authors**: ['Bilal Faye', 'Hanane Azzag', 'Mustapha Lebbah']
- **Abstrat**: Single-trajectory reinforcement learning (RL) methods aim to optimize policies from datasets consisting of (prompt, response, reward) triplets, where scalar rewards are directly available. This supervision format is highly practical, as it mirrors real-world human feedback, such as thumbs-up/down signals, and avoids the need for structured preference annotations. In contrast, pairwise preference-based methods like Direct Preference Optimization (DPO) rely on datasets with both preferred and dispreferred responses, which are harder to construct and less natural to collect. Among single-trajectory approaches, Direct Reward Optimization (DRO) has shown strong empirical performance due to its simplicity and stability. However, DRO requires approximating a value function, which introduces several limitations: high off-policy variance, coupling between policy and value learning, and a lack of absolute supervision on the policy itself. We introduce Reward Partitioning Optimization (RPO), a new method that resolves these limitations by removing the need to model the value function. Instead, RPO normalizes observed rewards using a partitioning approach estimated directly from data. This leads to a straightforward supervised learning objective on the policy, with no auxiliary models and no joint optimization. RPO provides direct and stable supervision on the policy, making it robust and easy to implement in practice. We validate RPO on scalar-feedback language modeling tasks using Flan-T5 encoder-decoder models. Our results demonstrate that RPO outperforms existing single-trajectory baselines such as DRO and Kahneman-Tversky Optimization (KTO). These findings confirm that RPO is a simple, effective, and theoretically grounded method for single-trajectory policy optimization.





## OneRec Technical Report
- **Url**: http://arxiv.org/abs/2506.13695v1
- **Authors**: ['Guorui Zhou', 'Jiaxin Deng', 'Jinghao Zhang', 'Kuo Cai', 'Lejian Ren', 'Qiang Luo', 'Qianqian Wang', 'Qigen Hu', 'Rui Huang', 'Shiyao Wang', 'Weifeng Ding', 'Wuchao Li', 'Xinchen Luo', 'Xingmei Wang', 'Zexuan Cheng', 'Zixing Zhang', 'Bin Zhang', 'Boxuan Wang', 'Chaoyi Ma', 'Chengru Song', 'Chenhui Wang', 'Di Wang', 'Dongxue Meng', 'Fan Yang', 'Fangyu Zhang', 'Feng Jiang', 'Fuxing Zhang', 'Gang Wang', 'Guowang Zhang', 'Han Li', 'Hengrui Hu', 'Hezheng Lin', 'Hongtao Cheng', 'Hongyang Cao', 'Huanjie Wang', 'Jiaming Huang', 'Jiapeng Chen', 'Jiaqiang Liu', 'Jinghui Jia', 'Kun Gai', 'Lantao Hu', 'Liang Zeng', 'Liao Yu', 'Qiang Wang', 'Qidong Zhou', 'Shengzhe Wang', 'Shihui He', 'Shuang Yang', 'Shujie Yang', 'Sui Huang', 'Tao Wu', 'Tiantian He', 'Tingting Gao', 'Wei Yuan', 'Xiao Liang', 'Xiaoxiao Xu', 'Xugang Liu', 'Yan Wang', 'Yi Wang', 'Yiwu Liu', 'Yue Song', 'Yufei Zhang', 'Yunfan Wu', 'Yunfeng Zhao', 'Zhanyu Liu']
- **Abstrat**: Recommender systems have been widely used in various large-scale user-oriented platforms for many years. However, compared to the rapid developments in the AI community, recommendation systems have not achieved a breakthrough in recent years. For instance, they still rely on a multi-stage cascaded architecture rather than an end-to-end approach, leading to computational fragmentation and optimization inconsistencies, and hindering the effective application of key breakthrough technologies from the AI community in recommendation scenarios.   To address these issues, we propose OneRec, which reshapes the recommendation system through an end-to-end generative approach and achieves promising results. Firstly, we have enhanced the computational FLOPs of the current recommendation model by 10 $\times$ and have identified the scaling laws for recommendations within certain boundaries. Secondly, reinforcement learning techniques, previously difficult to apply for optimizing recommendations, show significant potential in this framework. Lastly, through infrastructure optimizations, we have achieved 23.7% and 28.8% Model FLOPs Utilization (MFU) on flagship GPUs during training and inference, respectively, aligning closely with the LLM community. This architecture significantly reduces communication and storage overhead, resulting in operating expense that is only 10.6% of traditional recommendation pipelines. Deployed in Kuaishou/Kuaishou Lite APP, it handles 25% of total queries per second, enhancing overall App Stay Time by 0.54% and 1.24%, respectively. Additionally, we have observed significant increases in metrics such as 7-day Lifetime, which is a crucial indicator of recommendation experience. We also provide practical lessons and insights derived from developing, optimizing, and maintaining a production-scale recommendation system with significant real-world impact.





## Meta-learning how to Share Credit among Macro-Actions
- **Url**: http://arxiv.org/abs/2506.13690v1
- **Authors**: ['Ionel-Alexandru Hosu', 'Traian Rebedea', 'Razvan Pascanu']
- **Abstrat**: One proposed mechanism to improve exploration in reinforcement learning is through the use of macro-actions. Paradoxically though, in many scenarios the naive addition of macro-actions does not lead to better exploration, but rather the opposite. It has been argued that this was caused by adding non-useful macros and multiple works have focused on mechanisms to discover effectively environment-specific useful macros. In this work, we take a slightly different perspective. We argue that the difficulty stems from the trade-offs between reducing the average number of decisions per episode versus increasing the size of the action space. Namely, one typically treats each potential macro-action as independent and atomic, hence strictly increasing the search space and making typical exploration strategies inefficient. To address this problem we propose a novel regularization term that exploits the relationship between actions and macro-actions to improve the credit assignment mechanism by reducing the effective dimension of the action space and, therefore, improving exploration. The term relies on a similarity matrix that is meta-learned jointly with learning the desired policy. We empirically validate our strategy looking at macro-actions in Atari games, and the StreetFighter II environment. Our results show significant improvements over the Rainbow-DQN baseline in all environments. Additionally, we show that the macro-action similarity is transferable to related environments. We believe this work is a small but important step towards understanding how the similarity-imposed geometry on the action space can be exploited to improve credit assignment and exploration, therefore making learning more effective.





## Opt2Skill: Imitating Dynamically-feasible Whole-Body Trajectories for Versatile Humanoid Loco-Manipulation
- **Url**: http://arxiv.org/abs/2409.20514v4
- **Authors**: ['Fukang Liu', 'Zhaoyuan Gu', 'Yilin Cai', 'Ziyi Zhou', 'Hyunyoung Jung', 'Jaehwi Jang', 'Shijie Zhao', 'Sehoon Ha', 'Yue Chen', 'Danfei Xu', 'Ye Zhao']
- **Abstrat**: Humanoid robots are designed to perform diverse loco-manipulation tasks. However, they face challenges due to their high-dimensional and unstable dynamics, as well as the complex contact-rich nature of the tasks. Model-based optimal control methods offer flexibility to define precise motion but are limited by high computational complexity and accurate contact sensing. On the other hand, reinforcement learning (RL) handles high-dimensional spaces with strong robustness but suffers from inefficient learning, unnatural motion, and sim-to-real gaps. To address these challenges, we introduce Opt2Skill, an end-to-end pipeline that combines model-based trajectory optimization with RL to achieve robust whole-body loco-manipulation. Opt2Skill generates dynamic feasible and contact-consistent reference motions for the Digit humanoid robot using differential dynamic programming (DDP) and trains RL policies to track these optimal trajectories. Our results demonstrate that Opt2Skill outperforms baselines that rely on human demonstrations and inverse kinematics-based references, both in motion tracking and task success rates. Furthermore, we show that incorporating trajectories with torque information improves contact force tracking in contact-involved tasks, such as wiping a table. We have successfully transferred our approach to real-world applications.





## The Courage to Stop: Overcoming Sunk Cost Fallacy in Deep Reinforcement Learning
- **Url**: http://arxiv.org/abs/2506.13672v1
- **Authors**: ['Jiashun Liu', 'Johan Obando-Ceron', 'Pablo Samuel Castro', 'Aaron Courville', 'Ling Pan']
- **Abstrat**: Off-policy deep reinforcement learning (RL) typically leverages replay buffers for reusing past experiences during learning. This can help improve sample efficiency when the collected data is informative and aligned with the learning objectives; when that is not the case, it can have the effect of "polluting" the replay buffer with data which can exacerbate optimization challenges in addition to wasting environment interactions due to wasteful sampling. We argue that sampling these uninformative and wasteful transitions can be avoided by addressing the sunk cost fallacy, which, in the context of deep RL, is the tendency towards continuing an episode until termination. To address this, we propose learn to stop (LEAST), a lightweight mechanism that enables strategic early episode termination based on Q-value and gradient statistics, which helps agents recognize when to terminate unproductive episodes early. We demonstrate that our method improves learning efficiency on a variety of RL algorithms, evaluated on both the MuJoCo and DeepMind Control Suite benchmarks.





## We Should Identify and Mitigate Third-Party Safety Risks in MCP-Powered Agent Systems
- **Url**: http://arxiv.org/abs/2506.13666v1
- **Authors**: ['Junfeng Fang', 'Zijun Yao', 'Ruipeng Wang', 'Haokai Ma', 'Xiang Wang', 'Tat-Seng Chua']
- **Abstrat**: The development of large language models (LLMs) has entered in a experience-driven era, flagged by the emergence of environment feedback-driven learning via reinforcement learning and tool-using agents. This encourages the emergenece of model context protocol (MCP), which defines the standard on how should a LLM interact with external services, such as \api and data. However, as MCP becomes the de facto standard for LLM agent systems, it also introduces new safety risks. In particular, MCP introduces third-party services, which are not controlled by the LLM developers, into the agent systems. These third-party MCP services provider are potentially malicious and have the economic incentives to exploit vulnerabilities and sabotage user-agent interactions. In this position paper, we advocate the research community in LLM safety to pay close attention to the new safety risks issues introduced by MCP, and develop new techniques to build safe MCP-powered agent systems. To establish our position, we argue with three key parts. (1) We first construct \framework, a controlled framework to examine safety issues in MCP-powered agent systems. (2) We then conduct a series of pilot experiments to demonstrate the safety risks in MCP-powered agent systems is a real threat and its defense is not trivial. (3) Finally, we give our outlook by showing a roadmap to build safe MCP-powered agent systems. In particular, we would call for researchers to persue the following research directions: red teaming, MCP safe LLM development, MCP safety evaluation, MCP safety data accumulation, MCP service safeguard, and MCP safe ecosystem construction. We hope this position paper can raise the awareness of the research community in MCP safety and encourage more researchers to join this important research direction. Our code is available at https://github.com/littlelittlenine/SafeMCP.git.





## Ego-R1: Chain-of-Tool-Thought for Ultra-Long Egocentric Video Reasoning
- **Url**: http://arxiv.org/abs/2506.13654v1
- **Authors**: ['Shulin Tian', 'Ruiqi Wang', 'Hongming Guo', 'Penghao Wu', 'Yuhao Dong', 'Xiuying Wang', 'Jingkang Yang', 'Hao Zhang', 'Hongyuan Zhu', 'Ziwei Liu']
- **Abstrat**: We introduce Ego-R1, a novel framework for reasoning over ultra-long (i.e., in days and weeks) egocentric videos, which leverages a structured Chain-of-Tool-Thought (CoTT) process, orchestrated by an Ego-R1 Agent trained via reinforcement learning (RL). Inspired by human problem-solving strategies, CoTT decomposes complex reasoning into modular steps, with the RL agent invoking specific tools, one per step, to iteratively and collaboratively answer sub-questions tackling such tasks as temporal retrieval and multi-modal understanding. We design a two-stage training paradigm involving supervised finetuning (SFT) of a pretrained language model using CoTT data and RL to enable our agent to dynamically propose step-by-step tools for long-range reasoning. To facilitate training, we construct a dataset called Ego-R1 Data, which consists of Ego-CoTT-25K for SFT and Ego-QA-4.4K for RL. Furthermore, our Ego-R1 agent is evaluated on a newly curated week-long video QA benchmark, Ego-R1 Bench, which contains human-verified QA pairs from hybrid sources. Extensive results demonstrate that the dynamic, tool-augmented chain-of-thought reasoning by our Ego-R1 Agent can effectively tackle the unique challenges of understanding ultra-long egocentric videos, significantly extending the time coverage from few hours to a week.





## Optimistic Q-learning for average reward and episodic reinforcement learning
- **Url**: http://arxiv.org/abs/2407.13743v3
- **Authors**: ['Priyank Agrawal', 'Shipra Agrawal']
- **Abstrat**: We present an optimistic Q-learning algorithm for regret minimization in average reward reinforcement learning under an additional assumption on the underlying MDP that for all policies, the time to visit some frequent state $s_0$ is finite and upper bounded by $H$, either in expectation or with constant probability. Our setting strictly generalizes the episodic setting and is significantly less restrictive than the assumption of bounded hitting time \textit{for all states} made by most previous literature on model-free algorithms in average reward settings. We demonstrate a regret bound of $\tilde{O}(H^5 S\sqrt{AT})$, where $S$ and $A$ are the numbers of states and actions, and $T$ is the horizon. A key technical novelty of our work is the introduction of an $\overline{L}$ operator defined as $\overline{L} v = \frac{1}{H} \sum_{h=1}^H L^h v$ where $L$ denotes the Bellman operator. Under the given assumption, we show that the $\overline{L}$ operator has a strict contraction (in span) even in the average-reward setting where the discount factor is $1$. Our algorithm design uses ideas from episodic Q-learning to estimate and apply this operator iteratively. Thus, we provide a unified view of regret minimization in episodic and non-episodic settings, which may be of independent interest.





## JAEGER: Dual-Level Humanoid Whole-Body Controller
- **Url**: http://arxiv.org/abs/2505.06584v2
- **Authors**: ['Ziluo Ding', 'Haobin Jiang', 'Yuxuan Wang', 'Zhenguo Sun', 'Yu Zhang', 'Xiaojie Niu', 'Ming Yang', 'Weishuai Zeng', 'Xinrun Xu', 'Zongqing Lu']
- **Abstrat**: This paper presents JAEGER, a dual-level whole-body controller for humanoid robots that addresses the challenges of training a more robust and versatile policy. Unlike traditional single-controller approaches, JAEGER separates the control of the upper and lower bodies into two independent controllers, so that they can better focus on their distinct tasks. This separation alleviates the dimensionality curse and improves fault tolerance. JAEGER supports both root velocity tracking (coarse-grained control) and local joint angle tracking (fine-grained control), enabling versatile and stable movements. To train the controller, we utilize a human motion dataset (AMASS), retargeting human poses to humanoid poses through an efficient retargeting network, and employ a curriculum learning approach. This method performs supervised learning for initialization, followed by reinforcement learning for further exploration. We conduct our experiments on two humanoid platforms and demonstrate the superiority of our approach against state-of-the-art methods in both simulation and real environments.





## MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention
- **Url**: http://arxiv.org/abs/2506.13585v1
- **Authors**: ['MiniMax', ':', 'Aili Chen', 'Aonian Li', 'Bangwei Gong', 'Binyang Jiang', 'Bo Fei', 'Bo Yang', 'Boji Shan', 'Changqing Yu', 'Chao Wang', 'Cheng Zhu', 'Chengjun Xiao', 'Chengyu Du', 'Chi Zhang', 'Chu Qiao', 'Chunhao Zhang', 'Chunhui Du', 'Congchao Guo', 'Da Chen', 'Deming Ding', 'Dianjun Sun', 'Dong Li', 'Enwei Jiao', 'Haigang Zhou', 'Haimo Zhang', 'Han Ding', 'Haohai Sun', 'Haoyu Feng', 'Huaiguang Cai', 'Haichao Zhu', 'Jian Sun', 'Jiaqi Zhuang', 'Jiaren Cai', 'Jiayuan Song', 'Jin Zhu', 'Jingyang Li', 'Jinhao Tian', 'Jinli Liu', 'Junhao Xu', 'Junjie Yan', 'Junteng Liu', 'Junxian He', 'Kaiyi Feng', 'Ke Yang', 'Kecheng Xiao', 'Le Han', 'Leyang Wang', 'Lianfei Yu', 'Liheng Feng', 'Lin Li', 'Lin Zheng', 'Linge Du', 'Lingyu Yang', 'Lunbin Zeng', 'Minghui Yu', 'Mingliang Tao', 'Mingyuan Chi', 'Mozhi Zhang', 'Mujie Lin', 'Nan Hu', 'Nongyu Di', 'Peng Gao', 'Pengfei Li', 'Pengyu Zhao', 'Qibing Ren', 'Qidi Xu', 'Qile Li', 'Qin Wang', 'Rong Tian', 'Ruitao Leng', 'Shaoxiang Chen', 'Shaoyu Chen', 'Shengmin Shi', 'Shitong Weng', 'Shuchang Guan', 'Shuqi Yu', 'Sichen Li', 'Songquan Zhu', 'Tengfei Li', 'Tianchi Cai', 'Tianrun Liang', 'Weiyu Cheng', 'Weize Kong', 'Wenkai Li', 'Xiancai Chen', 'Xiangjun Song', 'Xiao Luo', 'Xiao Su', 'Xiaobo Li', 'Xiaodong Han', 'Xinzhu Hou', 'Xuan Lu', 'Xun Zou', 'Xuyang Shen', 'Yan Gong', 'Yan Ma', 'Yang Wang', 'Yiqi Shi', 'Yiran Zhong', 'Yonghong Duan', 'Yongxiang Fu', 'Yongyi Hu', 'Yu Gao', 'Yuanxiang Fan', 'Yufeng Yang', 'Yuhao Li', 'Yulin Hu', 'Yunan Huang', 'Yunji Li', 'Yunzhi Xu', 'Yuxin Mao', 'Yuxuan Shi', 'Yuze Wenren', 'Zehan Li', 'Zelin Li', 'Zhanxu Tian', 'Zhengmao Zhu', 'Zhenhua Fan', 'Zhenzhen Wu', 'Zhichao Xu', 'Zhihang Yu', 'Zhiheng Lyu', 'Zhuo Jiang', 'Zibo Gao', 'Zijia Wu', 'Zijian Song', 'Zijun Sun']
- **Abstrat**: We introduce MiniMax-M1, the world's first open-weight, large-scale hybrid-attention reasoning model. MiniMax-M1 is powered by a hybrid Mixture-of-Experts (MoE) architecture combined with a lightning attention mechanism. The model is developed based on our previous MiniMax-Text-01 model, which contains a total of 456 billion parameters with 45.9 billion parameters activated per token. The M1 model natively supports a context length of 1 million tokens, 8x the context size of DeepSeek R1. Furthermore, the lightning attention mechanism in MiniMax-M1 enables efficient scaling of test-time compute. These properties make M1 particularly suitable for complex tasks that require processing long inputs and thinking extensively. MiniMax-M1 is trained using large-scale reinforcement learning (RL) on diverse problems including sandbox-based, real-world software engineering environments. In addition to M1's inherent efficiency advantage for RL training, we propose CISPO, a novel RL algorithm to further enhance RL efficiency. CISPO clips importance sampling weights rather than token updates, outperforming other competitive RL variants. Combining hybrid-attention and CISPO enables MiniMax-M1's full RL training on 512 H800 GPUs to complete in only three weeks, with a rental cost of just $534,700. We release two versions of MiniMax-M1 models with 40K and 80K thinking budgets respectively, where the 40K model represents an intermediate phase of the 80K training. Experiments on standard benchmarks show that our models are comparable or superior to strong open-weight models such as the original DeepSeek-R1 and Qwen3-235B, with particular strengths in complex software engineering, tool utilization, and long-context tasks. We publicly release MiniMax-M1 at https://github.com/MiniMax-AI/MiniMax-M1.





## Can you see how I learn? Human observers' inferences about Reinforcement Learning agents' learning processes
- **Url**: http://arxiv.org/abs/2506.13583v1
- **Authors**: ['Bernhard Hilpert', 'Muhan Hou', 'Kim Baraka', 'Joost Broekens']
- **Abstrat**: Reinforcement Learning (RL) agents often exhibit learning behaviors that are not intuitively interpretable by human observers, which can result in suboptimal feedback in collaborative teaching settings. Yet, how humans perceive and interpret RL agent's learning behavior is largely unknown. In a bottom-up approach with two experiments, this work provides a data-driven understanding of the factors of human observers' understanding of the agent's learning process. A novel, observation-based paradigm to directly assess human inferences about agent learning was developed. In an exploratory interview study (\textit{N}=9), we identify four core themes in human interpretations: Agent Goals, Knowledge, Decision Making, and Learning Mechanisms. A second confirmatory study (\textit{N}=34) applied an expanded version of the paradigm across two tasks (navigation/manipulation) and two RL algorithms (tabular/function approximation). Analyses of 816 responses confirmed the reliability of the paradigm and refined the thematic framework, revealing how these themes evolve over time and interrelate. Our findings provide a human-centered understanding of how people make sense of agent learning, offering actionable insights for designing interpretable RL systems and improving transparency in Human-Robot Interaction.





## A Production Scheduling Framework for Reinforcement Learning Under Real-World Constraints
- **Url**: http://arxiv.org/abs/2506.13566v1
- **Authors**: ['Jonathan Hoss', 'Felix Schelling', 'Noah Klarmann']
- **Abstrat**: The classical Job Shop Scheduling Problem (JSSP) focuses on optimizing makespan under deterministic constraints. Real-world production environments introduce additional complexities that cause traditional scheduling approaches to be less effective. Reinforcement learning (RL) holds potential in addressing these challenges, as it allows agents to learn adaptive scheduling strategies. However, there is a lack of a comprehensive, general-purpose frameworks for effectively training and evaluating RL agents under real-world constraints. To address this gap, we propose a modular framework that extends classical JSSP formulations by incorporating key \mbox{real-world} constraints inherent to the shopfloor, including transport logistics, buffer management, machine breakdowns, setup times, and stochastic processing conditions, while also supporting multi-objective optimization. The framework is a customizable solution that offers flexibility in defining problem instances and configuring simulation parameters, enabling adaptation to diverse production scenarios. A standardized interface ensures compatibility with various RL approaches, providing a robust environment for training RL agents and facilitating the standardized comparison of different scheduling methods under dynamic and uncertain conditions. We release JobShopLab as an open-source tool for both research and industrial applications, accessible at: https://github.com/proto-lab-ro/jobshoplab





## A Dataless Reinforcement Learning Approach to Rounding Hyperplane Optimization for Max-Cut
- **Url**: http://arxiv.org/abs/2505.13405v4
- **Authors**: ['Gabriel Maliakal', 'Ismail Alkhouri', 'Alvaro Velasquez', 'Adam M Alessio', 'Saiprasad Ravishankar']
- **Abstrat**: The Maximum Cut (MaxCut) problem is NP-Complete, and obtaining its optimal solution is NP-hard in the worst case. As a result, heuristic-based algorithms are commonly used, though their design often requires significant domain expertise. More recently, learning-based methods trained on large (un)labeled datasets have been proposed; however, these approaches often struggle with generalizability and scalability. A well-known approximation algorithm for MaxCut is the Goemans-Williamson (GW) algorithm, which relaxes the Quadratic Unconstrained Binary Optimization (QUBO) formulation into a semidefinite program (SDP). The GW algorithm then applies hyperplane rounding by uniformly sampling a random hyperplane to convert the SDP solution into binary node assignments. In this paper, we propose a training-data-free approach based on a non-episodic reinforcement learning formulation, in which an agent learns to select improved rounding hyperplanes that yield better cuts than those produced by the GW algorithm. By optimizing over a Markov Decision Process (MDP), our method consistently achieves better cuts across large-scale graphs with varying densities and degree distributions.





## Learning Swing-up Maneuvers for a Suspended Aerial Manipulation Platform in a Hierarchical Control Framework
- **Url**: http://arxiv.org/abs/2506.13478v1
- **Authors**: ['Hemjyoti Das', 'Minh Nhat Vu', 'Christian Ott']
- **Abstrat**: In this work, we present a novel approach to augment a model-based control method with a reinforcement learning (RL) agent and demonstrate a swing-up maneuver with a suspended aerial manipulation platform. These platforms are targeted towards a wide range of applications on construction sites involving cranes, with swing-up maneuvers allowing it to perch at a given location, inaccessible with purely the thrust force of the platform. Our proposed approach is based on a hierarchical control framework, which allows different tasks to be executed according to their assigned priorities. An RL agent is then subsequently utilized to adjust the reference set-point of the lower-priority tasks to perform the swing-up maneuver, which is confined in the nullspace of the higher-priority tasks, such as maintaining a specific orientation and position of the end-effector. Our approach is validated using extensive numerical simulation studies.





## Language Agents for Hypothesis-driven Clinical Decision Making with Reinforcement Learning
- **Url**: http://arxiv.org/abs/2506.13474v1
- **Authors**: ['David Bani-Harouni', 'Chantal Pellegrini', 'Ege Ã–zsoy', 'Matthias Keicher', 'Nassir Navab']
- **Abstrat**: Clinical decision-making is a dynamic, interactive, and cyclic process where doctors have to repeatedly decide on which clinical action to perform and consider newly uncovered information for diagnosis and treatment. Large Language Models (LLMs) have the potential to support clinicians in this process, however, most applications of LLMs in clinical decision support suffer from one of two limitations: Either they assume the unrealistic scenario of immediate availability of all patient information and do not model the interactive and iterative investigation process, or they restrict themselves to the limited "out-of-the-box" capabilities of large pre-trained models without performing task-specific training. In contrast to this, we propose to model clinical decision-making for diagnosis with a hypothesis-driven uncertainty-aware language agent, LA-CDM, that converges towards a diagnosis via repeatedly requesting and interpreting relevant tests. Using a hybrid training paradigm combining supervised and reinforcement learning, we train LA-CDM with three objectives targeting critical aspects of clinical decision-making: accurate hypothesis generation, hypothesis uncertainty estimation, and efficient decision-making. We evaluate our methodology on MIMIC-CDM, a real-world dataset covering four abdominal diseases containing various clinical tests and show the benefit of explicitly training clinical decision-making for increasing diagnostic performance and efficiency.





# TD3
# Prioritized Experience Replay
# path planning
## Agentic 3D Scene Generation with Spatially Contextualized VLMs
- **Url**: http://arxiv.org/abs/2505.20129v2
- **Authors**: ['Xinhang Liu', 'Yu-Wing Tai', 'Chi-Keung Tang']
- **Abstrat**: Despite recent advances in multimodal content generation enabled by vision-language models (VLMs), their ability to reason about and generate structured 3D scenes remains largely underexplored. This limitation constrains their utility in spatially grounded tasks such as embodied AI, immersive simulations, and interactive 3D applications. We introduce a new paradigm that enables VLMs to generate, understand, and edit complex 3D environments by injecting a continually evolving spatial context. Constructed from multimodal input, this context consists of three components: a scene portrait that provides a high-level semantic blueprint, a semantically labeled point cloud capturing object-level geometry, and a scene hypergraph that encodes rich spatial relationships, including unary, binary, and higher-order constraints. Together, these components provide the VLM with a structured, geometry-aware working memory that integrates its inherent multimodal reasoning capabilities with structured 3D understanding for effective spatial reasoning. Building on this foundation, we develop an agentic 3D scene generation pipeline in which the VLM iteratively reads from and updates the spatial context. The pipeline features high-quality asset generation with geometric restoration, environment setup with automatic verification, and ergonomic adjustment guided by the scene hypergraph. Experiments show that our framework can handle diverse and challenging inputs, achieving a level of generalization not observed in prior work. Further results demonstrate that injecting spatial context enables VLMs to perform downstream tasks such as interactive scene editing and path planning, suggesting strong potential for spatially intelligent systems in computer graphics, 3D vision, and embodied applications. Project page: https://spatctxvlm.github.io/project_page/.




