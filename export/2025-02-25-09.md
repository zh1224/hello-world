# reinforcement learning
## Event-Based Limit Order Book Simulation under a Neural Hawkes Process: Application in Market-Making
- **Url**: http://arxiv.org/abs/2502.17417v1
- **Authors**: ['Luca Lalor', 'Anatoliy Swishchuk']
- **Abstrat**: In this paper, we propose an event-driven Limit Order Book (LOB) model that captures twelve of the most observed LOB events in exchange-based financial markets. To model these events, we propose using the state-of-the-art Neural Hawkes process, a more robust alternative to traditional Hawkes process models. More specifically, this model captures the dynamic relationships between different event types, particularly their long- and short-term interactions, using a Long Short-Term Memory neural network. Using this framework, we construct a midprice process that captures the event-driven behavior of the LOB by simulating high-frequency dynamics like how they appear in real financial markets. The empirical results show that our model captures many of the broader characteristics of the price fluctuations, particularly in terms of their overall volatility. We apply this LOB simulation model within a Deep Reinforcement Learning Market-Making framework, where the trading agent can now complete trade order fills in a manner that closely resembles real-market trade execution. Here, we also compare the results of the simulated model with those from real data, highlighting how the overall performance and the distribution of trade order fills closely align with the same analysis on real data.





## Random Latent Exploration for Deep Reinforcement Learning
- **Url**: http://arxiv.org/abs/2407.13755v2
- **Authors**: ['Srinath Mahankali', 'Zhang-Wei Hong', 'Ayush Sekhari', 'Alexander Rakhlin', 'Pulkit Agrawal']
- **Abstrat**: We introduce Random Latent Exploration (RLE), a simple yet effective exploration strategy in reinforcement learning (RL). On average, RLE outperforms noise-based methods, which perturb the agent's actions, and bonus-based exploration, which rewards the agent for attempting novel behaviors. The core idea of RLE is to encourage the agent to explore different parts of the environment by pursuing randomly sampled goals in a latent space. RLE is as simple as noise-based methods, as it avoids complex bonus calculations but retains the deep exploration benefits of bonus-based methods. Our experiments show that RLE improves performance on average in both discrete (e.g., Atari) and continuous control tasks (e.g., Isaac Gym), enhancing exploration while remaining a simple and general plug-in for existing RL algorithms.





## Learning to Reason at the Frontier of Learnability
- **Url**: http://arxiv.org/abs/2502.12272v3
- **Authors**: ['Thomas Foster', 'Jakob Foerster']
- **Abstrat**: Reinforcement learning is now widely adopted as the final stage of large language model training, especially for reasoning-style tasks such as maths problems. Typically, models attempt each question many times during a single training step and attempt to learn from their successes and failures. However, we demonstrate that throughout training with two popular algorithms (PPO and VinePPO) on two widely used datasets, many questions are either solved by all attempts - meaning they are already learned - or by none - providing no meaningful training signal. To address this, we adapt a method from the reinforcement learning literature - sampling for learnability - and apply it to the reinforcement learning stage of LLM training. Our curriculum prioritises questions with high variance of success, i.e. those where the agent sometimes succeeds, but not always. Our findings demonstrate that this curriculum consistently boosts training performance across multiple algorithms and datasets, paving the way for more efficient and effective reinforcement learning with LLMs.





## Big-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models
- **Url**: http://arxiv.org/abs/2502.17387v1
- **Authors**: ['Alon Albalak', 'Duy Phung', 'Nathan Lile', 'Rafael Rafailov', 'Kanishk Gandhi', 'Louis Castricato', 'Anikait Singh', 'Chase Blagden', 'Violet Xiang', 'Dakota Mahan', 'Nick Haber']
- **Abstrat**: Increasing interest in reasoning models has led math to become a prominent testing ground for algorithmic and methodological improvements. However, existing open math datasets either contain a small collection of high-quality, human-written problems or a large corpus of machine-generated problems of uncertain quality, forcing researchers to choose between quality and quantity. In this work, we present Big-Math, a dataset of over 250,000 high-quality math questions with verifiable answers, purposefully made for reinforcement learning (RL). To create Big-Math, we rigorously filter, clean, and curate openly available datasets, extracting questions that satisfy our three desiderata: (1) problems with uniquely verifiable solutions, (2) problems that are open-ended, (3) and problems with a closed-form solution. To ensure the quality of Big-Math, we manually verify each step in our filtering process. Based on the findings from our filtering process, we introduce 47,000 new questions with verified answers, Big-Math-Reformulated: closed-ended questions (i.e. multiple choice questions) that have been reformulated as open-ended questions through a systematic reformulation algorithm. Compared to the most commonly used existing open-source datasets for math reasoning, GSM8k and MATH, Big-Math is an order of magnitude larger, while our rigorous filtering ensures that we maintain the questions most suitable for RL. We also provide a rigorous analysis of the dataset, finding that Big-Math contains a high degree of diversity across problem domains, and incorporates a wide range of problem difficulties, enabling a wide range of downstream uses for models of varying capabilities and training requirements. By bridging the gap between data quality and quantity, Big-Math establish a robust foundation for advancing reasoning in LLMs.





## Distributed Coordination for Heterogeneous Non-Terrestrial Networks
- **Url**: http://arxiv.org/abs/2502.17366v1
- **Authors**: ['Jikang Deng', 'Hui Zhou', 'Mohamed-Slim Alouini']
- **Abstrat**: To guarantee global coverage and ubiquitous connectivity, the Non-terrestrial Network (NTN) technology has been regarded as a key enabling technology in the Six Generation (6G) network, which consists of the unmanned aerial vehicle (UAV), high-altitude platform (HAP), and satellite. It is noted that the unique characteristics of various NTN platforms directly impact the design and implementation of NTNs, which results in highly dynamic and heterogeneous networks. Even within the same tier, such as the space tier, the NTNs are developed based on different platforms including Low Earth Orbit (LEO), Medium Earth Orbit (MEO), and Geostationary Earth Orbit (GEO). Therefore, distributed coordination among heterogeneous NTNs remains an important challenge. Although distributed learning framework finds a wide range of applications by leveraging rich distributed data and computation resources. The explicit and systematic analysis of the individual layers' challenges, and corresponding distributed coordination solutions in heterogeneous NTNs has not been proposed yet. In this article, we first summarize the unique characteristics of each NTN platform, and analyze the corresponding impact on the design and implementation of the NTN. We then identify the communication challenges of heterogeneous NTNs in individual layers, where the potential coordinated solutions are identified. We further illustrate the multi-agent deep reinforcement learning (MADRL) algorithms tailored for coordinated solutions in heterogeneous NTNs. Last but not least, we present a case study of the user scheduling optimization problem in heterogeneous UAVs-based cellular networks, where the multi-agent deep deterministic policy gradient (MADDPG) technique is developed to validate the effectiveness of distributed coordination in heterogeneous NTNs.





## Offline Safe Reinforcement Learning Using Trajectory Classification
- **Url**: http://arxiv.org/abs/2412.15429v2
- **Authors**: ['Ze Gong', 'Akshat Kumar', 'Pradeep Varakantham']
- **Abstrat**: Offline safe reinforcement learning (RL) has emerged as a promising approach for learning safe behaviors without engaging in risky online interactions with the environment. Most existing methods in offline safe RL rely on cost constraints at each time step (derived from global cost constraints) and this can result in either overly conservative policies or violation of safety constraints. In this paper, we propose to learn a policy that generates desirable trajectories and avoids undesirable trajectories. To be specific, we first partition the pre-collected dataset of state-action trajectories into desirable and undesirable subsets. Intuitively, the desirable set contains high reward and safe trajectories, and undesirable set contains unsafe trajectories and low-reward safe trajectories. Second, we learn a policy that generates desirable trajectories and avoids undesirable trajectories, where (un)desirability scores are provided by a classifier learnt from the dataset of desirable and undesirable trajectories. This approach bypasses the computational complexity and stability issues of a min-max objective that is employed in existing methods. Theoretically, we also show our approach's strong connections to existing learning paradigms involving human feedback. Finally, we extensively evaluate our method using the DSRL benchmark for offline safe RL. Empirically, our method outperforms competitive baselines, achieving higher rewards and better constraint satisfaction across a wide variety of benchmark tasks.





## Comparative Analysis of Multi-Agent Reinforcement Learning Policies for Crop Planning Decision Support
- **Url**: http://arxiv.org/abs/2412.02057v2
- **Authors**: ['Anubha Mahajan', 'Shreya Hegde', 'Ethan Shay', 'Daniel Wu', 'Aviva Prins']
- **Abstrat**: In India, the majority of farmers are classified as small or marginal, making their livelihoods particularly vulnerable to economic losses due to market saturation and climate risks. Effective crop planning can significantly impact their expected income, yet existing decision support systems (DSS) often provide generic recommendations that fail to account for real-time market dynamics and the interactions among multiple farmers. In this paper, we evaluate the viability of three multi-agent reinforcement learning (MARL) approaches for optimizing total farmer income and promoting fairness in crop planning: Independent Q-Learning (IQL), where each farmer acts independently without coordination, Agent-by-Agent (ABA), which sequentially optimizes each farmer's policy in relation to the others, and the Multi-agent Rollout Policy, which jointly optimizes all farmers' actions for global reward maximization. Our results demonstrate that while IQL offers computational efficiency with linear runtime, it struggles with coordination among agents, leading to lower total rewards and an unequal distribution of income. Conversely, the Multi-agent Rollout policy achieves the highest total rewards and promotes equitable income distribution among farmers but requires significantly more computational resources, making it less practical for large numbers of agents. ABA strikes a balance between runtime efficiency and reward optimization, offering reasonable total rewards with acceptable fairness and scalability. These findings highlight the importance of selecting appropriate MARL approaches in DSS to provide personalized and equitable crop planning recommendations, advancing the development of more adaptive and farmer-centric agricultural decision-making systems.





## TDMPBC: Self-Imitative Reinforcement Learning for Humanoid Robot Control
- **Url**: http://arxiv.org/abs/2502.17322v1
- **Authors**: ['Zifeng Zhuang', 'Diyuan Shi', 'Runze Suo', 'Xiao He', 'Hongyin Zhang', 'Ting Wang', 'Shangke Lyu', 'Donglin Wang']
- **Abstrat**: Complex high-dimensional spaces with high Degree-of-Freedom and complicated action spaces, such as humanoid robots equipped with dexterous hands, pose significant challenges for reinforcement learning (RL) algorithms, which need to wisely balance exploration and exploitation under limited sample budgets. In general, feasible regions for accomplishing tasks within complex high-dimensional spaces are exceedingly narrow. For instance, in the context of humanoid robot motion control, the vast majority of space corresponds to falling, while only a minuscule fraction corresponds to standing upright, which is conducive to the completion of downstream tasks. Once the robot explores into a potentially task-relevant region, it should place greater emphasis on the data within that region. Building on this insight, we propose the $\textbf{S}$elf-$\textbf{I}$mitative $\textbf{R}$einforcement $\textbf{L}$earning ($\textbf{SIRL}$) framework, where the RL algorithm also imitates potentially task-relevant trajectories. Specifically, trajectory return is utilized to determine its relevance to the task and an additional behavior cloning is adopted whose weight is dynamically adjusted based on the trajectory return. As a result, our proposed algorithm achieves 120% performance improvement on the challenging HumanoidBench with 5% extra computation overhead. With further visualization, we find the significant performance gain does lead to meaningful behavior improvement that several tasks are solved successfully.





## Survey on Strategic Mining in Blockchain: A Reinforcement Learning Approach
- **Url**: http://arxiv.org/abs/2502.17307v1
- **Authors**: ['Jichen Li', 'Lijia Xie', 'Hanting Huang', 'Bo Zhou', 'Binfeng Song', 'Wanying Zeng', 'Xiaotie Deng', 'Xiao Zhang']
- **Abstrat**: Strategic mining attacks, such as selfish mining, exploit blockchain consensus protocols by deviating from honest behavior to maximize rewards. Markov Decision Process (MDP) analysis faces scalability challenges in modern digital economics, including blockchain. To address these limitations, reinforcement learning (RL) provides a scalable alternative, enabling adaptive strategy optimization in complex dynamic environments.   In this survey, we examine RL's role in strategic mining analysis, comparing it to MDP-based approaches. We begin by reviewing foundational MDP models and their limitations, before exploring RL frameworks that can learn near-optimal strategies across various protocols. Building on this analysis, we compare RL techniques and their effectiveness in deriving security thresholds, such as the minimum attacker power required for profitable attacks. Expanding the discussion further, we classify consensus protocols and propose open challenges, such as multi-agent dynamics and real-world validation.   This survey highlights the potential of reinforcement learning (RL) to address the challenges of selfish mining, including protocol design, threat detection, and security analysis, while offering a strategic roadmap for researchers in decentralized systems and AI-driven analytics.





## Linear $Q$-Learning Does Not Diverge in $L^2$: Convergence Rates to a Bounded Set
- **Url**: http://arxiv.org/abs/2501.19254v3
- **Authors**: ['Xinyu Liu', 'Zixuan Xie', 'Shangtong Zhang']
- **Abstrat**: $Q$-learning is one of the most fundamental reinforcement learning algorithms. It is widely believed that $Q$-learning with linear function approximation (i.e., linear $Q$-learning) suffers from possible divergence until the recent work Meyn (2024) which establishes the ultimate almost sure boundedness of the iterates of linear $Q$-learning. Building on this success, this paper further establishes the first $L^2$ convergence rate of linear $Q$-learning iterates (to a bounded set). Similar to Meyn (2024), we do not make any modification to the original linear $Q$-learning algorithm, do not make any Bellman completeness assumption, and do not make any near-optimality assumption on the behavior policy. All we need is an $\epsilon$-softmax behavior policy with an adaptive temperature. The key to our analysis is the general result of stochastic approximations under Markovian noise with fast-changing transition functions. As a side product, we also use this general result to establish the $L^2$ convergence rate of tabular $Q$-learning with an $\epsilon$-softmax behavior policy, for which we rely on a novel pseudo-contraction property of the weighted Bellman optimality operator.





## Text2World: Benchmarking Large Language Models for Symbolic World Model Generation
- **Url**: http://arxiv.org/abs/2502.13092v2
- **Authors**: ['Mengkang Hu', 'Tianxing Chen', 'Yude Zou', 'Yuheng Lei', 'Qiguang Chen', 'Ming Li', 'Yao Mu', 'Hongyuan Zhang', 'Wenqi Shao', 'Ping Luo']
- **Abstrat**: Recently, there has been growing interest in leveraging large language models (LLMs) to generate symbolic world models from textual descriptions. Although LLMs have been extensively explored in the context of world modeling, prior studies encountered several challenges, including evaluation randomness, dependence on indirect metrics, and a limited domain scope. To address these limitations, we introduce a novel benchmark, Text2World, based on planning domain definition language (PDDL), featuring hundreds of diverse domains and employing multi-criteria, execution-based metrics for a more robust evaluation. We benchmark current LLMs using Text2World and find that reasoning models trained with large-scale reinforcement learning outperform others. However, even the best-performing model still demonstrates limited capabilities in world modeling. Building on these insights, we examine several promising strategies to enhance the world modeling capabilities of LLMs, including test-time scaling, agent training, and more. We hope that Text2World can serve as a crucial resource, laying the groundwork for future research in leveraging LLMs as world models. The project page is available at https://text-to-world.github.io/.





## TAG: A Decentralized Framework for Multi-Agent Hierarchical Reinforcement Learning
- **Url**: http://arxiv.org/abs/2502.15425v2
- **Authors**: ['Giuseppe Paolo', 'Abdelhakim Benechehab', 'Hamza Cherkaoui', 'Albert Thomas', 'Balázs Kégl']
- **Abstrat**: Hierarchical organization is fundamental to biological systems and human societies, yet artificial intelligence systems often rely on monolithic architectures that limit adaptability and scalability. Current hierarchical reinforcement learning (HRL) approaches typically restrict hierarchies to two levels or require centralized training, which limits their practical applicability. We introduce TAME Agent Framework (TAG), a framework for constructing fully decentralized hierarchical multi-agent systems.TAG enables hierarchies of arbitrary depth through a novel LevelEnv concept, which abstracts each hierarchy level as the environment for the agents above it. This approach standardizes information flow between levels while preserving loose coupling, allowing for seamless integration of diverse agent types. We demonstrate the effectiveness of TAG by implementing hierarchical architectures that combine different RL agents across multiple levels, achieving improved performance over classical multi-agent RL baselines on standard benchmarks. Our results show that decentralized hierarchical organization enhances both learning speed and final performance, positioning TAG as a promising direction for scalable multi-agent systems.





## A Reinforcement Learning Approach to Non-prehensile Manipulation through Sliding
- **Url**: http://arxiv.org/abs/2502.17221v1
- **Authors**: ['Hamidreza Raei', 'Elena De Momi', 'Arash Ajoudani']
- **Abstrat**: Although robotic applications increasingly demand versatile and dynamic object handling, most existing techniques are predominantly focused on grasp-based manipulation, limiting their applicability in non-prehensile tasks. To address this need, this study introduces a Deep Deterministic Policy Gradient (DDPG) reinforcement learning framework for efficient non-prehensile manipulation, specifically for sliding an object on a surface. The algorithm generates a linear trajectory by precisely controlling the acceleration of a robotic arm rigidly coupled to the horizontal surface, enabling the relative manipulation of an object as it slides on top of the surface. Furthermore, two distinct algorithms have been developed to estimate the frictional forces dynamically during the sliding process. These algorithms provide online friction estimates after each action, which are fed back into the actor model as critical feedback after each action. This feedback mechanism enhances the policy's adaptability and robustness, ensuring more precise control of the platform's acceleration in response to varying surface condition. The proposed algorithm is validated through simulations and real-world experiments. Results demonstrate that the proposed framework effectively generalizes sliding manipulation across varying distances and, more importantly, adapts to different surfaces with diverse frictional properties. Notably, the trained model exhibits zero-shot sim-to-real transfer capabilities.





## Humanoid Whole-Body Locomotion on Narrow Terrain via Dynamic Balance and Reinforcement Learning
- **Url**: http://arxiv.org/abs/2502.17219v1
- **Authors**: ['Weiji Xie', 'Chenjia Bai', 'Jiyuan Shi', 'Junkai Yang', 'Yunfei Ge', 'Weinan Zhang', 'Xuelong Li']
- **Abstrat**: Humans possess delicate dynamic balance mechanisms that enable them to maintain stability across diverse terrains and under extreme conditions. However, despite significant advances recently, existing locomotion algorithms for humanoid robots are still struggle to traverse extreme environments, especially in cases that lack external perception (e.g., vision or LiDAR). This is because current methods often rely on gait-based or perception-condition rewards, lacking effective mechanisms to handle unobservable obstacles and sudden balance loss. To address this challenge, we propose a novel whole-body locomotion algorithm based on dynamic balance and Reinforcement Learning (RL) that enables humanoid robots to traverse extreme terrains, particularly narrow pathways and unexpected obstacles, using only proprioception. Specifically, we introduce a dynamic balance mechanism by leveraging an extended measure of Zero-Moment Point (ZMP)-driven rewards and task-driven rewards in a whole-body actor-critic framework, aiming to achieve coordinated actions of the upper and lower limbs for robust locomotion. Experiments conducted on a full-sized Unitree H1-2 robot verify the ability of our method to maintain balance on extremely narrow terrains and under external disturbances, demonstrating its effectiveness in enhancing the robot's adaptability to complex environments. The videos are given at https://whole-body-loco.github.io.





## Teleology-Driven Affective Computing: A Causal Framework for Sustained Well-Being
- **Url**: http://arxiv.org/abs/2502.17172v1
- **Authors**: ['Bin Yin', 'Chong-Yi Liu', 'Liya Fu', 'Jinkun Zhang']
- **Abstrat**: Affective computing has made significant strides in emotion recognition and generation, yet current approaches mainly focus on short-term pattern recognition and lack a comprehensive framework to guide affective agents toward long-term human well-being. To address this, we propose a teleology-driven affective computing framework that unifies major emotion theories (basic emotion, appraisal, and constructivist approaches) under the premise that affect is an adaptive, goal-directed process that facilitates survival and development. Our framework emphasizes aligning agent responses with both personal/individual and group/collective well-being over extended timescales. We advocate for creating a "dataverse" of personal affective events, capturing the interplay between beliefs, goals, actions, and outcomes through real-world experience sampling and immersive virtual reality. By leveraging causal modeling, this "dataverse" enables AI systems to infer individuals' unique affective concerns and provide tailored interventions for sustained well-being. Additionally, we introduce a meta-reinforcement learning paradigm to train agents in simulated environments, allowing them to adapt to evolving affective concerns and balance hierarchical goals - from immediate emotional needs to long-term self-actualization. This framework shifts the focus from statistical correlations to causal reasoning, enhancing agents' ability to predict and respond proactively to emotional challenges, and offers a foundation for developing personalized, ethically aligned affective systems that promote meaningful human-AI interactions and societal well-being.





## A Novel Multiple Access Scheme for Heterogeneous Wireless Communications using Symmetry-aware Continual Deep Reinforcement Learning
- **Url**: http://arxiv.org/abs/2502.17167v1
- **Authors**: ['Hamidreza Mazandarani', 'Masoud Shokrnezhad', 'Tarik Taleb']
- **Abstrat**: The Metaverse holds the potential to revolutionize digital interactions through the establishment of a highly dynamic and immersive virtual realm over wireless communications systems, offering services such as massive twinning and telepresence. This landscape presents novel challenges, particularly efficient management of multiple access to the frequency spectrum, for which numerous adaptive Deep Reinforcement Learning (DRL) approaches have been explored. However, challenges persist in adapting agents to heterogeneous and non-stationary wireless environments. In this paper, we present a novel approach that leverages Continual Learning (CL) to enhance intelligent Medium Access Control (MAC) protocols, featuring an intelligent agent coexisting with legacy User Equipments (UEs) with varying numbers, protocols, and transmission profiles unknown to the agent for the sake of backward compatibility and privacy. We introduce an adaptive Double and Dueling Deep Q-Learning (D3QL)-based MAC protocol, enriched by a symmetry-aware CL mechanism, which maximizes intelligent agent throughput while ensuring fairness. Mathematical analysis validates the efficiency of our proposed scheme, showcasing superiority over conventional DRL-based techniques in terms of throughput, collision rate, and fairness, coupled with real-time responsiveness in highly dynamic scenarios.





## A Physics-Informed Machine Learning Framework for Safe and Optimal Control of Autonomous Systems
- **Url**: http://arxiv.org/abs/2502.11057v2
- **Authors**: ['Manan Tayal', 'Aditya Singh', 'Shishir Kolathaya', 'Somil Bansal']
- **Abstrat**: As autonomous systems become more ubiquitous in daily life, ensuring high performance with guaranteed safety is crucial. However, safety and performance could be competing objectives, which makes their co-optimization difficult. Learning-based methods, such as Constrained Reinforcement Learning (CRL), achieve strong performance but lack formal safety guarantees due to safety being enforced as soft constraints, limiting their use in safety-critical settings. Conversely, formal methods such as Hamilton-Jacobi (HJ) Reachability Analysis and Control Barrier Functions (CBFs) provide rigorous safety assurances but often neglect performance, resulting in overly conservative controllers. To bridge this gap, we formulate the co-optimization of safety and performance as a state-constrained optimal control problem, where performance objectives are encoded via a cost function and safety requirements are imposed as state constraints. We demonstrate that the resultant value function satisfies a Hamilton-Jacobi-Bellman (HJB) equation, which we approximate efficiently using a novel physics-informed machine learning framework. In addition, we introduce a conformal prediction-based verification strategy to quantify the learning errors, recovering a high-confidence safety value function, along with a probabilistic error bound on performance degradation. Through several case studies, we demonstrate the efficacy of the proposed framework in enabling scalable learning of safe and performant controllers for complex, high-dimensional autonomous systems.





# TD3
# Prioritized Experience Replay
# path planning
## Fast Shortest Path Polyline Smoothing With $G^1$ Continuity and Bounded Curvature
- **Url**: http://arxiv.org/abs/2409.09816v2
- **Authors**: ['Patrick Pastorelli', 'Simone Dagnino', 'Enrico Saccon', 'Marco Frego', 'Luigi Palopoli']
- **Abstrat**: In this work, we propose a novel and efficient method for smoothing polylines in motion planning tasks. The algorithm applies to motion planning of vehicles with bounded curvature. In the paper, we show that the generated path: 1) has minimal length, 2) is $G^1$ continuous, and 3) is collision-free by construction, if the hypotheses are respected. We compare our solution with the state-of.the-art and show its convenience both in terms of computation time and of length of the compute path.





## Optimizing Urban Mobility Through Complex Network Analysis and Big Data from Smart Cards
- **Url**: http://arxiv.org/abs/2502.17054v1
- **Authors**: ['Li Sun', 'Negin Ashrafi', 'Maryam Pishgar']
- **Abstrat**: This study investigates the network characteristics of high-frequency (HF) and low-frequency (LF) travelers in urban public transport systems by analyzing 20 million smart card records from Beijing's transit network. A novel methodology integrates advanced data preprocessing, clustering techniques, and complex network analysis to differentiate HF and LF passenger behaviors and their impacts on network structure, robustness, and efficiency. The primary challenge is accurately segmenting and modeling the behaviors of diverse passenger groups within a large-scale, noisy dataset while maintaining computational efficiency and scalability. HF networks, representing the top 25% of travelers by usage frequency, exhibit high connectivity with an average clustering coefficient of 0.72 and greater node degree centrality. However, they have lower robustness, with efficiency declining by 35% under targeted disruptions and longer average path lengths of 6.2 during peak hours. In contrast, LF networks, which include 75% of travelers, are more dispersed yet resilient, with efficiency declining by only 10% under similar disruptions and stronger intracommunity connectivity. Temporal analysis reveals that HF passengers significantly contribute to peak-hour congestion, with 57.4% of HF trips occurring between 6:00 and 10:00 AM, while LF passengers show a broader temporal distribution, helping to mitigate congestion hotspots. Understanding these travel patterns is crucial for optimizing public transit systems. The findings suggest targeted strategies such as enhancing robustness in HF networks by diversifying key routes and improving accessibility in LF-dominated areas. This research provides a scalable framework for analyzing smart card data and offers actionable insights for optimizing transit networks, improving congestion management, and advancing sustainable urban mobility planning.




