# reinforcement learning
## LongReward: Improving Long-context Large Language Models with AI Feedback
- **Url**: http://arxiv.org/abs/2410.21252v1
- **Authors**: ['Jiajie Zhang', 'Zhongni Hou', 'Xin Lv', 'Shulin Cao', 'Zhenyu Hou', 'Yilin Niu', 'Lei Hou', 'Yuxiao Dong', 'Ling Feng', 'Juanzi Li']
- **Abstrat**: Though significant advancements have been achieved in developing long-context large language models (LLMs), the compromised quality of LLM-synthesized data for supervised fine-tuning (SFT) often affects the long-context performance of SFT models and leads to inherent limitations. In principle, reinforcement learning (RL) with appropriate reward signals can further enhance models' capacities. However, how to obtain reliable rewards in long-context scenarios remains unexplored. To this end, we propose LongReward, a novel method that utilizes an off-the-shelf LLM to provide rewards for long-context model responses from four human-valued dimensions: helpfulness, logicality, faithfulness, and completeness, each with a carefully designed assessment pipeline. By combining LongReward and offline RL algorithm DPO, we are able to effectively improve long-context SFT models. Our experiments indicate that LongReward not only significantly improves models' long-context performance but also enhances their ability to follow short instructions. We also find that long-context DPO with LongReward and conventional short-context DPO can be used together without hurting either one's performance.





## Quantum Reinforcement Learning-Based Two-Stage Unit Commitment Framework for Enhanced Power Systems Robustness
- **Url**: http://arxiv.org/abs/2410.21240v1
- **Authors**: ['Xiang Wei', 'Ziqing Zhu']
- **Abstrat**: The volatility of renewable energy sources and fluctuations in real-time electricity demand present significant challenges to traditional unit commitment (UC) methods, often causing system constraint violations. Conventional optimization algorithms face substantial difficulties in responding quickly to these variations, frequently requiring the relaxation of constraints or producing infeasible solutions. To address these challenges, a robust two-stage UC framework based on quantum reinforcement learning (QRL) is proposed in this work, which improves both decision-making speed and solution feasibility. In the first stage, the day-ahead scheduling of thermal generators is optimized. In the second stage, real-time adjustments are made to account for changes in renewable generation and load, with microgrids integrated to reduce the impact of uncertainties on the power system. Both stages are formulated as Markov decision processes (MDPs), and QRL is used to efficiently solve the problem. QRL provides key advantages, including more effective navigation of the high-dimensional solution space and faster convergence compared to classical methods, thus enhancing the robustness and computational efficiency of UC operations. The proposed QRL-based two-stage UC framework is validated using the IEEE RTS 24-bus system. Results demonstrate the effectiveness of the approach, showing improved solution feasibility and computational speed compared to conventional UC methods.





## Learning to Walk from Three Minutes of Real-World Data with Semi-structured Dynamics Models
- **Url**: http://arxiv.org/abs/2410.09163v2
- **Authors**: ['Jacob Levy', 'Tyler Westenbroek', 'David Fridovich-Keil']
- **Abstrat**: Traditionally, model-based reinforcement learning (MBRL) methods exploit neural networks as flexible function approximators to represent $\textit{a priori}$ unknown environment dynamics. However, training data are typically scarce in practice, and these black-box models often fail to generalize. Modeling architectures that leverage known physics can substantially reduce the complexity of system-identification, but break down in the face of complex phenomena such as contact. We introduce a novel framework for learning semi-structured dynamics models for contact-rich systems which seamlessly integrates structured first principles modeling techniques with black-box auto-regressive models. Specifically, we develop an ensemble of probabilistic models to estimate external forces, conditioned on historical observations and actions, and integrate these predictions using known Lagrangian dynamics. With this semi-structured approach, we can make accurate long-horizon predictions with substantially less data than prior methods. We leverage this capability and propose Semi-Structured Reinforcement Learning ($\texttt{SSRL}$) a simple model-based learning framework which pushes the sample complexity boundary for real-world learning. We validate our approach on a real-world Unitree Go1 quadruped robot, learning dynamic gaits -- from scratch -- on both hard and soft surfaces with just a few minutes of real-world data. Video and code are available at: https://sites.google.com/utexas.edu/ssrl





## Artificial Generational Intelligence: Cultural Accumulation in Reinforcement Learning
- **Url**: http://arxiv.org/abs/2406.00392v2
- **Authors**: ['Jonathan Cook', 'Chris Lu', 'Edward Hughes', 'Joel Z. Leibo', 'Jakob Foerster']
- **Abstrat**: Cultural accumulation drives the open-ended and diverse progress in capabilities spanning human history. It builds an expanding body of knowledge and skills by combining individual exploration with inter-generational information transmission. Despite its widespread success among humans, the capacity for artificial learning agents to accumulate culture remains under-explored. In particular, approaches to reinforcement learning typically strive for improvements over only a single lifetime. Generational algorithms that do exist fail to capture the open-ended, emergent nature of cultural accumulation, which allows individuals to trade-off innovation and imitation. Building on the previously demonstrated ability for reinforcement learning agents to perform social learning, we find that training setups which balance this with independent learning give rise to cultural accumulation. These accumulating agents outperform those trained for a single lifetime with the same cumulative experience. We explore this accumulation by constructing two models under two distinct notions of a generation: episodic generations, in which accumulation occurs via in-context learning and train-time generations, in which accumulation occurs via in-weights learning. In-context and in-weights cultural accumulation can be interpreted as analogous to knowledge and skill accumulation, respectively. To the best of our knowledge, this work is the first to present general models that achieve emergent cultural accumulation in reinforcement learning, opening up new avenues towards more open-ended learning systems, as well as presenting new opportunities for modelling human culture.





## Evaluation of two Cooperative Maneuver Planning Approaches at a Real-World T-Junction in Mixed Traffic
- **Url**: http://arxiv.org/abs/2403.16478v3
- **Authors**: ['Marvin Klimke', 'Max Bastian Mertens', 'Benjamin VÃ¶lz', 'Michael Buchholz']
- **Abstrat**: Connected automated driving promises a significant improvement of traffic efficiency and safety on highways and in urban areas. Cooperative maneuver planning at unsignalized intersections may facilitate active guidance of connected automated vehicles. Previous such works mostly employ simple rule-based or optimization-based approaches, often only for fully automated vehicles and only in simulated environments. In this article, we extend and evaluate our previously introduced approaches, which -- in contrast -- are capable of handling mixed traffic, i.e., automated vehicles and regular vehicles driven by humans sharing the road. They are based on a multi-scenario prediction and on graph-based reinforcement learning, respectively. For the first time in literature, we thoroughly evaluate cooperative planners in a high-fidelity simulation with fully automated traffic and mixed traffic using state-of-the-art human driver models and real-world automation software. In addition, we are the first to present respective real-world evaluations with three prototype automated vehicles in public traffic, which confirm the simulative results. Our quantitative evaluations show that cooperative maneuver planning achieves a significant reduction of crossing times and the number of stops even in a realistic environment with only few automated vehicles.





## A Probability--Quality Trade-off in Aligned Language Models and its Relation to Sampling Adaptors
- **Url**: http://arxiv.org/abs/2406.10203v4
- **Authors**: ['Naaman Tan', 'Josef Valvoda', 'Tianyu Liu', 'Anej Svete', 'Yanxia Qin', 'Kan Min-Yen', 'Ryan Cotterell']
- **Abstrat**: The relationship between the quality of a string, as judged by a human reader, and its probability, $p(\boldsymbol{y})$ under a language model undergirds the development of better language models. For example, many popular algorithms for sampling from a language model have been conceived with the goal of manipulating $p(\boldsymbol{y})$ to place higher probability on strings that humans deem of high quality. In this article, we examine the probability--quality relationship in language models explicitly aligned to human preferences, e.g., through reinforcement learning through human feedback. We show that, when sampling corpora from an aligned language model, there exists a trade-off between the strings' average reward and average log-likelihood under the prior language model, i.e., the same model before alignment with human preferences. We provide a formal treatment of this phenomenon and demonstrate how a choice of sampling adaptor allows for a selection of how much likelihood we exchange for the reward.





## Offline Reinforcement Learning With Combinatorial Action Spaces
- **Url**: http://arxiv.org/abs/2410.21151v1
- **Authors**: ['Matthew Landers', 'Taylor W. Killian', 'Hugo Barnes', 'Thomas Hartvigsen', 'Afsaneh Doryab']
- **Abstrat**: Reinforcement learning problems often involve large action spaces arising from the simultaneous execution of multiple sub-actions, resulting in combinatorial action spaces. Learning in combinatorial action spaces is difficult due to the exponential growth in action space size with the number of sub-actions and the dependencies among these sub-actions. In offline settings, this challenge is compounded by limited and suboptimal data. Current methods for offline learning in combinatorial spaces simplify the problem by assuming sub-action independence. We propose Branch Value Estimation (BVE), which effectively captures sub-action dependencies and scales to large combinatorial spaces by learning to evaluate only a small subset of actions at each timestep. Our experiments show that BVE outperforms state-of-the-art methods across a range of action space sizes.





## Transforming Location Retrieval at Airbnb: A Journey from Heuristics to Reinforcement Learning
- **Url**: http://arxiv.org/abs/2408.13399v2
- **Authors**: ['Dillon Davis', 'Huiji Gao', 'Thomas Legrand', 'Weiwei Guo', 'Malay Haldar', 'Alex Deng', 'Han Zhao', 'Liwei He', 'Sanjeev Katariya']
- **Abstrat**: The Airbnb search system grapples with many unique challenges as it continues to evolve. We oversee a marketplace that is nuanced by geography, diversity of homes, and guests with a variety of preferences. Crafting an efficient search system that can accommodate diverse guest needs, while showcasing relevant homes lies at the heart of Airbnb's success. Airbnb search has many challenges that parallel other recommendation and search systems but it has a unique information retrieval problem, upstream of ranking, called location retrieval. It requires defining a topological map area that is relevant to the searched query for homes listing retrieval. The purpose of this paper is to demonstrate the methodology, challenges, and impact of building a machine learning based location retrieval product from the ground up. Despite the lack of suitable, prevalent machine learning based approaches, we tackle cold start, generalization, differentiation and algorithmic bias. We detail the efficacy of heuristics, statistics, machine learning, and reinforcement learning approaches to solve these challenges, particularly for systems that are often unexplored by current literature.





## Robustness and Generalization in Quantum Reinforcement Learning via Lipschitz Regularization
- **Url**: http://arxiv.org/abs/2410.21117v1
- **Authors**: ['Nico Meyer', 'Julian Berberich', 'Christopher Mutschler', 'Daniel D. Scherer']
- **Abstrat**: Quantum machine learning leverages quantum computing to enhance accuracy and reduce model complexity compared to classical approaches, promising significant advancements in various fields. Within this domain, quantum reinforcement learning has garnered attention, often realized using variational quantum circuits to approximate the policy function. This paper addresses the robustness and generalization of quantum reinforcement learning by combining principles from quantum computing and control theory. Leveraging recent results on robust quantum machine learning, we utilize Lipschitz bounds to propose a regularized version of a quantum policy gradient approach, named the RegQPG algorithm. We show that training with RegQPG improves the robustness and generalization of the resulting policies. Furthermore, we introduce an algorithmic variant that incorporates curriculum learning, which minimizes failures during training. Our findings are validated through numerical experiments, demonstrating the practical benefits of our approach.





## Dual-Agent Deep Reinforcement Learning for Dynamic Pricing and Replenishment
- **Url**: http://arxiv.org/abs/2410.21109v1
- **Authors**: ['Yi Zheng', 'Zehao Li', 'Peng Jiang', 'Yijie Peng']
- **Abstrat**: We study the dynamic pricing and replenishment problems under inconsistent decision frequencies. Different from the traditional demand assumption, the discreteness of demand and the parameter within the Poisson distribution as a function of price introduce complexity into analyzing the problem property. We demonstrate the concavity of the single-period profit function with respect to product price and inventory within their respective domains. The demand model is enhanced by integrating a decision tree-based machine learning approach, trained on comprehensive market data. Employing a two-timescale stochastic approximation scheme, we address the discrepancies in decision frequencies between pricing and replenishment, ensuring convergence to local optimum. We further refine our methodology by incorporating deep reinforcement learning (DRL) techniques and propose a fast-slow dual-agent DRL algorithm. In this approach, two agents handle pricing and inventory and are updated on different scales. Numerical results from both single and multiple products scenarios validate the effectiveness of our methods.





## Masked Generative Priors Improve World Models Sequence Modelling Capabilities
- **Url**: http://arxiv.org/abs/2410.07836v3
- **Authors**: ['Cristian Meo', 'Mircea Lica', 'Zarif Ikram', 'Akihiro Nakano', 'Vedant Shah', 'Aniket Rajiv Didolkar', 'Dianbo Liu', 'Anirudh Goyal', 'Justin Dauwels']
- **Abstrat**: Deep Reinforcement Learning (RL) has become the leading approach for creating artificial agents in complex environments. Model-based approaches, which are RL methods with world models that predict environment dynamics, are among the most promising directions for improving data efficiency, forming a critical step toward bridging the gap between research and real-world deployment. In particular, world models enhance sample efficiency by learning in imagination, which involves training a generative sequence model of the environment in a self-supervised manner. Recently, Masked Generative Modelling has emerged as a more efficient and superior inductive bias for modelling and generating token sequences. Building on the Efficient Stochastic Transformer-based World Models (STORM) architecture, we replace the traditional MLP prior with a Masked Generative Prior (e.g., MaskGIT Prior) and introduce GIT-STORM. We evaluate our model on two downstream tasks: reinforcement learning and video prediction. GIT-STORM demonstrates substantial performance gains in RL tasks on the Atari 100k benchmark. Moreover, we apply Transformer-based World Models to continuous action environments for the first time, addressing a significant gap in prior research. To achieve this, we employ a state mixer function that integrates latent state representations with actions, enabling our model to handle continuous control tasks. We validate this approach through qualitative and quantitative analyses on the DeepMind Control Suite, showcasing the effectiveness of Transformer-based World Models in this new domain. Our results highlight the versatility and efficacy of the MaskGIT dynamics prior, paving the way for more accurate world models and effective RL policies.





## Stronger Regret Bounds for Safe Online Reinforcement Learning in the Linear Quadratic Regulator
- **Url**: http://arxiv.org/abs/2410.21081v1
- **Authors**: ['Benjamin Schiffer', 'Lucas Janson']
- **Abstrat**: Many practical applications of online reinforcement learning require the satisfaction of safety constraints while learning about the unknown environment. In this work, we study Linear Quadratic Regulator (LQR) learning with unknown dynamics, but with the additional constraint that the position must stay within a safe region for the entire trajectory with high probability. Unlike in previous works, we allow for both bounded and unbounded noise distributions and study stronger baselines of nonlinear controllers that are better suited for constrained problems than linear controllers. Due to these complications, we focus on 1-dimensional state- and action- spaces, however we also discuss how we expect the high-level takeaways can generalize to higher dimensions. Our primary contribution is the first $\tilde{O}_T(\sqrt{T})$-regret bound for constrained LQR learning, which we show relative to a specific baseline of non-linear controllers. We then prove that, for any non-linear baseline satisfying natural assumptions, $\tilde{O}_T(\sqrt{T})$-regret is possible when the noise distribution has sufficiently large support and $\tilde{O}_T(T^{2/3})$-regret is possible for any subgaussian noise distribution. An overarching theme of our results is that enforcing safety provides "free exploration" that compensates for the added cost of uncertainty in safety constrained control, resulting in the same regret rate as in the unconstrained problem.





## Getting By Goal Misgeneralization With a Little Help From a Mentor
- **Url**: http://arxiv.org/abs/2410.21052v1
- **Authors**: ['Tu Trinh', 'Mohamad H. Danesh', 'Nguyen X. Khanh', 'Benjamin Plaut']
- **Abstrat**: While reinforcement learning (RL) agents often perform well during training, they can struggle with distribution shift in real-world deployments. One particularly severe risk of distribution shift is goal misgeneralization, where the agent learns a proxy goal that coincides with the true goal during training but not during deployment. In this paper, we explore whether allowing an agent to ask for help from a supervisor in unfamiliar situations can mitigate this issue. We focus on agents trained with PPO in the CoinRun environment, a setting known to exhibit goal misgeneralization. We evaluate multiple methods for determining when the agent should request help and find that asking for help consistently improves performance. However, we also find that methods based on the agent's internal state fail to proactively request help, instead waiting until mistakes have already occurred. Further investigation suggests that the agent's internal state does not represent the coin at all, highlighting the importance of learning nuanced representations, the risks of ignoring everything not immediately relevant to reward, and the necessity of developing ask-for-help strategies tailored to the agent's training algorithm.





## Hybrid Quantum-Classical Reinforcement Learning in Latent Observation Spaces
- **Url**: http://arxiv.org/abs/2410.18284v2
- **Authors**: ['DÃ¡niel T. R. Nagy', 'Csaba CzabÃ¡n', 'Bence BakÃ³', 'PÃ©ter HÃ¡ga', 'ZsÃ³fia Kallus', 'ZoltÃ¡n ZimborÃ¡s']
- **Abstrat**: Recent progress in quantum machine learning has sparked interest in using quantum methods to tackle classical control problems via quantum reinforcement learning. However, the classical reinforcement learning environments often scale to high dimensional problem spaces, which represents a challenge for the limited and costly resources available for quantum agent implementations. We propose to solve this dimensionality challenge by a classical autoencoder and a quantum agent together, where a compressed representation of observations is jointly learned in a hybrid training loop. The latent representation of such an autoencoder will serve as a tailored observation space best suited for both the control problem and the QPU architecture, aligning with the agent's requirements. A series of numerical experiments are designed for a performance analysis of the latent-space learning method. Results are presented for different control problems and for both photonic (continuous-variable) and qubit-based agents, to show how the QNN learning process is improved by the joint training.





## FairStream: Fair Multimedia Streaming Benchmark for Reinforcement Learning Agents
- **Url**: http://arxiv.org/abs/2410.21029v1
- **Authors**: ['Jannis Weil', 'Jonas Ringsdorf', 'Julian Barthel', 'Yi-Ping Phoebe Chen', 'Tobias Meuser']
- **Abstrat**: Multimedia streaming accounts for the majority of traffic in today's internet. Mechanisms like adaptive bitrate streaming control the bitrate of a stream based on the estimated bandwidth, ideally resulting in smooth playback and a good Quality of Experience (QoE). However, selecting the optimal bitrate is challenging under volatile network conditions. This motivated researchers to train Reinforcement Learning (RL) agents for multimedia streaming. The considered training environments are often simplified, leading to promising results with limited applicability. Additionally, the QoE fairness across multiple streams is seldom considered by recent RL approaches. With this work, we propose a novel multi-agent environment that comprises multiple challenges of fair multimedia streaming: partial observability, multiple objectives, agent heterogeneity and asynchronicity. We provide and analyze baseline approaches across five different traffic classes to gain detailed insights into the behavior of the considered agents, and show that the commonly used Proximal Policy Optimization (PPO) algorithm is outperformed by a simple greedy heuristic. Future work includes the adaptation of multi-agent RL algorithms and further expansions of the environment.





## Reference-Free Formula Drift with Reinforcement Learning: From Driving Data to Tire Energy-Inspired, Real-World Policies
- **Url**: http://arxiv.org/abs/2410.20990v1
- **Authors**: ['Franck Djeumou', 'Michael Thompson', 'Makoto Suminaka', 'John Subosits']
- **Abstrat**: The skill to drift a car--i.e., operate in a state of controlled oversteer like professional drivers--could give future autonomous cars maximum flexibility when they need to retain control in adverse conditions or avoid collisions. We investigate real-time drifting strategies that put the car where needed while bypassing expensive trajectory optimization. To this end, we design a reinforcement learning agent that builds on the concept of tire energy absorption to autonomously drift through changing and complex waypoint configurations while safely staying within track bounds. We achieve zero-shot deployment on the car by training the agent in a simulation environment built on top of a neural stochastic differential equation vehicle model learned from pre-collected driving data. Experiments on a Toyota GR Supra and Lexus LC 500 show that the agent is capable of drifting smoothly through varying waypoint configurations with tracking error as low as 10 cm while stably pushing the vehicles to sideslip angles of up to 63{\deg}.





## BlueSuffix: Reinforced Blue Teaming for Vision-Language Models Against Jailbreak Attacks
- **Url**: http://arxiv.org/abs/2410.20971v1
- **Authors**: ['Yunhan Zhao', 'Xiang Zheng', 'Lin Luo', 'Yige Li', 'Xingjun Ma', 'Yu-Gang Jiang']
- **Abstrat**: Despite their superb multimodal capabilities, Vision-Language Models (VLMs) have been shown to be vulnerable to jailbreak attacks, which are inference-time attacks that induce the model to output harmful responses with tricky prompts. It is thus essential to defend VLMs against potential jailbreaks for their trustworthy deployment in real-world applications. In this work, we focus on black-box defense for VLMs against jailbreak attacks. Existing black-box defense methods are either unimodal or bimodal. Unimodal methods enhance either the vision or language module of the VLM, while bimodal methods robustify the model through text-image representation realignment. However, these methods suffer from two limitations: 1) they fail to fully exploit the cross-modal information, or 2) they degrade the model performance on benign inputs. To address these limitations, we propose a novel blue-team method BlueSuffix that defends the black-box target VLM against jailbreak attacks without compromising its performance. BlueSuffix includes three key components: 1) a visual purifier against jailbreak images, 2) a textual purifier against jailbreak texts, and 3) a blue-team suffix generator fine-tuned via reinforcement learning for enhancing cross-modal robustness. We empirically show on three VLMs (LLaVA, MiniGPT-4, and Gemini) and two safety benchmarks (MM-SafetyBench and RedTeam-2K) that BlueSuffix outperforms the baseline defenses by a significant margin. Our BlueSuffix opens up a promising direction for defending VLMs against jailbreak attacks.





## Bilevel Model for Electricity Market Mechanism Optimisation via Quantum Computing Enhanced Reinforcement Learning
- **Url**: http://arxiv.org/abs/2410.20968v1
- **Authors**: ['Shuyang Zhu', 'Ziqing Zhu']
- **Abstrat**: In response to the increasing complexity of electricity markets due to low-carbon requirements and the integration of sustainable energy sources, this paper proposes a dynamic quantum computing enhanced bilevel optimization model for electricity market operations. The upper level focuses on market mechanism optimization using Reinforcement Learning (RL), specifically Proximal Policy Optimization (PPO), while the lower level models the bidding strategies of Generating Companies (GENCOs) using a Multi-Agent Deep Q-Network (MADQN) enhanced with quantum computing through a Variational Quantum Circuit (VQC). The three main contributions of this work are: (1) establishing a dynamic bilevel model with timely feedback between the upper and lower levels; (2) parameterizing and optimizing market mechanisms to derive the most effective designs; and (3) introducing quantum computing into the context of electricity markets to more realistically simulate market operations. The proposed model is tested on the IEEE 30-bus system with six GENCOs, demonstrating its effectiveness in capturing the complexities of modern electricity markets.





## RIME: Robust Preference-based Reinforcement Learning with Noisy Preferences
- **Url**: http://arxiv.org/abs/2402.17257v4
- **Authors**: ['Jie Cheng', 'Gang Xiong', 'Xingyuan Dai', 'Qinghai Miao', 'Yisheng Lv', 'Fei-Yue Wang']
- **Abstrat**: Preference-based Reinforcement Learning (PbRL) circumvents the need for reward engineering by harnessing human preferences as the reward signal. However, current PbRL methods excessively depend on high-quality feedback from domain experts, which results in a lack of robustness. In this paper, we present RIME, a robust PbRL algorithm for effective reward learning from noisy preferences. Our method utilizes a sample selection-based discriminator to dynamically filter out noise and ensure robust training. To counteract the cumulative error stemming from incorrect selection, we suggest a warm start for the reward model, which additionally bridges the performance gap during the transition from pre-training to online training in PbRL. Our experiments on robotic manipulation and locomotion tasks demonstrate that RIME significantly enhances the robustness of the state-of-the-art PbRL method. Code is available at https://github.com/CJReinforce/RIME_ICML2024.





# TD3
# Prioritized Experience Replay
# path planning
## coVoxSLAM: GPU Accelerated Globally Consistent Dense SLAM
- **Url**: http://arxiv.org/abs/2410.21149v1
- **Authors**: ['Emiliano HÃ¶ss', 'Pablo De CristÃ³foris']
- **Abstrat**: A dense SLAM system is essential for mobile robots, as it provides localization and allows navigation, path planning, obstacle avoidance, and decision-making in unstructured environments. Due to increasing computational demands the use of GPUs in dense SLAM is expanding. In this work, we present coVoxSLAM, a novel GPU-accelerated volumetric SLAM system that takes full advantage of the parallel processing power of the GPU to build globally consistent maps even in large-scale environments. It was deployed on different platforms (discrete and embedded GPU) and compared with the state of the art. The results obtained using public datasets show that coVoxSLAM delivers a significant performance improvement considering execution times while maintaining accurate localization. The presented system is available as open-source on GitHub https://github.com/lrse-uba/coVoxSLAM.





## Motion Planning for Robotics: A Review for Sampling-based Planners
- **Url**: http://arxiv.org/abs/2410.19414v2
- **Authors**: ['Liding Zhang', 'Kuanqi Cai', 'Zewei Sun', 'Zhenshan Bing', 'Chaoqun Wang', 'Luis Figueredo', 'Sami Haddadin', 'Alois Knoll']
- **Abstrat**: Recent advancements in robotics have transformed industries such as manufacturing, logistics, surgery, and planetary exploration. A key challenge is developing efficient motion planning algorithms that allow robots to navigate complex environments while avoiding collisions and optimizing metrics like path length, sweep area, execution time, and energy consumption. Among the available algorithms, sampling-based methods have gained the most traction in both research and industry due to their ability to handle complex environments, explore free space, and offer probabilistic completeness along with other formal guarantees. Despite their widespread application, significant challenges still remain. To advance future planning algorithms, it is essential to review the current state-of-the-art solutions and their limitations. In this context, this work aims to shed light on these challenges and assess the development and applicability of sampling-based methods. Furthermore, we aim to provide an in-depth analysis of the design and evaluation of ten of the most popular planners across various scenarios. Our findings highlight the strides made in sampling-based methods while underscoring persistent challenges. This work offers an overview of the important ongoing research in robotic motion planning.




