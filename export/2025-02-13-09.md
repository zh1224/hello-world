# reinforcement learning
## A Real-to-Sim-to-Real Approach to Robotic Manipulation with VLM-Generated Iterative Keypoint Rewards
- **Url**: http://arxiv.org/abs/2502.08643v1
- **Authors**: ['Shivansh Patel', 'Xinchen Yin', 'Wenlong Huang', 'Shubham Garg', 'Hooshang Nayyeri', 'Li Fei-Fei', 'Svetlana Lazebnik', 'Yunzhu Li']
- **Abstrat**: Task specification for robotic manipulation in open-world environments is challenging, requiring flexible and adaptive objectives that align with human intentions and can evolve through iterative feedback. We introduce Iterative Keypoint Reward (IKER), a visually grounded, Python-based reward function that serves as a dynamic task specification. Our framework leverages VLMs to generate and refine these reward functions for multi-step manipulation tasks. Given RGB-D observations and free-form language instructions, we sample keypoints in the scene and generate a reward function conditioned on these keypoints. IKER operates on the spatial relationships between keypoints, leveraging commonsense priors about the desired behaviors, and enabling precise SE(3) control. We reconstruct real-world scenes in simulation and use the generated rewards to train reinforcement learning (RL) policies, which are then deployed into the real world-forming a real-to-sim-to-real loop. Our approach demonstrates notable capabilities across diverse scenarios, including both prehensile and non-prehensile tasks, showcasing multi-step task execution, spontaneous error recovery, and on-the-fly strategy adjustments. The results highlight IKER's effectiveness in enabling robots to perform multi-step tasks in dynamic environments through iterative reward shaping.





## Necessary and Sufficient Oracles: Toward a Computational Taxonomy For Reinforcement Learning
- **Url**: http://arxiv.org/abs/2502.08632v1
- **Authors**: ['Dhruv Rohatgi', 'Dylan J. Foster']
- **Abstrat**: Algorithms for reinforcement learning (RL) in large state spaces crucially rely on supervised learning subroutines to estimate objects such as value functions or transition probabilities. Since only the simplest supervised learning problems can be solved provably and efficiently, practical performance of an RL algorithm depends on which of these supervised learning "oracles" it assumes access to (and how they are implemented). But which oracles are better or worse? Is there a minimal oracle?   In this work, we clarify the impact of the choice of supervised learning oracle on the computational complexity of RL, as quantified by the oracle strength. First, for the task of reward-free exploration in Block MDPs in the standard episodic access model -- a ubiquitous setting for RL with function approximation -- we identify two-context regression as a minimal oracle, i.e. an oracle that is both necessary and sufficient (under a mild regularity assumption). Second, we identify one-context regression as a near-minimal oracle in the stronger reset access model, establishing a provable computational benefit of resets in the process. Third, we broaden our focus to Low-Rank MDPs, where we give cryptographic evidence that the analogous oracle from the Block MDP setting is insufficient.





## Efficient IAM Greybox Penetration Testing
- **Url**: http://arxiv.org/abs/2304.14540v7
- **Authors**: ['Yang Hu', 'Wenxi Wang', 'Sarfraz Khurshid', 'Mohit Tiwari']
- **Abstrat**: Identity and Access Management (IAM) is an access control service in cloud platforms. To securely manage cloud resources, customers need to configure IAM to specify the access control rules for their cloud organizations. However, misconfigured IAM can lead to privilege escalation (PE) attacks, causing significant economic loss. Third-party cloud security services detect such issues using whitebox penetration testing, which requires full access to IAM configurations. However, since these configurations often contain sensitive data, customers must manually anonymize them to protect their privacy. To address the dual challenges of anonymization and data privacy, we introduce TAC, the first greybox penetration testing approach for third-party services to efficiently detect IAM PEs. Instead of requiring customers to blindly anonymize their entire IAM configuration, TAC intelligently interacts with customers by querying only a small fraction of information in the IAM configuration that is necessary for PE detection. To achieve this, TAC integrates two key innovations: (1) a comprehensive IAM modeling approach to detect a wide range of IAM PEs using partial information collected from query responses, and (2) a query optimization mechanism leveraging Reinforcement Learning (RL) and Graph Neural Networks (GNNs) to minimize customer inputs. Additionally, to address the scarcity of real-world IAM PE datasets, we introduce IAMVulGen, a synthesizer that generates a large number of diverse IAM PEs that mimic real-world scenarios. Experimental results on both synthetic and real-world benchmarks show that TAC, as a greybox approach, achieves competitively low and, in some cases, significantly lower false negative rates than state-ofthe-art whitebox approaches, while utilizing a limited number of queries.





## Deep Reinforcement Learning for Bipedal Locomotion: A Brief Survey
- **Url**: http://arxiv.org/abs/2404.17070v3
- **Authors**: ['Lingfan Bao', 'Joseph Humphreys', 'Tianhu Peng', 'Chengxu Zhou']
- **Abstrat**: Bipedal robots are gaining global recognition due to their potential applications and advancements in artificial intelligence, particularly through Deep Reinforcement Learning (DRL). While DRL has significantly advanced bipedal locomotion, the development of a unified framework capable of handling a wide range of tasks remains an ongoing challenge. This survey systematically categorises, compares, and analyses existing DRL frameworks for bipedal locomotion, organising them into end-to-end and hierarchical control schemes. End-to-end frameworks are evaluated based on their learning approaches, while hierarchical frameworks are examined in terms of layered structures that integrate learning-based or traditional model-based methods. We provide a detailed evaluation of the composition, strengths, limitations, and capabilities of each framework. Additionally, this survey identifies key research gaps and proposes future directions aimed at creating a more integrated and efficient framework for bipedal locomotion, with wide-ranging applications in real-world environments.





## Acceleration of crystal structure relaxation with Deep Reinforcement Learning
- **Url**: http://arxiv.org/abs/2502.08405v1
- **Authors**: ['Elena Trukhan', 'Efim Mazhnik', 'Artem R. Oganov']
- **Abstrat**: We introduce a Deep Reinforcement Learning (DRL) model for the structure relaxation of crystal materials and compare different types of neural network architectures and reinforcement learning algorithms for this purpose. Experiments are conducted on Al-Fe structures, with potential energy surfaces generated using EAM potentials. We examine the influence of parameter settings on model performance and benchmark the best-performing models against classical optimization algorithms. Additionally, the model's capacity to generalize learned interaction patterns from smaller atomic systems to more complex systems is assessed. The results demonstrate the potential of DRL models to enhance the efficiency of structure relaxation compared to classical optimizers.





## Learning Humanoid Standing-up Control across Diverse Postures
- **Url**: http://arxiv.org/abs/2502.08378v1
- **Authors**: ['Tao Huang', 'Junli Ren', 'Huayi Wang', 'Zirui Wang', 'Qingwei Ben', 'Muning Wen', 'Xiao Chen', 'Jianan Li', 'Jiangmiao Pang']
- **Abstrat**: Standing-up control is crucial for humanoid robots, with the potential for integration into current locomotion and loco-manipulation systems, such as fall recovery. Existing approaches are either limited to simulations that overlook hardware constraints or rely on predefined ground-specific motion trajectories, failing to enable standing up across postures in real-world scenes. To bridge this gap, we present HoST (Humanoid Standing-up Control), a reinforcement learning framework that learns standing-up control from scratch, enabling robust sim-to-real transfer across diverse postures. HoST effectively learns posture-adaptive motions by leveraging a multi-critic architecture and curriculum-based training on diverse simulated terrains. To ensure successful real-world deployment, we constrain the motion with smoothness regularization and implicit motion speed bound to alleviate oscillatory and violent motions on physical hardware, respectively. After simulation-based training, the learned control policies are directly deployed on the Unitree G1 humanoid robot. Our experimental results demonstrate that the controllers achieve smooth, stable, and robust standing-up motions across a wide range of laboratory and outdoor environments. Videos are available at https://taohuang13.github.io/humanoid-standingup.github.io/.





## Towards Principled Multi-Agent Task Agnostic Exploration
- **Url**: http://arxiv.org/abs/2502.08365v1
- **Authors**: ['Riccardo Zamboni', 'Mirco Mutti', 'Marcello Restelli']
- **Abstrat**: In reinforcement learning, we typically refer to task-agnostic exploration when we aim to explore the environment without access to the task specification a priori. In a single-agent setting the problem has been extensively studied and mostly understood. A popular approach cast the task-agnostic objective as maximizing the entropy of the state distribution induced by the agent's policy, from which principles and methods follows. In contrast, little is known about task-agnostic exploration in multi-agent settings, which are ubiquitous in the real world. How should different agents explore in the presence of others? In this paper, we address this question through a generalization to multiple agents of the problem of maximizing the state distribution entropy. First, we investigate alternative formulations, highlighting respective positives and negatives. Then, we present a scalable, decentralized, trust-region policy search algorithm to address the problem in practical settings. Finally, we provide proof of concept experiments to both corroborate the theoretical findings and pave the way for task-agnostic exploration in challenging multi-agent settings.





## Deterministic generation of non-classical mechanical states in cavity optomechanics via reinforcement learning
- **Url**: http://arxiv.org/abs/2502.08350v1
- **Authors**: ['Yu-Hong Liu', 'Qing-Shou Tan', 'Le-Man Kuang', 'Jie-Qiao Liao']
- **Abstrat**: Non-classical mechanical states, as vital quantum resources for exploring macroscopic quantum behavior, have wide applications in the study of the fundamental quantum mechanics and modern quantum technology. In this work, we propose a scheme for deterministically generating non-classical mechanical states in cavity optomechanical systems. By working in the eigen-representation of the nonlinear optomechanical systems, we identify the carrier-wave resonance conditions and seek for the optimal driving pulses for state preparations. Concretely, we employ the reinforcement learning method to optimize the pulsed driving fields, effectively suppressing the undesired transitions induced by both the pulsed driving fields and dissipations. This approach enables the high-fidelity preparation of phononic Fock states and superposed Fock states in the single-resonator optomechanical systems, as well as two-mode entangled states in the two-resonator optomechanical systems. The statistical properties of the generated states are also examined. Our results open a way for quantum state engineering in quantum optics and quantum information science via reinforcement learning.





## Hierarchical Learning-based Graph Partition for Large-scale Vehicle Routing Problems
- **Url**: http://arxiv.org/abs/2502.08340v1
- **Authors**: ['Yuxin Pan', 'Ruohong Liu', 'Yize Chen', 'Zhiguang Cao', 'Fangzhen Lin']
- **Abstrat**: Neural solvers based on the divide-and-conquer approach for Vehicle Routing Problems (VRPs) in general, and capacitated VRP (CVRP) in particular, integrates the global partition of an instance with local constructions for each subproblem to enhance generalization. However, during the global partition phase, misclusterings within subgraphs have a tendency to progressively compound throughout the multi-step decoding process of the learning-based partition policy. This suboptimal behavior in the global partition phase, in turn, may lead to a dramatic deterioration in the performance of the overall decomposition-based system, despite using optimal local constructions. To address these challenges, we propose a versatile Hierarchical Learning-based Graph Partition (HLGP) framework, which is tailored to benefit the partition of CVRP instances by synergistically integrating global and local partition policies. Specifically, the global partition policy is tasked with creating the coarse multi-way partition to generate the sequence of simpler two-way partition subtasks. These subtasks mark the initiation of the subsequent K local partition levels. At each local partition level, subtasks exclusive for this level are assigned to the local partition policy which benefits from the insensitive local topological features to incrementally alleviate the compounded errors. This framework is versatile in the sense that it optimizes the involved partition policies towards a unified objective harmoniously compatible with both reinforcement learning (RL) and supervised learning (SL). (*Due to the notification of arXiv "The Abstract field cannot be longer than 1,920 characters", the appeared Abstract is shortened. For the full Abstract, please download the Article.)





## Hierarchical Multi-Agent Framework for Carbon-Efficient Liquid-Cooled Data Center Clusters
- **Url**: http://arxiv.org/abs/2502.08337v1
- **Authors**: ['Soumyendu Sarkar', 'Avisek Naug', 'Antonio Guillen', 'Vineet Gundecha', 'Ricardo Luna Gutierrez', 'Sahand Ghorbanpour', 'Sajad Mousavi', 'Ashwin Ramesh Babu', 'Desik Rengarajan', 'Cullen Bash']
- **Abstrat**: Reducing the environmental impact of cloud computing requires efficient workload distribution across geographically dispersed Data Center Clusters (DCCs) and simultaneously optimizing liquid and air (HVAC) cooling with time shift of workloads within individual data centers (DC). This paper introduces Green-DCC, which proposes a Reinforcement Learning (RL) based hierarchical controller to optimize both workload and liquid cooling dynamically in a DCC. By incorporating factors such as weather, carbon intensity, and resource availability, Green-DCC addresses realistic constraints and interdependencies. We demonstrate how the system optimizes multiple data centers synchronously, enabling the scope of digital twins, and compare the performance of various RL approaches based on carbon emissions and sustainability metrics while also offering a framework and benchmark simulation for broader ML research in sustainability.





## Salience-Invariant Consistent Policy Learning for Generalization in Visual Reinforcement Learning
- **Url**: http://arxiv.org/abs/2502.08336v1
- **Authors**: ['Sun Jingbo', 'Tu Songjun', 'Zhang Qichao', 'Chen Ke', 'Zhao Dongbin']
- **Abstrat**: Generalizing policies to unseen scenarios remains a critical challenge in visual reinforcement learning, where agents often overfit to the specific visual observations of the training environment. In unseen environments, distracting pixels may lead agents to extract representations containing task-irrelevant information. As a result, agents may deviate from the optimal behaviors learned during training, thereby hindering visual generalization.To address this issue, we propose the Salience-Invariant Consistent Policy Learning (SCPL) algorithm, an efficient framework for zero-shot generalization. Our approach introduces a novel value consistency module alongside a dynamics module to effectively capture task-relevant representations. The value consistency module, guided by saliency, ensures the agent focuses on task-relevant pixels in both original and perturbed observations, while the dynamics module uses augmented data to help the encoder capture dynamic- and reward-relevant representations. Additionally, our theoretical analysis highlights the importance of policy consistency for generalization. To strengthen this, we introduce a policy consistency module with a KL divergence constraint to maintain consistent policies across original and perturbed observations.Extensive experiments on the DMC-GB, Robotic Manipulation, and CARLA benchmarks demonstrate that SCPL significantly outperforms state-of-the-art methods in terms of generalization. Notably, SCPL achieves average performance improvements of 14\%, 39\%, and 69\% in the challenging DMC video hard setting, the Robotic hard setting, and the CARLA benchmark, respectively.Project Page: https://sites.google.com/view/scpl-rl.





## Noise-conditioned Energy-based Annealed Rewards (NEAR): A Generative Framework for Imitation Learning from Observation
- **Url**: http://arxiv.org/abs/2501.14856v2
- **Authors**: ['Anish Abhijit Diwan', 'Julen Urain', 'Jens Kober', 'Jan Peters']
- **Abstrat**: This paper introduces a new imitation learning framework based on energy-based generative models capable of learning complex, physics-dependent, robot motion policies through state-only expert motion trajectories. Our algorithm, called Noise-conditioned Energy-based Annealed Rewards (NEAR), constructs several perturbed versions of the expert's motion data distribution and learns smooth, and well-defined representations of the data distribution's energy function using denoising score matching. We propose to use these learnt energy functions as reward functions to learn imitation policies via reinforcement learning. We also present a strategy to gradually switch between the learnt energy functions, ensuring that the learnt rewards are always well-defined in the manifold of policy-generated samples. We evaluate our algorithm on complex humanoid tasks such as locomotion and martial arts and compare it with state-only adversarial imitation learning algorithms like Adversarial Motion Priors (AMP). Our framework sidesteps the optimisation challenges of adversarial imitation learning techniques and produces results comparable to AMP in several quantitative metrics across multiple imitation settings.





## The Danger of Overthinking: Examining the Reasoning-Action Dilemma in Agentic Tasks
- **Url**: http://arxiv.org/abs/2502.08235v1
- **Authors**: ['Alejandro Cuadron', 'Dacheng Li', 'Wenjie Ma', 'Xingyao Wang', 'Yichuan Wang', 'Siyuan Zhuang', 'Shu Liu', 'Luis Gaspar Schroeder', 'Tian Xia', 'Huanzhi Mao', 'Nicholas Thumiger', 'Aditya Desai', 'Ion Stoica', 'Ana Klimovic', 'Graham Neubig', 'Joseph E. Gonzalez']
- **Abstrat**: Large Reasoning Models (LRMs) represent a breakthrough in AI problem-solving capabilities, but their effectiveness in interactive environments can be limited. This paper introduces and analyzes overthinking in LRMs. A phenomenon where models favor extended internal reasoning chains over environmental interaction. Through experiments on software engineering tasks using SWE Bench Verified, we observe three recurring patterns: Analysis Paralysis, Rogue Actions, and Premature Disengagement. We propose a framework to study these behaviors, which correlates with human expert assessments, and analyze 4018 trajectories. We observe that higher overthinking scores correlate with decreased performance, with reasoning models exhibiting stronger tendencies toward overthinking compared to non-reasoning models. Our analysis reveals that simple efforts to mitigate overthinking in agentic environments, such as selecting the solution with the lower overthinking score, can improve model performance by almost 30% while reducing computational costs by 43%. These results suggest that mitigating overthinking has strong practical implications. We suggest that by leveraging native function-calling capabilities and selective reinforcement learning overthinking tendencies could be mitigated. We also open-source our evaluation framework and dataset to facilitate research in this direction at https://github.com/AlexCuadron/Overthinking.





## Geometry-aware RL for Manipulation of Varying Shapes and Deformable Objects
- **Url**: http://arxiv.org/abs/2502.07005v2
- **Authors**: ['Tai Hoang', 'Huy Le', 'Philipp Becker', 'Vien Anh Ngo', 'Gerhard Neumann']
- **Abstrat**: Manipulating objects with varying geometries and deformable objects is a major challenge in robotics. Tasks such as insertion with different objects or cloth hanging require precise control and effective modelling of complex dynamics. In this work, we frame this problem through the lens of a heterogeneous graph that comprises smaller sub-graphs, such as actuators and objects, accompanied by different edge types describing their interactions. This graph representation serves as a unified structure for both rigid and deformable objects tasks, and can be extended further to tasks comprising multiple actuators. To evaluate this setup, we present a novel and challenging reinforcement learning benchmark, including rigid insertion of diverse objects, as well as rope and cloth manipulation with multiple end-effectors. These tasks present a large search space, as both the initial and target configurations are uniformly sampled in 3D space. To address this issue, we propose a novel graph-based policy model, dubbed Heterogeneous Equivariant Policy (HEPi), utilizing $SE(3)$   equivariant message passing networks as the main backbone to exploit the geometric symmetry. In addition, by modeling explicit heterogeneity, HEPi can outperform Transformer-based and non-heterogeneous equivariant policies in terms of average returns, sample efficiency, and generalization to unseen objects.





## Sample-Efficient Reinforcement Learning from Human Feedback via Information-Directed Sampling
- **Url**: http://arxiv.org/abs/2502.05434v2
- **Authors**: ['Han Qi', 'Haochen Yang', 'Qiaosheng Zhang', 'Zhuoran Yang']
- **Abstrat**: We study the problem of reinforcement learning from human feedback (RLHF), a critical problem in training large language models, from a theoretical perspective. Our main contribution is the design of novel sample-efficient RLHF algorithms based on information-directed sampling (IDS), an online decision-making principle inspired by information theory. Our algorithms maximize the sum of the value function and a mutual information term that encourages exploration of the unknown environment (which quantifies the information gained about the environment through observed human feedback data). To tackle the challenge of large state spaces and improve sample efficiency, we construct a simplified \emph{surrogate environment} and introduce a novel distance measure (named the \emph{$\ell_g$-distance}), enabling our IDS-based algorithm to achieve a Bayesian regret upper bound of order $O(H^{\frac{3}{2}}\sqrt{\log(K(\epsilon)) T})$, where $H$ is the episode length, $T$ is the number of episode and $K(\epsilon)$ is related to the covering number of the environment. Specializing to the tabular settings, this regret bound is of order $\tilde{O}(H^2\sqrt{SAT})$, where $S$ and $A$ are the numbers of states and actions. Finally, we propose an Approximate-IDS algorithm that is computationally more efficient while maintaining nearly the same sample efficiency. The design principle of this approximate algorithm is not only effective in RLHF settings but also applicable to the standard RL framework. Moreover, our work showcases the value of information theory in reinforcement learning and in the training of large language models.





# TD3
# Prioritized Experience Replay
# path planning
## Accelerating Stable Matching between Workers and Time-Dependent Tasks for Dynamic MCS: A Stagewise Service Trading Approach
- **Url**: http://arxiv.org/abs/2502.08386v1
- **Authors**: ['Houyi Qi', 'Minghui Liwang', 'Xianbin Wang', 'Liqun Fu', 'Yiguang Hong', 'Li Li', 'Zhipeng Cheng']
- **Abstrat**: Designing proper incentives in mobile crowdsensing (MCS) networks represents a critical mechanism in engaging distributed mobile users (workers) to contribute heterogeneous data for diverse applications (tasks). We develop a novel stagewise trading framework to reach efficient and stable matching between tasks and workers, upon considering the diversity of tasks and the dynamism of MCS networks. This framework integrates futures and spot trading stages, where in the former, we propose futures trading-driven stable matching and pre-path-planning (FT-SMP^3) for long-term task-worker assignment and pre-planning of workers' paths based on historical statistics and risk analysis. While in the latter, we investigate spot trading-driven DQN path planning and onsite worker recruitment (ST-DP^2WR) mechanism to enhance workers' and tasks' practical utilities by facilitating temporary worker recruitment. We prove that our proposed mechanisms support crucial properties such as stability, individual rationality, competitive equilibrium, and weak Pareto optimality theoretically. Also, comprehensive evaluations confirm the satisfaction of these properties in practical network settings, demonstrating our commendable performance in terms of service quality, running time, and decision-making overheads.





## Fare Structure Design in Public Transport
- **Url**: http://arxiv.org/abs/2502.08228v1
- **Authors**: ['Anita Schöbel', 'Reena Urban']
- **Abstrat**: Fare planning is one among several steps in public transport planning. Fares are relevant for the covering of costs of the public transport operator, but also affect the ridership and the passenger satisfaction. A fare structure is the assignment of prices to all paths in a network. In practice, often a given fare structure shall be changed to fulfill new requirements, meaning that a new fare strategy is desired. This motivates the usage of prices of the former fare structure or other desirable prices as reference prices. In this paper, we investigate the fare structure design problem that aims to determine fares such that the sum of absolute deviations between the new fares and the reference prices is minimized. Fare strategies that are considered here are flat tariffs, affine distance tariffs and zone tariffs. Additionally, we regard constraints that ensure that it is not beneficial to buy a ticket for a longer journey than actually traveled (no-elongation property) or to split a ticket into several sub-tickets to cover a journey (no-stopover property). Our literature review provides an overview of the research on fare planning. We analyze the fare structure design problem for flat, distance and zone tariffs, pointing out connections to median problems. Further, we study its complexity which ranges from linear-time solvability to NP-complete cases.




