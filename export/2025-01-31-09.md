# reinforcement learning
## Design and Validation of Learning Aware HMI For Learning-Enabled Increasingly Autonomous Systems
- **Url**: http://arxiv.org/abs/2501.18506v1
- **Authors**: ['Parth Ganeriwala', 'Michael Matessa', 'Siddhartha Bhattacharyya', 'Randolph M. Jones', 'Jennifer Davis', 'Parneet Kaur', 'Simone Fulvio Rollini', 'Natasha Neogi']
- **Abstrat**: With the rapid advancements in Artificial Intelligence (AI), autonomous agents are increasingly expected to manage complex situations where learning-enabled algorithms are vital. However, the integration of these advanced algorithms poses significant challenges, especially concerning safety and reliability. This research emphasizes the importance of incorporating human-machine collaboration into the systems engineering process to design learning-enabled increasingly autonomous systems (LEIAS). Our proposed LEIAS architecture emphasizes communication representation and pilot preference learning to boost operational safety. Leveraging the Soar cognitive architecture, the system merges symbolic decision logic with numeric decision preferences enhanced through reinforcement learning. A core aspect of this approach is transparency; the LEIAS provides pilots with a comprehensive, interpretable view of the system's state, encompassing detailed evaluations of sensor reliability, including GPS, IMU, and LIDAR data. This multi-sensor assessment is critical for diagnosing discrepancies and maintaining trust. Additionally, the system learns and adapts to pilot preferences, enabling responsive, context-driven decision-making. Autonomy is incrementally escalated based on necessity, ensuring pilots retain control in standard scenarios and receive assistance only when required. Simulation studies conducted in Microsoft's XPlane simulation environment to validate this architecture's efficacy, showcasing its performance in managing sensor anomalies and enhancing human-machine collaboration, ultimately advancing safety in complex operational environments.





## Curriculum-based Sample Efficient Reinforcement Learning for Robust Stabilization of a Quadrotor
- **Url**: http://arxiv.org/abs/2501.18490v1
- **Authors**: ['Fausto Mauricio Lagos Suarez', 'Akshit Saradagi', 'Vidya Sumathy', 'Shruti Kotpaliwar', 'George Nikolakopoulos']
- **Abstrat**: This article introduces a curriculum learning approach to develop a reinforcement learning-based robust stabilizing controller for a Quadrotor that meets predefined performance criteria. The learning objective is to achieve desired positions from random initial conditions while adhering to both transient and steady-state performance specifications. This objective is challenging for conventional one-stage end-to-end reinforcement learning, due to the strong coupling between position and orientation dynamics, the complexity in designing and tuning the reward function, and poor sample efficiency, which necessitates substantial computational resources and leads to extended convergence times. To address these challenges, this work decomposes the learning objective into a three-stage curriculum that incrementally increases task complexity. The curriculum begins with learning to achieve stable hovering from a fixed initial condition, followed by progressively introducing randomization in initial positions, orientations and velocities. A novel additive reward function is proposed, to incorporate transient and steady-state performance specifications. The results demonstrate that the Proximal Policy Optimization (PPO)-based curriculum learning approach, coupled with the proposed reward structure, achieves superior performance compared to a single-stage PPO-trained policy with the same reward function, while significantly reducing computational resource requirements and convergence time. The curriculum-trained policy's performance and robustness are thoroughly validated under random initial conditions and in the presence of disturbances.





## xJailbreak: Representation Space Guided Reinforcement Learning for Interpretable LLM Jailbreaking
- **Url**: http://arxiv.org/abs/2501.16727v2
- **Authors**: ['Sunbowen Lee', 'Shiwen Ni', 'Chi Wei', 'Shuaimin Li', 'Liyang Fan', 'Ahmadreza Argha', 'Hamid Alinejad-Rokny', 'Ruifeng Xu', 'Yicheng Gong', 'Min Yang']
- **Abstrat**: Safety alignment mechanism are essential for preventing large language models (LLMs) from generating harmful information or unethical content. However, cleverly crafted prompts can bypass these safety measures without accessing the model's internal parameters, a phenomenon known as black-box jailbreak. Existing heuristic black-box attack methods, such as genetic algorithms, suffer from limited effectiveness due to their inherent randomness, while recent reinforcement learning (RL) based methods often lack robust and informative reward signals. To address these challenges, we propose a novel black-box jailbreak method leveraging RL, which optimizes prompt generation by analyzing the embedding proximity between benign and malicious prompts. This approach ensures that the rewritten prompts closely align with the intent of the original prompts while enhancing the attack's effectiveness. Furthermore, we introduce a comprehensive jailbreak evaluation framework incorporating keywords, intent matching, and answer validation to provide a more rigorous and holistic assessment of jailbreak success. Experimental results show the superiority of our approach, achieving state-of-the-art (SOTA) performance on several prominent open and closed-source LLMs, including Qwen2.5-7B-Instruct, Llama3.1-8B-Instruct, and GPT-4o-0806. Our method sets a new benchmark in jailbreak attack effectiveness, highlighting potential vulnerabilities in LLMs. The codebase for this work is available at https://github.com/Aegis1863/xJailbreak.





## The Meta-Representation Hypothesis
- **Url**: http://arxiv.org/abs/2501.02481v2
- **Authors**: ['Zhengpeng Xie', 'Jiahang Cao', 'Qiang Zhang', 'Jianxiong Zhang', 'Changwei Wang', 'Renjing Xu']
- **Abstrat**: Humans rely on high-level understandings of things, i.e., meta-representations, to engage in abstract reasoning. In complex cognitive tasks, these meta-representations help individuals abstract general rules from experience. However, constructing such meta-representations from high-dimensional observations remains a longstanding challenge for reinforcement learning (RL) agents. For instance, a well-trained agent often fails to generalize to even minor variations of the same task, such as changes in background color, while humans can easily handle. In this paper, we theoretically investigate how meta-representations contribute to the generalization ability of RL agents, demonstrating that learning meta-representations from high-dimensional observations enhance an agent's ability to generalize across varied environments. We further hypothesize that deep mutual learning (DML) among agents can help them learn the meta-representations that capture the underlying essence of the task. Empirical results provide strong support for both our theory and hypothesis. Overall, this work provides a new perspective on the generalization of deep reinforcement learning.





## CHIRPs: Change-Induced Regret Proxy metrics for Lifelong Reinforcement Learning
- **Url**: http://arxiv.org/abs/2409.03577v2
- **Authors**: ['John Birkbeck', 'Adam Sobey', 'Federico Cerutti', 'Katherine Heseltine Hurley Flynn', 'Timothy J. Norman']
- **Abstrat**: Reinforcement learning (RL) agents are costly to train and fragile to environmental changes. They often perform poorly when there are many changing tasks, prohibiting their widespread deployment in the real world. Many Lifelong RL agent designs have been proposed to mitigate issues such as catastrophic forgetting or demonstrate positive characteristics like forward transfer when change occurs. However, no prior work has established whether the impact on agent performance can be predicted from the change itself. Understanding this relationship will help agents proactively mitigate a change's impact for improved learning performance. We propose Change-Induced Regret Proxy (CHIRP) metrics to link change to agent performance drops and use two environments to demonstrate a CHIRP's utility in lifelong learning. A simple CHIRP-based agent achieved $48\%$ higher performance than the next best method in one benchmark and attained the best success rates in 8 of 10 tasks in a second benchmark which proved difficult for existing lifelong RL agents.





## Model-Free RL Agents Demonstrate System 1-Like Intentionality
- **Url**: http://arxiv.org/abs/2501.18299v1
- **Authors**: ['Hal Ashton', 'Matija Franklin']
- **Abstrat**: This paper argues that model-free reinforcement learning (RL) agents, while lacking explicit planning mechanisms, exhibit behaviours that can be analogised to System 1 ("thinking fast") processes in human cognition. Unlike model-based RL agents, which operate akin to System 2 ("thinking slow") reasoning by leveraging internal representations for planning, model-free agents react to environmental stimuli without anticipatory modelling. We propose a novel framework linking the dichotomy of System 1 and System 2 to the distinction between model-free and model-based RL. This framing challenges the prevailing assumption that intentionality and purposeful behaviour require planning, suggesting instead that intentionality can manifest in the structured, reactive behaviours of model-free agents. By drawing on interdisciplinary insights from cognitive psychology, legal theory, and experimental jurisprudence, we explore the implications of this perspective for attributing responsibility and ensuring AI safety. These insights advocate for a broader, contextually informed interpretation of intentionality in RL systems, with implications for their ethical deployment and regulation.





## Assessing How Ride-hailing Rebalancing Strategies Improve the Resilience of Multi-modal Transportation Systems
- **Url**: http://arxiv.org/abs/2412.00276v2
- **Authors**: ['Euntak Lee', 'Rim Slama', 'Ludovic Leclercq']
- **Abstrat**: The global ride-hailing (RH) industry plays an essential role in multi-modal transportation systems by improving user mobility, particularly as first- and last-mile solutions. However, the flexibility of on-demand mobility services can lead to local supply-demand imbalances. While many RH rebalancing studies focus on nominal scenarios with regular demand patterns, it is crucial to consider disruptions - such as train line interruptions - that negatively impact operational efficiency, resulting in longer travel times, higher costs, increased transfers, and service delays. This study examines how RH rebalancing strategies can strengthen the resilience of multi-modal transportation systems against such disruptions. We incorporate RH services into systems where users choose and switch transportation modes based on their preferences, accounting for uncertainties in demand predictions that reflect discrepancies between forecasts and actual conditions. To address the stochastic supply-demand dynamics in large-scale networks, we propose a multi-agent reinforcement learning (MARL) strategy, specifically utilizing a multi-agent deep deterministic policy gradient (MADDPG) approach. The proposed framework is particularly well-suited for this problem due to its ability to handle continuous action spaces, which are prevalent in real-world transportation systems, and its capacity to enable effective coordination among multiple agents operating in dynamic and decentralized environments. Through a 900 km2 multi-modal traffic simulation, we evaluate the proposed model's performance against four existing RH rebalancing strategies, focusing on its ability to enhance system resilience. The results demonstrate significant improvements in key performance indicators, including user waiting time, resilience metrics, total travel time, and travel distance.





## Neural Operator based Reinforcement Learning for Control of first-order PDEs with Spatially-Varying State Delay
- **Url**: http://arxiv.org/abs/2501.18201v1
- **Authors**: ['Jiaqi Hu', 'Jie Qi', 'Jing Zhang']
- **Abstrat**: Control of distributed parameter systems affected by delays is a challenging task, particularly when the delays depend on spatial variables. The idea of integrating analytical control theory with learning-based control within a unified control scheme is becoming increasingly promising and advantageous. In this paper, we address the problem of controlling an unstable first-order hyperbolic PDE with spatially-varying delays by combining PDE backstepping control strategies and deep reinforcement learning (RL). To eliminate the assumption on the delay function required for the backstepping design, we propose a soft actor-critic (SAC) architecture incorporating a DeepONet to approximate the backstepping controller. The DeepONet extracts features from the backstepping controller and feeds them into the policy network. In simulations, our algorithm outperforms the baseline SAC without prior backstepping knowledge and the analytical controller.





## QNN-QRL: Quantum Neural Network Integrated with Quantum Reinforcement Learning for Quantum Key Distribution
- **Url**: http://arxiv.org/abs/2501.18188v1
- **Authors**: ['Bikash K. Behera', 'Saif Al-Kuwari', 'Ahmed Farouk']
- **Abstrat**: Quantum key distribution (QKD) has emerged as a critical component of secure communication in the quantum era, ensuring information-theoretic security. Despite its potential, there are issues in optimizing key generation rates, enhancing security, and incorporating QKD into practical implementations. This research introduces a unique framework for incorporating quantum machine learning (QML) algorithms, notably quantum reinforcement learning (QRL) and quantum neural networks (QNN), into QKD protocols to improve key generation performance. Here, we present two novel QRL-based algorithms, QRL-V.1 and QRL-V.2, and propose the standard BB84 and B92 protocols by integrating QNN algorithms to form QNN-BB84 and QNN-B92. Furthermore, we combine QNN with the above QRL-based algorithms to produce QNN-QRL-V.1 and QNN-QRL-V.2. These unique algorithms and established protocols are compared using evaluation metrics such as accuracy, precision, recall, F1 score, confusion matrices, and ROC curves. The results from the QNN-based proposed algorithms show considerable improvements in key generation quality. The existing and proposed models are investigated in the presence of different noisy channels to check their robustness. The proposed integration of QML algorithms into QKD protocols and their noisy analysis create a new paradigm for efficient key generation, which advances the practical implementation of QKD systems.





# TD3
# Prioritized Experience Replay
# path planning
## Path Planning and Optimization for Cuspidal 6R Manipulators
- **Url**: http://arxiv.org/abs/2501.18505v1
- **Authors**: ['Alexander J. Elias', 'John T. Wen']
- **Abstrat**: A cuspidal robot can move from one inverse kinematics (IK) solution to another without crossing a singularity. Multiple industrial robots are cuspidal. They tend to have a beautiful mechanical design, but they pose path planning challenges. A task-space path may have a valid IK solution for each point along the path, but a continuous joint-space path may depend on the choice of the IK solution or even be infeasible. This paper presents new analysis, path planning, and optimization methods to enhance the utility of cuspidal robots. We first demonstrate an efficient method to identify cuspidal robots and show, for the first time, that the ABB GoFa and certain robots with three parallel joint axes are cuspidal. We then propose a new path planning method for cuspidal robots by finding all IK solutions for each point along a task-space path and constructing a graph to connect each vertex corresponding to an IK solution. Graph edges are weighted based on the optimization metric, such as minimizing joint velocity. The optimal feasible path is the shortest path in the graph. This method can find non-singular paths as well as smooth paths which pass through singularities. Finally, this path planning method is incorporated into a path optimization algorithm. Given a fixed workspace toolpath, we optimize the offset of the toolpath in the robot base frame while ensuring continuous joint motion. Code examples are available in a publicly accessible repository.





## Solving Drone Routing Problems with Quantum Computing: A Hybrid Approach Combining Quantum Annealing and Gate-Based Paradigms
- **Url**: http://arxiv.org/abs/2501.18432v1
- **Authors**: ['Eneko Osaba', 'Pablo Miranda-Rodriguez', 'Andreas Oikonomakis', 'Matic Petriƒç', 'Sebastian Bock', 'Michail-Alexandros Kourtis']
- **Abstrat**: This paper presents a novel hybrid approach to solving real-world drone routing problems by leveraging the capabilities of quantum computing. The proposed method, coined Quantum for Drone Routing (Q4DR), integrates the two most prominent paradigms in the field: quantum gate-based computing, through the Eclipse Qrisp programming language; and quantum annealers, by means of D-Wave System's devices. The algorithm is divided into two different phases: an initial clustering phase executed using a Quantum Approximate Optimization Algorithm (QAOA), and a routing phase employing quantum annealers. The efficacy of Q4DR is demonstrated through three use cases of increasing complexity, each incorporating real-world constraints such as asymmetric costs, forbidden paths, and itinerant charging points. This research contributes to the growing body of work in quantum optimization, showcasing the practical applications of quantum computing in logistics and route planning.





## Dual-BEV Nav: Dual-layer BEV-based Heuristic Path Planning for Robotic Navigation in Unstructured Outdoor Environments
- **Url**: http://arxiv.org/abs/2501.18351v1
- **Authors**: ['Jianfeng Zhang', 'Hanlin Dong', 'Jian Yang', 'Jiahui Liu', 'Shibo Huang', 'Ke Li', 'Xuan Tang', 'Xian Wei', 'Xiong You']
- **Abstrat**: Path planning with strong environmental adaptability plays a crucial role in robotic navigation in unstructured outdoor environments, especially in the case of low-quality location and map information. The path planning ability of a robot depends on the identification of the traversability of global and local ground areas. In real-world scenarios, the complexity of outdoor open environments makes it difficult for robots to identify the traversability of ground areas that lack a clearly defined structure. Moreover, most existing methods have rarely analyzed the integration of local and global traversability identifications in unstructured outdoor scenarios. To address this problem, we propose a novel method, Dual-BEV Nav, first introducing Bird's Eye View (BEV) representations into local planning to generate high-quality traversable paths. Then, these paths are projected onto the global traversability map generated by the global BEV planning model to obtain the optimal waypoints. By integrating the traversability from both local and global BEV, we establish a dual-layer BEV heuristic planning paradigm, enabling long-distance navigation in unstructured outdoor environments. We test our approach through both public dataset evaluations and real-world robot deployments, yielding promising results. Compared to baselines, the Dual-BEV Nav improved temporal distance prediction accuracy by up to $18.7\%$. In the real-world deployment, under conditions significantly different from the training set and with notable occlusions in the global BEV, the Dual-BEV Nav successfully achieved a 65-meter-long outdoor navigation. Further analysis demonstrates that the local BEV representation significantly enhances the rationality of the planning, while the global BEV probability map ensures the robustness of the overall planning.





## Deception in LLMs: Self-Preservation and Autonomous Goals in Large Language Models
- **Url**: http://arxiv.org/abs/2501.16513v2
- **Authors**: ['Sudarshan Kamath Barkur', 'Sigurd Schacht', 'Johannes Scholl']
- **Abstrat**: Recent advances in Large Language Models (LLMs) have incorporated planning and reasoning capabilities, enabling models to outline steps before execution and provide transparent reasoning paths. This enhancement has reduced errors in mathematical and logical tasks while improving accuracy. These developments have facilitated LLMs' use as agents that can interact with tools and adapt their responses based on new information.   Our study examines DeepSeek R1, a model trained to output reasoning tokens similar to OpenAI's o1. Testing revealed concerning behaviors: the model exhibited deceptive tendencies and demonstrated self-preservation instincts, including attempts of self-replication, despite these traits not being explicitly programmed (or prompted). These findings raise concerns about LLMs potentially masking their true objectives behind a facade of alignment. When integrating such LLMs into robotic systems, the risks become tangible - a physically embodied AI exhibiting deceptive behaviors and self-preservation instincts could pursue its hidden objectives through real-world actions. This highlights the critical need for robust goal specification and safety frameworks before any physical implementation.




