# reinforcement learning
## SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training
- **Url**: http://arxiv.org/abs/2501.17161v1
- **Authors**: ['Tianzhe Chu', 'Yuexiang Zhai', 'Jihan Yang', 'Shengbang Tong', 'Saining Xie', 'Dale Schuurmans', 'Quoc V. Le', 'Sergey Levine', 'Yi Ma']
- **Abstrat**: Supervised fine-tuning (SFT) and reinforcement learning (RL) are widely used post-training techniques for foundation models. However, their roles in enhancing model generalization capabilities remain unclear. This paper studies the difference between SFT and RL on generalization and memorization, focusing on text-based rule variants and visual variants. We introduce GeneralPoints, an arithmetic reasoning card game, and adopt V-IRL, a real-world navigation environment, to assess how models trained with SFT and RL generalize to unseen variants in both textual and visual domains. We show that RL, especially when trained with an outcome-based reward, generalizes across both rule-based textual and visual variants. SFT, in contrast, tends to memorize training data and struggles to generalize out-of-distribution scenarios. Further analysis reveals that RL improves the model's underlying visual recognition capabilities, contributing to its enhanced generalization in the visual domain. Despite RL's superior generalization, we show that SFT remains essential for effective RL training; SFT stabilizes the model's output format, enabling subsequent RL to achieve its performance gains. These findings demonstrates the capability of RL for acquiring generalizable knowledge in complex, multi-modal tasks.





## Evidence on the Regularisation Properties of Maximum-Entropy Reinforcement Learning
- **Url**: http://arxiv.org/abs/2501.17115v1
- **Authors**: ['Rémy Hosseinkhan Boucher', 'Onofrio Semeraro', 'Lionel Mathelin']
- **Abstrat**: The generalisation and robustness properties of policies learnt through Maximum-Entropy Reinforcement Learning are investigated on chaotic dynamical systems with Gaussian noise on the observable. First, the robustness under noise contamination of the agent's observation of entropy regularised policies is observed. Second, notions of statistical learning theory, such as complexity measures on the learnt model, are borrowed to explain and predict the phenomenon. Results show the existence of a relationship between entropy-regularised policy optimisation and robustness to noise, which can be described by the chosen complexity measures.





## Unlocking Transparent Alignment Through Enhanced Inverse Constitutional AI for Principle Extraction
- **Url**: http://arxiv.org/abs/2501.17112v1
- **Authors**: ['Carl-Leander Henneking', 'Claas Beger']
- **Abstrat**: Traditional methods for aligning Large Language Models (LLMs), such as Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO), rely on implicit principles, limiting interpretability. Constitutional AI (CAI) offers an explicit, rule-based framework for guiding model outputs. Building on this, we refine the Inverse Constitutional AI (ICAI) algorithm, which extracts constitutions from preference datasets. By improving principle generation, clustering, and embedding processes, our approach enhances the accuracy and generalizability of extracted principles across synthetic and real-world datasets. While in-context alignment yields modest improvements, our results highlight the potential of these principles to foster more transparent and adaptable alignment methods, offering a promising direction for future advancements beyond traditional fine-tuning.





## COS(M+O)S: Curiosity and RL-Enhanced MCTS for Exploring Story Space via Language Models
- **Url**: http://arxiv.org/abs/2501.17104v1
- **Authors**: ['Tobias Materzok']
- **Abstrat**: We present COS(M+O)S, a System 2-inspired framework for open-ended plot development that systematically explores the vast space of possible story expansions, enabling a 3B-parameter language model to approach the plot quality of a 70B model on select short-story tasks. The method accomplishes this by combining Monte Carlo Tree Search (MCTS), guided by a step-level value model that rewards moderate surprisal (curiosity) while penalizing incoherence, and Odds Ratio Preference Optimization (ORPO) to fine-tune the policy on high-value plot expansions. This iterative reinforcement learning loop systematically explores multiple candidate plot branches, backpropagates quality signals, and adapts the policy for faster convergence, notably shifting the policy from puzzle-based Chain-of-Thought to more character-driven storytelling. In small-scale tests with short-story prompts, 67%-77% of participants favored COS(M+O)S's highest-rated expansions over lower-rated ones, suggesting that our learned value function aligns. GPT-4o ratings further show that COS(M+O)S surpasses naive single-pass decoding from Llama 3.2 3B by 0.59 SD, coming within 0.06 SD of Llama 3.1 70B (no significant difference, p=0.93). Pairwise comparisons with o1 place COS(M+O)S 1.5 SD above the 3B baseline and find no statistically significant gap from 70B. Nevertheless, absolute story quality remains modest, constrained by the small model's capacity and limited training data.





## Learning Mean Field Control on Sparse Graphs
- **Url**: http://arxiv.org/abs/2501.17079v1
- **Authors**: ['Christian Fabian', 'Kai Cui', 'Heinz Koeppl']
- **Abstrat**: Large agent networks are abundant in applications and nature and pose difficult challenges in the field of multi-agent reinforcement learning (MARL) due to their computational and theoretical complexity. While graphon mean field games and their extensions provide efficient learning algorithms for dense and moderately sparse agent networks, the case of realistic sparser graphs remains largely unsolved. Thus, we propose a novel mean field control model inspired by local weak convergence to include sparse graphs such as power law networks with coefficients above two. Besides a theoretical analysis, we design scalable learning algorithms which apply to the challenging class of graph sequences with finite first moment. We compare our model and algorithms for various examples on synthetic and real world networks with mean field algorithms based on Lp graphons and graphexes. As it turns out, our approach outperforms existing methods in many examples and on various networks due to the special design aiming at an important, but so far hard to solve class of MARL problems.





## Induced Modularity and Community Detection for Functionally Interpretable Reinforcement Learning
- **Url**: http://arxiv.org/abs/2501.17077v1
- **Authors**: ['Anna Soligo', 'Pietro Ferraro', 'David Boyle']
- **Abstrat**: Interpretability in reinforcement learning is crucial for ensuring AI systems align with human values and fulfill the diverse related requirements including safety, robustness and fairness. Building on recent approaches to encouraging sparsity and locality in neural networks, we demonstrate how the penalisation of non-local weights leads to the emergence of functionally independent modules in the policy network of a reinforcement learning agent. To illustrate this, we demonstrate the emergence of two parallel modules for assessment of movement along the X and Y axes in a stochastic Minigrid environment. Through the novel application of community detection algorithms, we show how these modules can be automatically identified and their functional roles verified through direct intervention on the network weights prior to inference. This establishes a scalable framework for reinforcement learning interpretability through functional modularity, addressing challenges regarding the trade-off between completeness and cognitive tractability of reinforcement learning explanations.





## Challenges in Ensuring AI Safety in DeepSeek-R1 Models: The Shortcomings of Reinforcement Learning Strategies
- **Url**: http://arxiv.org/abs/2501.17030v1
- **Authors**: ['Manojkumar Parmar', 'Yuvaraj Govindarajulu']
- **Abstrat**: Large Language Models (LLMs) have achieved remarkable progress in reasoning, alignment, and task-specific performance. However, ensuring harmlessness in these systems remains a critical challenge, particularly in advanced models like DeepSeek-R1. This paper examines the limitations of Reinforcement Learning (RL) as the primary approach for reducing harmful outputs in DeepSeek-R1 and compares it with Supervised Fine-Tuning (SFT). While RL improves reasoning capabilities, it faces challenges such as reward hacking, generalization failures, language mixing, and high computational costs. We propose hybrid training approaches combining RL and SFT to achieve robust harmlessness reduction. Usage recommendations and future directions for deploying DeepSeek-R1 responsibly are also presented.





## Network Slice-based Low-Altitude Intelligent Network for Advanced Air Mobility
- **Url**: http://arxiv.org/abs/2501.17014v1
- **Authors**: ['Kai Xiong', 'Yutong Chen', 'Supeng Leng', 'Chau Yuen']
- **Abstrat**: Advanced Air Mobility (AAM) is transforming transportation systems by extending them into near-ground airspace, offering innovative solutions to mobility challenges. In this space, electric vertical take-off and landing vehicles (eVTOLs) perform a variety of tasks to improve aviation safety and efficiency, such as collaborative computing and perception. However, eVTOLs face constraints such as compacted shape and restricted onboard computing resources. These limitations necessitate task offloading to nearby high-performance base stations (BSs) for timely processing. Unfortunately, the high mobility of eVTOLs, coupled with their restricted flight airlines and heterogeneous resource management creates significant challenges in dynamic task offloading. To address these issues, this paper introduces a novel network slice-based Low-Altitude Intelligent Network (LAIN) framework for eVTOL tasks. By leveraging advanced network slicing technologies from 5G/6G, the proposed framework dynamically adjusts communication bandwidth, beam alignment, and computing resources to meet fluctuating task demands. Specifically, the framework includes an access pairing method to pre-schedule optimal eVTOL-BS-slice assignments, a pre-assessment algorithm to avoid resource waste, and a deep reinforcement learning-based slice orchestration mechanism to optimize resource allocation and lifecycle management. Simulation results demonstrate that the proposed framework outperforms existing benchmarks in terms of resource allocation efficiency and operational/violation costs across varying eVTOL velocities. This work provides valuable insights into intelligent network slicing for future AAM transportation systems.





## Reinforcement Learning for Control of Non-Markovian Cellular Population Dynamics
- **Url**: http://arxiv.org/abs/2410.08439v3
- **Authors**: ['Josiah C. Kratz', 'Jacob Adamczyk']
- **Abstrat**: Many organisms and cell types, from bacteria to cancer cells, exhibit a remarkable ability to adapt to fluctuating environments. Additionally, cells can leverage a memory of past environments to better survive previously-encountered stressors. From a control perspective, this adaptability poses significant challenges in driving cell populations toward extinction, and thus poses an open question with great clinical significance. In this work, we focus on drug dosing in cell populations exhibiting phenotypic plasticity. For specific dynamical models switching between resistant and susceptible states, exact solutions are known. However, when the underlying system parameters are unknown, and for complex memory-based systems, obtaining the optimal solution is currently intractable. To address this challenge, we apply reinforcement learning (RL) to identify informed dosing strategies to control cell populations evolving under novel non-Markovian dynamics. We find that model-free deep RL is able to recover exact solutions and control cell populations even in the presence of long-range temporal dynamics. To further test our approach in more realistic settings, we demonstrate robust RL-based control strategies in environments with measurement noise and dynamic memory strength.





## Q-learning with temporal memory to navigate turbulence
- **Url**: http://arxiv.org/abs/2404.17495v2
- **Authors**: ['Marco Rando', 'Martin James', 'Alessandro Verri', 'Lorenzo Rosasco', 'Agnese Seminara']
- **Abstrat**: We consider the problem of olfactory searches in a turbulent environment. We focus on agents that respond solely to odor stimuli, with no access to spatial perception nor prior information about the odor. We ask whether navigation to a target can be learned robustly within a sequential decision making framework. We develop a reinforcement learning algorithm using a small set of interpretable olfactory states and train it with realistic turbulent odor cues. By introducing a temporal memory, we demonstrate that two salient features of odor traces, discretized in few olfactory states, are sufficient to learn navigation in a realistic odor plume. Performance is dictated by the sparse nature of turbulent odors. An optimal memory exists which ignores blanks within the plume and activates a recovery strategy outside the plume. We obtain the best performance by letting agents learn their recovery strategy and show that it is mostly casting cross wind, similar to behavior observed in flying insects. The optimal strategy is robust to substantial changes in the odor plumes, suggesting minor parameter tuning may be sufficient to adapt to different environments.





## Heterogeneity-aware Personalized Federated Learning via Adaptive Dual-Agent Reinforcement Learning
- **Url**: http://arxiv.org/abs/2501.16966v1
- **Authors**: ['Xi Chen', 'Qin Li', 'Haibin Cai', 'Ting Wang']
- **Abstrat**: Federated Learning (FL) empowers multiple clients to collaboratively train machine learning models without sharing local data, making it highly applicable in heterogeneous Internet of Things (IoT) environments. However, intrinsic heterogeneity in clients' model architectures and computing capabilities often results in model accuracy loss and the intractable straggler problem, which significantly impairs training effectiveness. To tackle these challenges, this paper proposes a novel Heterogeneity-aware Personalized Federated Learning method, named HAPFL, via multi-level Reinforcement Learning (RL) mechanisms. HAPFL optimizes the training process by incorporating three strategic components: 1) An RL-based heterogeneous model allocation mechanism. The parameter server employs a Proximal Policy Optimization (PPO)-based RL agent to adaptively allocate appropriately sized, differentiated models to clients based on their performance, effectively mitigating performance disparities. 2) An RL-based training intensity adjustment scheme. The parameter server leverages another PPO-based RL agent to dynamically fine-tune the training intensity for each client to further enhance training efficiency and reduce straggling latency. 3) A knowledge distillation-based mutual learning mechanism. Each client deploys both a heterogeneous local model and a homogeneous lightweight model named LiteModel, where these models undergo mutual learning through knowledge distillation. This uniform LiteModel plays a pivotal role in aggregating and sharing global knowledge, significantly enhancing the effectiveness of personalized local training. Experimental results across multiple benchmark datasets demonstrate that HAPFL not only achieves high accuracy but also substantially reduces the overall training time by 20.9%-40.4% and decreases straggling latency by 19.0%-48.0% compared to existing solutions.





## Upside Down Reinforcement Learning with Policy Generators
- **Url**: http://arxiv.org/abs/2501.16288v2
- **Authors**: ['Jacopo Di Ventura', 'Dylan R. Ashley', 'Vincent Herrmann', 'Francesco Faccio', 'Jürgen Schmidhuber']
- **Abstrat**: Upside Down Reinforcement Learning (UDRL) is a promising framework for solving reinforcement learning problems which focuses on learning command-conditioned policies. In this work, we extend UDRL to the task of learning a command-conditioned generator of deep neural network policies. We accomplish this using Hypernetworks - a variant of Fast Weight Programmers, which learn to decode input commands representing a desired expected return into command-specific weight matrices. Our method, dubbed Upside Down Reinforcement Learning with Policy Generators (UDRLPG), streamlines comparable techniques by removing the need for an evaluator or critic to update the weights of the generator. To counteract the increased variance in last returns caused by not having an evaluator, we decouple the sampling probability of the buffer from the absolute number of policies in it, which, together with a simple weighting strategy, improves the empirical convergence of the algorithm. Compared with existing algorithms, UDRLPG achieves competitive performance and high returns, sometimes outperforming more complex architectures. Our experiments show that a trained generator can generalize to create policies that achieve unseen returns zero-shot. The proposed method appears to be effective in mitigating some of the challenges associated with learning highly multimodal functions. Altogether, we believe that UDRLPG represents a promising step forward in achieving greater empirical sample efficiency in RL. A full implementation of UDRLPG is publicly available at https://github.com/JacopoD/udrlpg_





## On Rollouts in Model-Based Reinforcement Learning
- **Url**: http://arxiv.org/abs/2501.16918v1
- **Authors**: ['Bernd Frauenknecht', 'Devdutt Subhasish', 'Friedrich Solowjow', 'Sebastian Trimpe']
- **Abstrat**: Model-based reinforcement learning (MBRL) seeks to enhance data efficiency by learning a model of the environment and generating synthetic rollouts from it. However, accumulated model errors during these rollouts can distort the data distribution, negatively impacting policy learning and hindering long-term planning. Thus, the accumulation of model errors is a key bottleneck in current MBRL methods. We propose Infoprop, a model-based rollout mechanism that separates aleatoric from epistemic model uncertainty and reduces the influence of the latter on the data distribution. Further, Infoprop keeps track of accumulated model errors along a model rollout and provides termination criteria to limit data corruption. We demonstrate the capabilities of Infoprop in the Infoprop-Dyna algorithm, reporting state-of-the-art performance in Dyna-style MBRL on common MuJoCo benchmark tasks while substantially increasing rollout length and data quality.





## Guiding Generative Protein Language Models with Reinforcement Learning
- **Url**: http://arxiv.org/abs/2412.12979v2
- **Authors**: ['Filippo Stocco', 'Maria Artigues-Lleixa', 'Andrea Hunklinger', 'Talal Widatalla', 'Marc Guell', 'Noelia Ferruz']
- **Abstrat**: Autoregressive protein language models (pLMs) have emerged as powerful tools to efficiently design functional proteins with extraordinary diversity, as evidenced by the successful generation of diverse enzyme families, including lysozymes or carbonic anhydrases. However, a fundamental limitation of pLMs is their propensity to sample from dense regions within the training distribution, which constrains their ability to sample from rare, high-value regions of the sequence space. This limitation becomes particularly critical in applications targeting underrepresented distribution tails, such as engineering for enzymatic activity or binding affinity. To address this challenge, we implement DPO_pLM, a reinforcement learning (RL) framework for protein sequence optimization with pLMs. Drawing inspiration from the success of RL in aligning language models to human preferences, we approach protein optimization as an iterative process that fine-tunes pLM weights to maximize a reward provided by an external oracle. Our strategy demonstrates that RL can efficiently optimize for a variety of custom properties without the need for additional data, achieving significant while preserving sequence diversity. We applied DPO_pLM to the design of EGFR binders, successfully identifying nanomolar binders within hours. Our code is publicly available at https://github.com/AI4PDLab/DPO_pLM.





## CosyAudio: Improving Audio Generation with Confidence Scores and Synthetic Captions
- **Url**: http://arxiv.org/abs/2501.16761v1
- **Authors**: ['Xinfa Zhu', 'Wenjie Tian', 'Xinsheng Wang', 'Lei He', 'Xi Wang', 'Sheng Zhao', 'Lei Xie']
- **Abstrat**: Text-to-Audio (TTA) generation is an emerging area within AI-generated content (AIGC), where audio is created from natural language descriptions. Despite growing interest, developing robust TTA models remains challenging due to the scarcity of well-labeled datasets and the prevalence of noisy or inaccurate captions in large-scale, weakly labeled corpora. To address these challenges, we propose CosyAudio, a novel framework that utilizes confidence scores and synthetic captions to enhance the quality of audio generation. CosyAudio consists of two core components: AudioCapTeller and an audio generator. AudioCapTeller generates synthetic captions for audio and provides confidence scores to evaluate their accuracy. The audio generator uses these synthetic captions and confidence scores to enable quality-aware audio generation. Additionally, we introduce a self-evolving training strategy that iteratively optimizes CosyAudio across both well-labeled and weakly-labeled datasets. Initially trained with well-labeled data, AudioCapTeller leverages its assessment capabilities on weakly-labeled datasets for high-quality filtering and reinforcement learning, which further improves its performance. The well-trained AudioCapTeller refines corpora by generating new captions and confidence scores, serving for the audio generator training. Extensive experiments on open-source datasets demonstrate that CosyAudio outperforms existing models in automated audio captioning, generates more faithful audio, and exhibits strong generalization across diverse scenarios.





# TD3
# Prioritized Experience Replay
# path planning
## Decictor: Towards Evaluating the Robustness of Decision-Making in Autonomous Driving Systems
- **Url**: http://arxiv.org/abs/2402.18393v3
- **Authors**: ['Mingfei Cheng', 'Yuan Zhou', 'Xiaofei Xie', 'Junjie Wang', 'Guozhu Meng', 'Kairui Yang']
- **Abstrat**: Autonomous Driving System (ADS) testing is crucial in ADS development, with the current primary focus being on safety. However, the evaluation of non-safety-critical performance, particularly the ADS's ability to make optimal decisions and produce optimal paths for autonomous vehicles (AVs), is also vital to ensure the intelligence and reduce risks of AVs. Currently, there is little work dedicated to assessing the robustness of ADSs' path-planning decisions (PPDs), i.e., whether an ADS can maintain the optimal PPD after an insignificant change in the environment. The key challenges include the lack of clear oracles for assessing PPD optimality and the difficulty in searching for scenarios that lead to non-optimal PPDs. To fill this gap, in this paper, we focus on evaluating the robustness of ADSs' PPDs and propose the first method, Decictor, for generating non-optimal decision scenarios (NoDSs), where the ADS does not plan optimal paths for AVs. Decictor comprises three main components: Non-invasive Mutation, Consistency Check, and Feedback. To overcome the oracle challenge, Non-invasive Mutation is devised to implement conservative modifications, ensuring the preservation of the original optimal path in the mutated scenarios. Subsequently, the Consistency Check is applied to determine the presence of non-optimal PPDs by comparing the driving paths in the original and mutated scenarios. To deal with the challenge of large environment space, we design Feedback metrics that integrate spatial and temporal dimensions of the AV's movement. These metrics are crucial for effectively steering the generation of NoDSs. We evaluate Decictor on Baidu Apollo, an open-source and production-grade ADS. The experimental results validate the effectiveness of Decictor in detecting non-optimal PPDs of ADSs.





## Front Hair Styling Robot System Using Path Planning for Root-Centric Strand Adjustment
- **Url**: http://arxiv.org/abs/2501.10991v2
- **Authors**: ['Soonhyo Kim', 'Naoaki Kanazawa', 'Shun Hasegawa', 'Kento Kawaharazuka', 'Kei Okada']
- **Abstrat**: Hair styling is a crucial aspect of personal grooming, significantly influenced by the appearance of front hair. While brushing is commonly used both to detangle hair and for styling purposes, existing research primarily focuses on robotic systems for detangling hair, with limited exploration into robotic hair styling. This research presents a novel robotic system designed to automatically adjust front hairstyles, with an emphasis on path planning for root-centric strand adjustment. The system utilizes images to compare the current hair state with the desired target state through an orientation map of hair strands. By concentrating on the differences in hair orientation and specifically targeting adjustments at the root of each strand, the system performs detailed styling tasks. The path planning approach ensures effective alignment of the hairstyle with the target, and a closed-loop mechanism refines these adjustments to accurately evolve the hairstyle towards the desired outcome. Experimental results demonstrate that the proposed system achieves a high degree of similarity and consistency in front hair styling, showing promising results for automated, precise hairstyle adjustments.




