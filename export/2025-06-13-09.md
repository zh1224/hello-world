# reinforcement learning
## Eye, Robot: Learning to Look to Act with a BC-RL Perception-Action Loop
- **Url**: http://arxiv.org/abs/2506.10968v1
- **Authors**: ['Justin Kerr', 'Kush Hari', 'Ethan Weber', 'Chung Min Kim', 'Brent Yi', 'Tyler Bonnen', 'Ken Goldberg', 'Angjoo Kanazawa']
- **Abstrat**: Humans do not passively observe the visual world -- we actively look in order to act. Motivated by this principle, we introduce EyeRobot, a robotic system with gaze behavior that emerges from the need to complete real-world tasks. We develop a mechanical eyeball that can freely rotate to observe its surroundings and train a gaze policy to control it using reinforcement learning. We accomplish this by first collecting teleoperated demonstrations paired with a 360 camera. This data is imported into a simulation environment that supports rendering arbitrary eyeball viewpoints, allowing episode rollouts of eye gaze on top of robot demonstrations. We then introduce a BC-RL loop to train the hand and eye jointly: the hand (BC) agent is trained from rendered eye observations, and the eye (RL) agent is rewarded when the hand produces correct action predictions. In this way, hand-eye coordination emerges as the eye looks towards regions which allow the hand to complete the task. EyeRobot implements a foveal-inspired policy architecture allowing high resolution with a small compute budget, which we find also leads to the emergence of more stable fixation as well as improved ability to track objects and ignore distractors. We evaluate EyeRobot on five panoramic workspace manipulation tasks requiring manipulation in an arc surrounding the robot arm. Our experiments suggest EyeRobot exhibits hand-eye coordination behaviors which effectively facilitate manipulation over large workspaces with a single camera. See project site for videos: https://www.eyerobot.net/





## AssistanceZero: Scalably Solving Assistance Games
- **Url**: http://arxiv.org/abs/2504.07091v2
- **Authors**: ['Cassidy Laidlaw', 'Eli Bronstein', 'Timothy Guo', 'Dylan Feng', 'Lukas Berglund', 'Justin Svegliato', 'Stuart Russell', 'Anca Dragan']
- **Abstrat**: Assistance games are a promising alternative to reinforcement learning from human feedback (RLHF) for training AI assistants. Assistance games resolve key drawbacks of RLHF, such as incentives for deceptive behavior, by explicitly modeling the interaction between assistant and user as a two-player game where the assistant cannot observe their shared goal. Despite their potential, assistance games have only been explored in simple settings. Scaling them to more complex environments is difficult because it requires both solving intractable decision-making problems under uncertainty and accurately modeling human users' behavior. We present the first scalable approach to solving assistance games and apply it to a new, challenging Minecraft-based assistance game with over $10^{400}$ possible goals. Our approach, AssistanceZero, extends AlphaZero with a neural network that predicts human actions and rewards, enabling it to plan under uncertainty. We show that AssistanceZero outperforms model-free RL algorithms and imitation learning in the Minecraft-based assistance game. In a human study, our AssistanceZero-trained assistant significantly reduces the number of actions participants take to complete building tasks in Minecraft. Our results suggest that assistance games are a tractable framework for training effective AI assistants in complex environments. Our code and models are available at https://github.com/cassidylaidlaw/minecraft-building-assistance-game.





## Spurious Rewards: Rethinking Training Signals in RLVR
- **Url**: http://arxiv.org/abs/2506.10947v1
- **Authors**: ['Rulin Shao', 'Shuyue Stella Li', 'Rui Xin', 'Scott Geng', 'Yiping Wang', 'Sewoong Oh', 'Simon Shaolei Du', 'Nathan Lambert', 'Sewon Min', 'Ranjay Krishna', 'Yulia Tsvetkov', 'Hannaneh Hajishirzi', 'Pang Wei Koh', 'Luke Zettlemoyer']
- **Abstrat**: We show that reinforcement learning with verifiable rewards (RLVR) can elicit strong mathematical reasoning in certain models even with spurious rewards that have little, no, or even negative correlation with the correct answer. For example, RLVR improves MATH-500 performance for Qwen2.5-Math-7B in absolute points by 21.4% (random reward), 13.8% (format reward), 24.1% (incorrect label), 26.0% (1-shot RL), and 27.1% (majority voting) -- nearly matching the 29.1% gained with ground truth rewards. However, the spurious rewards that work for Qwen often fail to yield gains with other model families like Llama3 or OLMo2. In particular, we find code reasoning -- thinking in code without actual code execution -- to be a distinctive Qwen2.5-Math behavior that becomes significantly more frequent after RLVR, from 65% to over 90%, even with spurious rewards. Overall, we hypothesize that, given the lack of useful reward signal, RLVR must somehow be surfacing useful reasoning representations learned during pretraining, although the exact mechanism remains a topic for future work. We suggest that future RLVR research should possibly be validated on diverse models rather than a single de facto choice, as we show that it is easy to get significant performance gains on Qwen models even with completely spurious reward signals.





## Self-Adapting Language Models
- **Url**: http://arxiv.org/abs/2506.10943v1
- **Authors**: ['Adam Zweiger', 'Jyothish Pari', 'Han Guo', 'Ekin Akyürek', 'Yoon Kim', 'Pulkit Agrawal']
- **Abstrat**: Large language models (LLMs) are powerful but static; they lack mechanisms to adapt their weights in response to new tasks, knowledge, or examples. We introduce Self-Adapting LLMs (SEAL), a framework that enables LLMs to self-adapt by generating their own finetuning data and update directives. Given a new input, the model produces a self-edit-a generation that may restructure the information in different ways, specify optimization hyperparameters, or invoke tools for data augmentation and gradient-based updates. Through supervised finetuning (SFT), these self-edits result in persistent weight updates, enabling lasting adaptation. To train the model to produce effective self-edits, we use a reinforcement learning loop with the downstream performance of the updated model as the reward signal. Unlike prior approaches that rely on separate adaptation modules or auxiliary networks, SEAL directly uses the model's own generation to control its adaptation process. Experiments on knowledge incorporation and few-shot generalization show that SEAL is a promising step toward language models capable of self-directed adaptation. Our website and code is available at https://jyopari.github.io/posts/seal.





## Passivity-Centric Safe Reinforcement Learning for Contact-Rich Robotic Tasks
- **Url**: http://arxiv.org/abs/2503.00287v2
- **Authors**: ['Heng Zhang', 'Gokhan Solak', 'Sebastian Hjorth', 'Arash Ajoudani']
- **Abstrat**: Reinforcement learning (RL) has achieved remarkable success in various robotic tasks; however, its deployment in real-world scenarios, particularly in contact-rich environments, often overlooks critical safety and stability aspects. Policies without passivity guarantees can result in system instability, posing risks to robots, their environments, and human operators. In this work, we investigate the limitations of traditional RL policies when deployed in contact-rich tasks and explore the combination of energy-based passive control with safe RL in both training and deployment to answer these challenges. Firstly, we reveal the discovery that standard RL policy does not satisfy stability in contact-rich scenarios. Secondly, we introduce a \textit{passivity-aware} RL policy training with energy-based constraints in our safe RL formulation. Lastly, a passivity filter is exerted on the policy output for \textit{passivity-ensured} control during deployment. We conduct comparative studies on a contact-rich robotic maze exploration task, evaluating the effects of learning passivity-aware policies and the importance of passivity-ensured control. The experiments demonstrate that a passivity-agnostic RL policy easily violates energy constraints in deployment, even though it achieves high task completion in training. The results show that our proposed approach guarantees control stability through passivity filtering and improves the energy efficiency through passivity-aware training. A video of real-world experiments is available as supplementary material. We also release the checkpoint model and offline data for pre-training at \href{https://huggingface.co/Anonymous998/passiveRL/tree/main}{Hugging Face}.





## Magistral
- **Url**: http://arxiv.org/abs/2506.10910v1
- **Authors**: ['Mistral-AI', ':', 'Abhinav Rastogi', 'Albert Q. Jiang', 'Andy Lo', 'Gabrielle Berrada', 'Guillaume Lample', 'Jason Rute', 'Joep Barmentlo', 'Karmesh Yadav', 'Kartik Khandelwal', 'Khyathi Raghavi Chandu', 'Léonard Blier', 'Lucile Saulnier', 'Matthieu Dinot', 'Maxime Darrin', 'Neha Gupta', 'Roman Soletskyi', 'Sagar Vaze', 'Teven Le Scao', 'Yihan Wang', 'Adam Yang', 'Alexander H. Liu', 'Alexandre Sablayrolles', 'Amélie Héliou', 'Amélie Martin', 'Andy Ehrenberg', 'Anmol Agarwal', 'Antoine Roux', 'Arthur Darcet', 'Arthur Mensch', 'Baptiste Bout', 'Baptiste Rozière', 'Baudouin De Monicault', 'Chris Bamford', 'Christian Wallenwein', 'Christophe Renaudin', 'Clémence Lanfranchi', 'Darius Dabert', 'Devon Mizelle', 'Diego de las Casas', 'Elliot Chane-Sane', 'Emilien Fugier', 'Emma Bou Hanna', 'Gauthier Delerce', 'Gauthier Guinet', 'Georgii Novikov', 'Guillaume Martin', 'Himanshu Jaju', 'Jan Ludziejewski', 'Jean-Hadrien Chabran', 'Jean-Malo Delignon', 'Joachim Studnia', 'Jonas Amar', 'Josselin Somerville Roberts', 'Julien Denize', 'Karan Saxena', 'Kush Jain', 'Lingxiao Zhao', 'Louis Martin', 'Luyu Gao', 'Lélio Renard Lavaud', 'Marie Pellat', 'Mathilde Guillaumin', 'Mathis Felardos', 'Maximilian Augustin', 'Mickaël Seznec', 'Nikhil Raghuraman', 'Olivier Duchenne', 'Patricia Wang', 'Patrick von Platen', 'Patryk Saffer', 'Paul Jacob', 'Paul Wambergue', 'Paula Kurylowicz', 'Pavankumar Reddy Muddireddy', 'Philomène Chagniot', 'Pierre Stock', 'Pravesh Agrawal', 'Romain Sauvestre', 'Rémi Delacourt', 'Sanchit Gandhi', 'Sandeep Subramanian', 'Shashwat Dalal', 'Siddharth Gandhi', 'Soham Ghosh', 'Srijan Mishra', 'Sumukh Aithal', 'Szymon Antoniak', 'Thibault Schueller', 'Thibaut Lavril', 'Thomas Robert', 'Thomas Wang', 'Timothée Lacroix', 'Valeriia Nemychnikova', 'Victor Paltz', 'Virgile Richard', 'Wen-Ding Li', 'William Marshall', 'Xuanyu Zhang', 'Yunhao Tang']
- **Abstrat**: We introduce Magistral, Mistral's first reasoning model and our own scalable reinforcement learning (RL) pipeline. Instead of relying on existing implementations and RL traces distilled from prior models, we follow a ground up approach, relying solely on our own models and infrastructure. Notably, we demonstrate a stack that enabled us to explore the limits of pure RL training of LLMs, present a simple method to force the reasoning language of the model, and show that RL on text data alone maintains most of the initial checkpoint's capabilities. We find that RL on text maintains or improves multimodal understanding, instruction following and function calling. We present Magistral Medium, trained for reasoning on top of Mistral Medium 3 with RL alone, and we open-source Magistral Small (Apache 2.0) which further includes cold-start data from Magistral Medium.





## Adaptive Job Scheduling in Quantum Clouds Using Reinforcement Learning
- **Url**: http://arxiv.org/abs/2506.10889v1
- **Authors**: ['Waylon Luo', 'Jiapeng Zhao', 'Tong Zhan', 'Qiang Guan']
- **Abstrat**: Present-day quantum systems face critical bottlenecks, including limited qubit counts, brief coherence intervals, and high susceptibility to errors-all of which obstruct the execution of large and complex circuits. The advancement of quantum algorithms has outpaced the capabilities of existing quantum hardware, making it difficult to scale computations effectively. Additionally, inconsistencies in hardware performance and pervasive quantum noise undermine system stability and computational accuracy. To optimize quantum workloads under these constraints, strategic approaches to task scheduling and resource coordination are essential. These methods must aim to accelerate processing, retain operational fidelity, and reduce the communication burden inherent to distributed setups. One of the persistent challenges in this domain is how to efficiently divide and execute large circuits across multiple quantum processors (QPUs), especially in error-prone environments. In response, we introduce a simulation-based tool that supports distributed scheduling and concurrent execution of quantum jobs on networked QPUs connected via real-time classical channels. The tool models circuit decomposition for workloads that surpass individual QPU limits, allowing for parallel execution through inter-processor communication. Using this simulation environment, we compare four distinct scheduling techniques-among them, a model informed by reinforcement learning. These strategies are evaluated across multiple metrics, including runtime efficiency, fidelity preservation, and communication costs. Our analysis underscores the trade-offs inherent in each approach and highlights how parallelized, noise-aware scheduling can meaningfully improve computational throughput in distributed quantum infrastructures.





## Sailing by the Stars: A Survey on Reward Models and Learning Strategies for Learning from Rewards
- **Url**: http://arxiv.org/abs/2505.02686v2
- **Authors**: ['Xiaobao Wu']
- **Abstrat**: Recent developments in Large Language Models (LLMs) have shifted from pre-training scaling to post-training and test-time scaling. Across these developments, a key unified paradigm has arisen: Learning from Rewards, where reward signals act as the guiding stars to steer LLM behavior. It has underpinned a wide range of prevalent techniques, such as reinforcement learning (RLHF, RLAIF, DPO, and GRPO), reward-guided decoding, and post-hoc correction. Crucially, this paradigm enables the transition from passive learning from static data to active learning from dynamic feedback. This endows LLMs with aligned preferences and deep reasoning capabilities for diverse tasks. In this survey, we present a comprehensive overview of learning from rewards, from the perspective of reward models and learning strategies across training, inference, and post-inference stages. We further discuss the benchmarks for reward models and the primary applications. Finally we highlight the challenges and future directions. We maintain a paper collection at https://github.com/bobxwu/learning-from-rewards-llm-papers.





## Q-Ponder: A Unified Training Pipeline for Reasoning-based Visual Quality Assessment
- **Url**: http://arxiv.org/abs/2506.05384v2
- **Authors**: ['Zhuoxuan Cai', 'Jian Zhang', 'Xinbin Yuan', 'Peng-Tao Jiang', 'Wenxiang Chen', 'Bowen Tang', 'Lujian Yao', 'Qiyuan Wang', 'Jinwen Chen', 'Bo Li']
- **Abstrat**: Recent studies demonstrate that multimodal large language models (MLLMs) can proficiently evaluate visual quality through interpretable assessments. However, existing approaches typically treat quality scoring and reasoning descriptions as separate tasks with disjoint optimization objectives, leading to a trade-off: models adept at quality reasoning descriptions struggle with precise score regression, while score-focused models lack interpretability. This limitation hinders the full potential of MLLMs in visual quality assessment, where accuracy and interpretability should be mutually reinforcing. To address this, we propose a unified two-stage training framework comprising a cold-start stage and a reinforcement learning-based fine-tuning stage. Specifically, in the first stage, we distill high-quality data from a teacher model through expert-designed prompts, initializing reasoning capabilities via cross-entropy loss supervision. In the second stage, we introduce a novel reward with Group Relative Policy Optimization (GRPO) to jointly optimize scoring accuracy and reasoning consistency. We designate the models derived from these two stages as Q-Ponder-CI and Q-Ponder. Extensive experiments show that Q-Ponder achieves state-of-the-art (SOTA) performance on quality score regression benchmarks, delivering up to 6.5% higher SRCC on cross-domain datasets. Furthermore, Q-Ponder significantly outperforms description-based SOTA models, including its teacher model Qwen-2.5-VL-72B, particularly in description accuracy and reasonableness, demonstrating the generalization potential over diverse tasks.





## Viability of Future Actions: Robust Safety in Reinforcement Learning via Entropy Regularization
- **Url**: http://arxiv.org/abs/2506.10871v1
- **Authors**: ['Pierre-François Massiani', 'Alexander von Rohr', 'Lukas Haverbeck', 'Sebastian Trimpe']
- **Abstrat**: Despite the many recent advances in reinforcement learning (RL), the question of learning policies that robustly satisfy state constraints under unknown disturbances remains open. In this paper, we offer a new perspective on achieving robust safety by analyzing the interplay between two well-established techniques in model-free RL: entropy regularization, and constraints penalization. We reveal empirically that entropy regularization in constrained RL inherently biases learning toward maximizing the number of future viable actions, thereby promoting constraints satisfaction robust to action noise. Furthermore, we show that by relaxing strict safety constraints through penalties, the constrained RL problem can be approximated arbitrarily closely by an unconstrained one and thus solved using standard model-free RL. This reformulation preserves both safety and optimality while empirically improving resilience to disturbances. Our results indicate that the connection between entropy regularization and robustness is a promising avenue for further empirical and theoretical investigation, as it enables robust safety in RL through simple reward shaping.





## Joint Beamforming with Extremely Large Scale RIS: A Sequential Multi-Agent A2C Approach
- **Url**: http://arxiv.org/abs/2506.10815v1
- **Authors**: ['Zhi Chai', 'Jiajie Xu', 'Justin P Coon', 'Mohamed-Slim Alouini']
- **Abstrat**: It is a challenging problem to jointly optimize the base station (BS) precoding matrix and the reconfigurable intelligent surface (RIS) phases simultaneously in a RIS-assisted multiple-user multiple-input-multiple-output (MU-MIMO) scenario when the size of the RIS becomes extremely large. In this paper, we propose a deep reinforcement learning algorithm called sequential multi-agent advantage actor-critic (A2C) to solve this problem. In addition, the discrete phase of RISs, imperfect channel state information (CSI), and channel correlations between users are taken into consideration. The computational complexity is also analyzed, and the performance of the proposed algorithm is compared with the zero-forcing (ZF) beamformer in terms of the sum spectral efficiency (SE). It is noted that the computational complexity of the proposed algorithm is lower than the benchmark, while the performance is better than the benchmark. Throughout simulations, it is also found that the proposed algorithm is robust to medium channel estimation error.





## Human-Robot Navigation using Event-based Cameras and Reinforcement Learning
- **Url**: http://arxiv.org/abs/2506.10790v1
- **Authors**: ['Ignacio Bugueno-Cordova', 'Javier Ruiz-del-Solar', 'Rodrigo Verschae']
- **Abstrat**: This work introduces a robot navigation controller that combines event cameras and other sensors with reinforcement learning to enable real-time human-centered navigation and obstacle avoidance. Unlike conventional image-based controllers, which operate at fixed rates and suffer from motion blur and latency, this approach leverages the asynchronous nature of event cameras to process visual information over flexible time intervals, enabling adaptive inference and control. The framework integrates event-based perception, additional range sensing, and policy optimization via Deep Deterministic Policy Gradient, with an initial imitation learning phase to improve sample efficiency. Promising results are achieved in simulated environments, demonstrating robust navigation, pedestrian following, and obstacle avoidance. A demo video is available at the project website.





## Divide-Fuse-Conquer: Eliciting "Aha Moments" in Multi-Scenario Games
- **Url**: http://arxiv.org/abs/2505.16401v4
- **Authors**: ['Xiaoqing Zhang', 'Huabin Zheng', 'Ang Lv', 'Yuhan Liu', 'Zirui Song', 'Xiuying Chen', 'Rui Yan', 'Flood Sung']
- **Abstrat**: Large language models (LLMs) have been observed to suddenly exhibit advanced reasoning abilities during reinforcement learning (RL), resembling an ``aha moment'' triggered by simple outcome-based rewards. While RL has proven effective in eliciting such breakthroughs in tasks involving mathematics, coding, and vision, it faces significant challenges in multi-scenario games. The diversity of game rules, interaction modes, and environmental complexities often leads to policies that perform well in one scenario but fail to generalize to others. Simply combining multiple scenarios during training introduces additional challenges, such as training instability and poor performance. To overcome these challenges, we propose Divide-Fuse-Conquer, a framework designed to enhance generalization in multi-scenario RL. This approach starts by heuristically grouping games based on characteristics such as rules and difficulties. Specialized models are then trained for each group to excel at games in the group is what we refer to as the divide step. Next, we fuse model parameters from different groups as a new model, and continue training it for multiple groups, until the scenarios in all groups are conquered. Experiments across 18 TextArena games show that Qwen2.5-32B-Align trained with the Divide-Fuse-Conquer strategy reaches a performance level comparable to Claude3.5, achieving 7 wins and 4 draws. We hope our approach can inspire future research on using reinforcement learning to improve the generalization of LLMs.





## Mimicking Human Intuition: Cognitive Belief-Driven Reinforcement Learning
- **Url**: http://arxiv.org/abs/2410.01739v3
- **Authors**: ['Xingrui Gu', 'Guanren Qiao', 'Chuyi Jiang']
- **Abstrat**: Traditional reinforcement learning (RL) methods mainly rely on trial-and-error exploration, often lacking mechanisms to guide agents toward more informative decision-making and struggling to leverage past experiences, resulting in low sample efficiency. To overcome this issue, we propose an innovative framework inspired by cognitive principles: Cognitive Belief-Driven Reinforcement Learning (CBD-RL). By incorporating cognitive heuristics, CBD-RL transforms conventional trial-and-error learning into a more structured and guided learning paradigm, simulating the human reasoning process. This framework's core is a belief system that optimizes action probabilities by integrating feedback with prior experience, thus enhancing decision making under uncertainty. It also organizes state-action pairs into meaningful categories, promoting generalization and improving sample efficiency. The concrete implementations of this framework, CBDQ, CBDPPO, and CBDSAC, demonstrate superior performance in discrete and continuous action spaces in diverse environments such as Atari and MuJoCo. By bridging cognitive science and reinforcement learning, this research opens a new avenue for developing RL systems that are more interpretable, efficient, and cognitively inspired.





## PosterCraft: Rethinking High-Quality Aesthetic Poster Generation in a Unified Framework
- **Url**: http://arxiv.org/abs/2506.10741v1
- **Authors**: ['SiXiang Chen', 'Jianyu Lai', 'Jialin Gao', 'Tian Ye', 'Haoyu Chen', 'Hengyu Shi', 'Shitong Shao', 'Yunlong Lin', 'Song Fei', 'Zhaohu Xing', 'Yeying Jin', 'Junfeng Luo', 'Xiaoming Wei', 'Lei Zhu']
- **Abstrat**: Generating aesthetic posters is more challenging than simple design images: it requires not only precise text rendering but also the seamless integration of abstract artistic content, striking layouts, and overall stylistic harmony. To address this, we propose PosterCraft, a unified framework that abandons prior modular pipelines and rigid, predefined layouts, allowing the model to freely explore coherent, visually compelling compositions. PosterCraft employs a carefully designed, cascaded workflow to optimize the generation of high-aesthetic posters: (i) large-scale text-rendering optimization on our newly introduced Text-Render-2M dataset; (ii) region-aware supervised fine-tuning on HQ-Poster100K; (iii) aesthetic-text-reinforcement learning via best-of-n preference optimization; and (iv) joint vision-language feedback refinement. Each stage is supported by a fully automated data-construction pipeline tailored to its specific needs, enabling robust training without complex architectural modifications. Evaluated on multiple experiments, PosterCraft significantly outperforms open-source baselines in rendering accuracy, layout coherence, and overall visual appeal-approaching the quality of SOTA commercial systems. Our code, models, and datasets can be found in the Project page: https://ephemeral182.github.io/PosterCraft





## APEX: Action Priors Enable Efficient Exploration for Skill Imitation on Articulated Robots
- **Url**: http://arxiv.org/abs/2505.10022v2
- **Authors**: ['Shivam Sood', 'Laukik B Nakhwa', 'Yuhong Cao', 'Sun Ge', 'Guillaume Sartoretti']
- **Abstrat**: Learning by imitation provides an effective way for robots to develop well-regulated complex behaviors and directly benefit from natural demonstrations. State-of-the-art imitation learning (IL) approaches typically leverage Adversarial Motion Priors (AMP), which, despite their impressive results, suffer from two key limitations. They are prone to mode collapse, which often leads to overfitting to the simulation environment and thus increased sim-to-real gap, and they struggle to learn diverse behaviors effectively. To overcome these limitations, we introduce APEX (Action Priors enable Efficient eXploration): a simple yet versatile IL framework that integrates demonstrations directly into reinforcement learning (RL), maintaining high exploration while grounding behavior with expert-informed priors. We achieve this through a combination of decaying action priors, which initially bias exploration toward expert demonstrations but gradually allow the policy to explore independently. This is complemented by a multi-critic RL framework that effectively balances stylistic consistency with task performance. Our approach achieves sample-efficient IL and enables the acquisition of diverse skills within a single policy. APEX generalizes to varying velocities and preserves reference-like styles across complex tasks such as navigating rough terrain and climbing stairs, utilizing only flat-terrain kinematic motion data as a prior. We validate our framework through extensive hardware experiments on the Unitree Go2 quadruped. There, APEX yields diverse and agile locomotion gaits, inherent gait transitions, and the highest reported speed for the platform to the best of our knowledge (peak velocity of ~3.3 m/s on hardware). Our results establish APEX as a compelling alternative to existing IL methods, offering better efficiency, adaptability, and real-world performance. https://marmotlab.github.io/APEX/





## Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models
- **Url**: http://arxiv.org/abs/2506.01413v2
- **Authors**: ['Yulei Qin', 'Gang Li', 'Zongyi Li', 'Zihan Xu', 'Yuchen Shi', 'Zhekai Lin', 'Xiao Cui', 'Ke Li', 'Xing Sun']
- **Abstrat**: Existing large language models (LLMs) face challenges of following complex instructions, especially when multiple constraints are present and organized in paralleling, chaining, and branching structures. One intuitive solution, namely chain-of-thought (CoT), is expected to universally improve capabilities of LLMs. However, we find that the vanilla CoT exerts a negative impact on performance due to its superficial reasoning pattern of simply paraphrasing the instructions. It fails to peel back the compositions of constraints for identifying their relationship across hierarchies of types and dimensions. To this end, we propose a systematic method to boost LLMs in dealing with complex instructions via incentivizing reasoning for test-time compute scaling. First, we stem from the decomposition of complex instructions under existing taxonomies and propose a reproducible data acquisition method. Second, we exploit reinforcement learning (RL) with verifiable rule-centric reward signals to cultivate reasoning specifically for instruction following. We address the shallow, non-essential nature of reasoning under complex instructions via sample-wise contrast for superior CoT enforcement. We also exploit behavior cloning of experts to facilitate steady distribution shift from fast-thinking LLMs to skillful reasoners. Extensive evaluations on seven comprehensive benchmarks confirm the validity of the proposed method, where a 1.5B LLM achieves 11.74% gains with performance comparable to a 8B LLM. Codes and data are available at https://github.com/yuleiqin/RAIF.





## On Broken Symmetry in Cognition
- **Url**: http://arxiv.org/abs/2303.06047v2
- **Authors**: ['Xin Li']
- **Abstrat**: Cognition is not passive data accumulation but the active resolution of uncertainty through symmetry breaking. This paper argues that both cognitive evolution and development unfold via sequential symmetry-breaking transitions that disrupt innate regularities across space, time, self, and representation. First, spatial symmetry is broken through bilateral body plans and neural codes like grid and place cells, which privilege egocentric orientation and localized encoding. Second, reinforcement learning introduces temporal asymmetry by favoring future rewards, establishing a directional flow of inference. Third, goal-directed simulation breaks spatiotemporal symmetry between internal self-models and the external world, enabling embodied inference and solving the combinatorial search problem. Fourth, social cognition via mentalizing and imitation breaks the symmetry between minds, allowing agents to infer others' beliefs. Finally, language imposes a linear, recursive structure onto unordered thought, breaking expressive symmetry through syntax and grammar. These asymmetries are unified by the Context-Content Uncertainty Principle (CCUP), which frames cognition as a cyclical entropy-minimizing process. At the core lies the principle of structure-before-specificity: ambiguous input is first mapped onto stable latent structures before being bound to specific instances. This promotes generalization, reduces sample complexity, and prevents overfitting. Inverting inference, from content back to context, further breaks the curse of dimensionality by constraining inference to goal-consistent manifolds. Thus, symmetry breaking is not incidental but the foundational mechanism by which cognition organizes, stabilizes, and scales intelligent behavior in an uncertain and dynamic world.





## SR-Reward: Taking The Path More Traveled
- **Url**: http://arxiv.org/abs/2501.02330v3
- **Authors**: ['Seyed Mahdi B. Azad', 'Zahra Padar', 'Gabriel Kalweit', 'Joschka Boedecker']
- **Abstrat**: In this paper, we propose a novel method for learning reward functions directly from offline demonstrations. Unlike traditional inverse reinforcement learning (IRL), our approach decouples the reward function from the learner's policy, eliminating the adversarial interaction typically required between the two. This results in a more stable and efficient training process. Our reward function, called \textit{SR-Reward}, leverages successor representation (SR) to encode a state based on expected future states' visitation under the demonstration policy and transition dynamics. By utilizing the Bellman equation, SR-Reward can be learned concurrently with most reinforcement learning (RL) algorithms without altering the existing training pipeline. We also introduce a negative sampling strategy to mitigate overestimation errors by reducing rewards for out-of-distribution data, thereby enhancing robustness. This strategy inherently introduces a conservative bias into RL algorithms that employ the learned reward. We evaluate our method on the D4RL benchmark, achieving competitive results compared to offline RL algorithms with access to true rewards and imitation learning (IL) techniques like behavioral cloning. Moreover, our ablation studies on data size and quality reveal the advantages and limitations of SR-Reward as a proxy for true rewards.





## Beamforming and Resource Allocation for Delay Optimization in RIS-Assisted OFDM Systems
- **Url**: http://arxiv.org/abs/2506.03586v3
- **Authors**: ['Yu Ma', 'Xiao Li', 'Chongtao Guo', 'Le Liang', 'Shi Jin']
- **Abstrat**: This paper investigates a joint phase design and resource allocation problem in downlink reconfigurable intelligent surface (RIS)-assisted orthogonal frequency division multiplexing (OFDM) systems to optimize average delay, where data packets for each user arrive at the base station stochastically. The sequential optimization problem is inherently a Markov decision process (MDP), making it fall within the scope of reinforcement learning. To effectively handle the mixed action space and reduce the state space dimensionality, a hybrid deep reinforcement learning (DRL) approach is proposed. Specifically, proximal policy optimization (PPO)-$\Theta$ is employed to optimize RIS phase shift design, while PPO-N is responsible for subcarrier allocation decisions. To further mitigate the curse of dimensionality associated with subcarrier allocation, a multi-agent strategy is introduced to optimize subcarrier allocation indicater more efficiently. Moreover, to achieve more adaptive resource allocation and accurately capture network dynamics, key factors closely related to average delay, including the number of backlogged packets in buffers and the current packet arrivals, are incorporated into the state space. Furthermore, a transfer learning framework is introduced to enhance training efficiency and accelerate convergence. Simulation results demonstrate that the proposed algorithm significantly reduces average delay, enhances resource allocation efficiency, and achieves superior system robustness and fairness compared to baseline methods.





# TD3
# Prioritized Experience Replay
# path planning