# reinforcement learning
## A Comprehensive Overview and Comparative Analysis on Deep Learning Models: CNN, RNN, LSTM, GRU
- **Url**: http://arxiv.org/abs/2305.17473v3
- **Authors**: ['Farhad Mortezapour Shiri', 'Thinagaran Perumal', 'Norwati Mustapha', 'Raihani Mohamed']
- **Abstrat**: Deep learning (DL) has emerged as a powerful subset of machine learning (ML) and artificial intelligence (AI), outperforming traditional ML methods, especially in handling unstructured and large datasets. Its impact spans across various domains, including speech recognition, healthcare, autonomous vehicles, cybersecurity, predictive analytics, and more. However, the complexity and dynamic nature of real-world problems present challenges in designing effective deep learning models. Consequently, several deep learning models have been developed to address different problems and applications. In this article, we conduct a comprehensive survey of various deep learning models, including Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Generative Models, Deep Reinforcement Learning (DRL), and Deep Transfer Learning. We examine the structure, applications, benefits, and limitations of each model. Furthermore, we perform an analysis using three publicly available datasets: IMDB, ARAS, and Fruit-360. We compare the performance of six renowned deep learning models: CNN, Simple RNN, Long Short-Term Memory (LSTM), Bidirectional LSTM, Gated Recurrent Unit (GRU), and Bidirectional GRU.





## Algorithmic collusion in a two-sided market: A rideshare example
- **Url**: http://arxiv.org/abs/2405.02835v2
- **Authors**: ['Pravesh Koirala', 'Forrest Laine']
- **Abstrat**: With dynamic pricing on the rise, firms are using sophisticated algorithms for price determination. These algorithms are often non-interpretable and there has been a recent interest in their seemingly emergent ability to tacitly collude with each other without any prior communication whatsoever. Most of the previous works investigate algorithmic collusion on simple reinforcement learning (RL) based algorithms operating on a basic market model. Instead, we explore the collusive tendencies of Proximal Policy Optimization (PPO), a state-of-the-art continuous state/action space RL algorithm, on a complex double-sided hierarchical market model of rideshare. For this purpose, we extend a mathematical program network (MPN) based rideshare model to a temporal multi origin-destination setting and use PPO to solve for a repeated duopoly game. Our results indicate that PPO can either converge to a competitive or a collusive equilibrium depending upon the underlying market characteristics, even when the hyper-parameters are held constant.





## Iterated $Q$-Network: Beyond One-Step Bellman Updates in Deep Reinforcement Learning
- **Url**: http://arxiv.org/abs/2403.02107v3
- **Authors**: ['Th√©o Vincent', 'Daniel Palenicek', 'Boris Belousov', 'Jan Peters', "Carlo D'Eramo"]
- **Abstrat**: The vast majority of Reinforcement Learning methods is largely impacted by the computation effort and data requirements needed to obtain effective estimates of action-value functions, which in turn determine the quality of the overall performance and the sample-efficiency of the learning procedure. Typically, action-value functions are estimated through an iterative scheme that alternates the application of an empirical approximation of the Bellman operator and a subsequent projection step onto a considered function space. It has been observed that this scheme can be potentially generalized to carry out multiple iterations of the Bellman operator at once, benefiting the underlying learning algorithm. However, till now, it has been challenging to effectively implement this idea, especially in high-dimensional problems. In this paper, we introduce iterated $Q$-Network (i-QN), a novel principled approach that enables multiple consecutive Bellman updates by learning a tailored sequence of action-value functions where each serves as the target for the next. We show that i-QN is theoretically grounded and that it can be seamlessly used in value-based and actor-critic methods. We empirically demonstrate the advantages of i-QN in Atari $2600$ games and MuJoCo continuous control problems.





## Improving Small-Scale Large Language Models Function Calling for Reasoning Tasks
- **Url**: http://arxiv.org/abs/2410.18890v1
- **Authors**: ['Graziano A. Manduzio', 'Federico A. Galatolo', 'Mario G. C. A. Cimino', 'Enzo Pasquale Scilingo', 'Lorenzo Cominelli']
- **Abstrat**: Recent advancements in Large Language Models (LLMs) have demonstrated exceptional capabilities in natural language understanding and generation. While these models excel in general complex reasoning tasks, they still face challenges in mathematical problem-solving and logical reasoning. To address these limitations, researchers have explored function calling abilities, allowing LLMs to execute provided functions and utilize their outputs for task completion. However, concentrating on specific tasks can be very inefficient for large-scale LLMs to be used, because of the expensive cost of training and inference stages they need in terms of computational resources. This study introduces a novel framework for training smaller language models in function calling, focusing on specific logical and mathematical reasoning tasks. The approach aims to improve performances of small-scale models for these tasks using function calling, ensuring a high level of accuracy. Our framework employs an agent that, given a problem and a set of callable functions, queries the LLM by injecting a description and examples of the usable functions into the prompt and managing their calls in a step-by-step reasoning chain. This process is used to create a dataset of correct and incorrect reasoning chain chat completions from a large-scale LLM. This dataset is used to train a smaller LLM using Reinforcement Learning from Human Feedback (RLHF), specifically employing the Direct Preference Optimization (DPO) technique. Experimental results demonstrate how the proposed approach balances the trade-off between model size and performance, improving the ability of function calling for reasoning tasks, in smaller models.





## Diff-Instruct++: Training One-step Text-to-image Generator Model to Align with Human Preferences
- **Url**: http://arxiv.org/abs/2410.18881v1
- **Authors**: ['Weijian Luo']
- **Abstrat**: One-step text-to-image generator models offer advantages such as swift inference efficiency, flexible architectures, and state-of-the-art generation performance. In this paper, we study the problem of aligning one-step generator models with human preferences for the first time. Inspired by the success of reinforcement learning using human feedback (RLHF), we formulate the alignment problem as maximizing expected human reward functions while adding an Integral Kullback-Leibler divergence term to prevent the generator from diverging. By overcoming technical challenges, we introduce Diff-Instruct++ (DI++), the first, fast-converging and image data-free human preference alignment method for one-step text-to-image generators. We also introduce novel theoretical insights, showing that using CFG for diffusion distillation is secretly doing RLHF with DI++. Such an interesting finding brings understanding and potential contributions to future research involving CFG. In the experiment sections, we align both UNet-based and DiT-based one-step generators using DI++, which use the Stable Diffusion 1.5 and the PixelArt-$\alpha$ as the reference diffusion processes. The resulting DiT-based one-step text-to-image model achieves a strong Aesthetic Score of 6.19 and an Image Reward of 1.24 on the COCO validation prompt dataset. It also achieves a leading Human preference Score (HPSv2.0) of 28.48, outperforming other open-sourced models such as Stable Diffusion XL, DMD2, SD-Turbo, as well as PixelArt-$\alpha$. Both theoretical contributions and empirical evidence indicate that DI++ is a strong human-preference alignment approach for one-step text-to-image models.





## Survey of Learning-based Approaches for Robotic In-Hand Manipulation
- **Url**: http://arxiv.org/abs/2401.07915v2
- **Authors**: ['Abraham Itzhak Weinberg', 'Alon Shirizly', 'Osher Azulay', 'Avishai Sintov']
- **Abstrat**: Human dexterity is an invaluable capability for precise manipulation of objects in complex tasks. The capability of robots to similarly grasp and perform in-hand manipulation of objects is critical for their use in the ever changing human environment, and for their ability to replace manpower. In recent decades, significant effort has been put in order to enable in-hand manipulation capabilities to robotic systems. Initial robotic manipulators followed carefully programmed paths, while later attempts provided a solution based on analytical modeling of motion and contact. However, these have failed to provide practical solutions due to inability to cope with complex environments and uncertainties. Therefore, the effort has shifted to learning-based approaches where data is collected from the real world or through a simulation, during repeated attempts to complete various tasks. The vast majority of learning approaches focused on learning data-based models that describe the system to some extent or Reinforcement Learning (RL). RL, in particular, has seen growing interest due to the remarkable ability to generate solutions to problems with minimal human guidance. In this survey paper, we track the developments of learning approaches for in-hand manipulations and, explore the challenges and opportunities. This survey is designed both as an introduction for novices in the field with a glossary of terms as well as a guide of novel advances for advanced practitioners.





## Learning Collusion in Episodic, Inventory-Constrained Markets
- **Url**: http://arxiv.org/abs/2410.18871v1
- **Authors**: ['Paul Friedrich', 'Barna P√°sztor', 'Giorgia Ramponi']
- **Abstrat**: Pricing algorithms have demonstrated the capability to learn tacit collusion that is largely unaddressed by current regulations. Their increasing use in markets, including oligopolistic industries with a history of collusion, calls for closer examination by competition authorities. In this paper, we extend the study of tacit collusion in learning algorithms from basic pricing games to more complex markets characterized by perishable goods with fixed supply and sell-by dates, such as airline tickets, perishables, and hotel rooms. We formalize collusion within this framework and introduce a metric based on price levels under both the competitive (Nash) equilibrium and collusive (monopolistic) optimum. Since no analytical expressions for these price levels exist, we propose an efficient computational approach to derive them. Through experiments, we demonstrate that deep reinforcement learning agents can learn to collude in this more complex domain. Additionally, we analyze the underlying mechanisms and structures of the collusive strategies these agents adopt.





## Hierarchical Multi-agent Reinforcement Learning for Cyber Network Defense
- **Url**: http://arxiv.org/abs/2410.17351v2
- **Authors**: ['Aditya Vikram Singh', 'Ethan Rathbun', 'Emma Graham', 'Lisa Oakley', 'Simona Boboila', 'Alina Oprea', 'Peter Chin']
- **Abstrat**: Recent advances in multi-agent reinforcement learning (MARL) have created opportunities to solve complex real-world tasks. Cybersecurity is a notable application area, where defending networks against sophisticated adversaries remains a challenging task typically performed by teams of security operators. In this work, we explore novel MARL strategies for building autonomous cyber network defenses that address challenges such as large policy spaces, partial observability, and stealthy, deceptive adversarial strategies. To facilitate efficient and generalized learning, we propose a hierarchical Proximal Policy Optimization (PPO) architecture that decomposes the cyber defense task into specific sub-tasks like network investigation and host recovery. Our approach involves training sub-policies for each sub-task using PPO enhanced with domain expertise. These sub-policies are then leveraged by a master defense policy that coordinates their selection to solve complex network defense tasks. Furthermore, the sub-policies can be fine-tuned and transferred with minimal cost to defend against shifts in adversarial behavior or changes in network settings. We conduct extensive experiments using CybORG Cage 4, the state-of-the-art MARL environment for cyber defense. Comparisons with multiple baselines across different adversaries show that our hierarchical learning approach achieves top performance in terms of convergence speed, episodic return, and several interpretable metrics relevant to cybersecurity, including the fraction of clean machines on the network, precision, and false positives on recoveries.





## Optimizing over Multiple Distributions under Generalized Quasar-Convexity Condition
- **Url**: http://arxiv.org/abs/2407.14839v2
- **Authors**: ['Shihong Ding', 'Long Yang', 'Luo Luo', 'Cong Fang']
- **Abstrat**: We study a typical optimization model where the optimization variable is composed of multiple probability distributions. Though the model appears frequently in practice, such as for policy problems, it lacks specific analysis in the general setting. For this optimization problem, we propose a new structural condition/landscape description named generalized quasar-convexity (GQC) beyond the realms of convexity. In contrast to original quasar-convexity \citep{hinder2020near}, GQC allows an individual quasar-convex parameter $\gamma_i$ for each variable block $i$ and the smaller of $\gamma_i$ implies less block-convexity. To minimize the objective function, we consider a generalized oracle termed as the internal function that includes the standard gradient oracle as a special case. We provide optimistic mirror descent (OMD) for multiple distributions and prove that the algorithm can achieve an adaptive $\tilde{\mathcal{O}}((\sum_{i=1}^d1/\gamma_i)\epsilon^{-1})$ iteration complexity to find an $epsilon$-suboptimal global solution without pre-known the exact values of $\gamma_i$ when the objective admits "polynomial-like" structural. Notably, it achieves iteration complexity that does not explicitly depend on the number of distributions and strictly faster $(\sum_{i=1}^d 1/\gamma_i \text{ v.s. } d\max_{i\in[1:d]} 1/\gamma_i)$ than mirror decent methods. We also extend GQC to the minimax optimization problem proposing the generalized quasar-convexity-concavity (GQCC) condition and a decentralized variant of OMD with regularization. Finally, we show the applications of our algorithmic framework on discounted Markov Decision Processes problem and Markov games, which bring new insights on the landscape analysis of reinforcement learning.





## Towards Visual Text Design Transfer Across Languages
- **Url**: http://arxiv.org/abs/2410.18823v1
- **Authors**: ['Yejin Choi', 'Jiwan Chung', 'Sumin Shim', 'Giyeong Oh', 'Youngjae Yu']
- **Abstrat**: Visual text design plays a critical role in conveying themes, emotions, and atmospheres in multimodal formats such as film posters and album covers. Translating these visual and textual elements across languages extends the concept of translation beyond mere text, requiring the adaptation of aesthetic and stylistic features. To address this, we introduce a novel task of Multimodal Style Translation (MuST-Bench), a benchmark designed to evaluate the ability of visual text generation models to perform translation across different writing systems while preserving design intent. Our initial experiments on MuST-Bench reveal that existing visual text generation models struggle with the proposed task due to the inadequacy of textual descriptions in conveying visual design. In response, we introduce SIGIL, a framework for multimodal style translation that eliminates the need for style descriptions. SIGIL enhances image generation models through three innovations: glyph latent for multilingual settings, pretrained VAEs for stable style guidance, and an OCR model with reinforcement learning feedback for optimizing readable character generation. SIGIL outperforms existing baselines by achieving superior style consistency and legibility while maintaining visual fidelity, setting itself apart from traditional description-based approaches. We release MuST-Bench publicly for broader use and exploration https://huggingface.co/datasets/yejinc/MuST-Bench.





## PointPatchRL -- Masked Reconstruction Improves Reinforcement Learning on Point Clouds
- **Url**: http://arxiv.org/abs/2410.18800v1
- **Authors**: ['Bal√°zs Gyenes', 'Nikolai Franke', 'Philipp Becker', 'Gerhard Neumann']
- **Abstrat**: Perceiving the environment via cameras is crucial for Reinforcement Learning (RL) in robotics. While images are a convenient form of representation, they often complicate extracting important geometric details, especially with varying geometries or deformable objects. In contrast, point clouds naturally represent this geometry and easily integrate color and positional data from multiple camera views. However, while deep learning on point clouds has seen many recent successes, RL on point clouds is under-researched, with only the simplest encoder architecture considered in the literature. We introduce PointPatchRL (PPRL), a method for RL on point clouds that builds on the common paradigm of dividing point clouds into overlapping patches, tokenizing them, and processing the tokens with transformers. PPRL provides significant improvements compared with other point-cloud processing architectures previously used for RL. We then complement PPRL with masked reconstruction for representation learning and show that our method outperforms strong model-free and model-based baselines on image observations in complex manipulation tasks containing deformable objects and variations in target object geometry. Videos and code are available at https://alrhub.github.io/pprl-website





## Adapting MLOps for Diverse In-Network Intelligence in 6G Era: Challenges and Solutions
- **Url**: http://arxiv.org/abs/2410.18793v1
- **Authors**: ['Peizheng Li', 'Ioannis Mavromatis', 'Tim Farnham', 'Adnan Aijaz', 'Aftab Khan']
- **Abstrat**: Seamless integration of artificial intelligence (AI) and machine learning (ML) techniques with wireless systems is a crucial step for 6G AInization. However, such integration faces challenges in terms of model functionality and lifecycle management. ML operations (MLOps) offer a systematic approach to tackle these challenges. Existing approaches toward implementing MLOps in a centralized platform often overlook the challenges posed by diverse learning paradigms and network heterogeneity. This article provides a new approach to MLOps targeting the intricacies of future wireless networks. Considering unique aspects of the future radio access network (RAN), we formulate three operational pipelines, namely reinforcement learning operations (RLOps), federated learning operations (FedOps), and generative AI operations (GenOps). These pipelines form the foundation for seamlessly integrating various learning/inference capabilities into networks. We outline the specific challenges and proposed solutions for each operation, facilitating large-scale deployment of AI-Native 6G networks.





## Multi-agent cooperation through learning-aware policy gradients
- **Url**: http://arxiv.org/abs/2410.18636v1
- **Authors**: ['Alexander Meulemans', 'Seijin Kobayashi', 'Johannes von Oswald', 'Nino Scherrer', 'Eric Elmoznino', 'Blake Richards', 'Guillaume Lajoie', 'Blaise Ag√ºera y Arcas', 'Jo√£o Sacramento']
- **Abstrat**: Self-interested individuals often fail to cooperate, posing a fundamental challenge for multi-agent learning. How can we achieve cooperation among self-interested, independent learning agents? Promising recent work has shown that in certain tasks cooperation can be established between learning-aware agents who model the learning dynamics of each other. Here, we present the first unbiased, higher-derivative-free policy gradient algorithm for learning-aware reinforcement learning, which takes into account that other agents are themselves learning through trial and error based on multiple noisy trials. We then leverage efficient sequence models to condition behavior on long observation histories that contain traces of the learning dynamics of other agents. Training long-context policies with our algorithm leads to cooperative behavior and high returns on standard social dilemmas, including a challenging environment where temporally-extended action coordination is required. Finally, we derive from the iterated prisoner's dilemma a novel explanation for how and when cooperation arises among self-interested learning-aware agents.





## Leveraging Graph Neural Networks and Multi-Agent Reinforcement Learning for Inventory Control in Supply Chains
- **Url**: http://arxiv.org/abs/2410.18631v1
- **Authors**: ['Niki Kotecha', 'Antonio del Rio Chanona']
- **Abstrat**: Inventory control in modern supply chains has attracted significant attention due to the increasing number of disruptive shocks and the challenges posed by complex dynamics, uncertainties, and limited collaboration. Traditional methods, which often rely on static parameters, struggle to adapt to changing environments. This paper proposes a Multi-Agent Reinforcement Learning (MARL) framework with Graph Neural Networks (GNNs) for state representation to address these limitations.   Our approach redefines the action space by parameterizing heuristic inventory control policies, making it adaptive as the parameters dynamically adjust based on system conditions. By leveraging the inherent graph structure of supply chains, our framework enables agents to learn the system's topology, and we employ a centralized learning, decentralized execution scheme that allows agents to learn collaboratively while overcoming information-sharing constraints. Additionally, we incorporate global mean pooling and regularization techniques to enhance performance.   We test the capabilities of our proposed approach on four different supply chain configurations and conduct a sensitivity analysis. This work paves the way for utilizing MARL-GNN frameworks to improve inventory management in complex, decentralized supply chain environments.





## SAMG: State-Action-Aware Offline-to-Online Reinforcement Learning with Offline Model Guidance
- **Url**: http://arxiv.org/abs/2410.18626v1
- **Authors**: ['Liyu Zhang', 'Haochi Wu', 'Xu Wan', 'Quan Kong', 'Ruilong Deng', 'Mingyang Sun']
- **Abstrat**: The offline-to-online (O2O) paradigm in reinforcement learning (RL) utilizes pre-trained models on offline datasets for subsequent online fine-tuning. However, conventional O2O RL algorithms typically require maintaining and retraining the large offline datasets to mitigate the effects of out-of-distribution (OOD) data, which limits their efficiency in exploiting online samples. To address this challenge, we introduce a new paradigm called SAMG: State-Action-Conditional Offline-to-Online Reinforcement Learning with Offline Model Guidance. In particular, rather than directly training on offline data, SAMG freezes the pre-trained offline critic to provide offline values for each state-action pair to deliver compact offline information. This framework eliminates the need for retraining with offline data by freezing and leveraging these values of the offline model. These are then incorporated with the online target critic using a Bellman equation weighted by a policy state-action-aware coefficient. This coefficient, derived from a conditional variational auto-encoder (C-VAE), aims to capture the reliability of the offline data on a state-action level. SAMG could be easily integrated with existing Q-function based O2O RL algorithms. Theoretical analysis shows good optimality and lower estimation error of SAMG. Empirical evaluations demonstrate that SAMG outperforms four state-of-the-art O2O RL algorithms in the D4RL benchmark.





## Evolutionary Dispersal of Ecological Species via Multi-Agent Deep Reinforcement Learning
- **Url**: http://arxiv.org/abs/2410.18621v1
- **Authors**: ['Wonhyung Choi', 'Inkyung Ahn']
- **Abstrat**: Understanding species dynamics in heterogeneous environments is essential for ecosystem studies. Traditional models assumed homogeneous habitats, but recent approaches include spatial and temporal variability, highlighting species migration. We adopt starvation-driven diffusion (SDD) models as nonlinear diffusion to describe species dispersal based on local resource conditions, showing advantages for species survival. However, accurate prediction remains challenging due to model simplifications. This study uses multi-agent reinforcement learning (MARL) with deep Q-networks (DQN) to simulate single species and predator-prey interactions, incorporating SDD-type rewards. Our simulations reveal evolutionary dispersal strategies, providing insights into species dispersal mechanisms and validating traditional mathematical models.





## Learning Transparent Reward Models via Unsupervised Feature Selection
- **Url**: http://arxiv.org/abs/2410.18608v1
- **Authors**: ['Daulet Baimukashev', 'Gokhan Alcan', 'Kevin Sebastian Luck', 'Ville Kyrki']
- **Abstrat**: In complex real-world tasks such as robotic manipulation and autonomous driving, collecting expert demonstrations is often more straightforward than specifying precise learning objectives and task descriptions. Learning from expert data can be achieved through behavioral cloning or by learning a reward function, i.e., inverse reinforcement learning. The latter allows for training with additional data outside the training distribution, guided by the inferred reward function. We propose a novel approach to construct compact and transparent reward models from automatically selected state features. These inferred rewards have an explicit form and enable the learning of policies that closely match expert behavior by training standard reinforcement learning algorithms from scratch. We validate our method's performance in various robotic environments with continuous and high-dimensional state spaces. Webpage: \url{https://sites.google.com/view/transparent-reward}.





## Resilience-based post disaster recovery optimization for infrastructure system via Deep Reinforcement Learning
- **Url**: http://arxiv.org/abs/2410.18577v1
- **Authors**: ['Huangbin Liang', 'Beatriz Moya', 'Francisco Chinesta', 'Eleni Chatzi']
- **Abstrat**: Infrastructure systems are critical in modern communities but are highly susceptible to various natural and man-made disasters. Efficient post-disaster recovery requires repair-scheduling approaches under the limitation of capped resources that need to be shared across the system. Existing approaches, including component ranking methods, greedy evolutionary algorithms, and data-driven machine learning models, face various limitations when tested within such a context. To tackle these issues, we propose a novel approach to optimize post-disaster recovery of infrastructure systems by leveraging Deep Reinforcement Learning (DRL) methods and incorporating a specialized resilience metric to lead the optimization. The system topology is represented adopting a graph-based structure, where the system's recovery process is formulated as a sequential decision-making problem. Deep Q-learning algorithms are employed to learn optimal recovery strategies by mapping system states to specific actions, as for instance which component ought to be repaired next, with the goal of maximizing long-term recovery from a resilience-oriented perspective. To demonstrate the efficacy of our proposed approach, we implement this scheme on the example of post-earthquake recovery optimization for an electrical substation system. We assess different deep Q-learning algorithms to this end, namely vanilla Deep Q-Networks (DQN), Double DQN(DDQN), Duel DQN, and duel DDQN, demonstrating superiority of the DDQN for the considered problem. A further comparative analysis against baseline methods during testing reveals the superior performance of the proposed method in terms of both optimization effect and computational cost, rendering this an attractive approach in the context of resilience enhancement and rapid response and recovery.





# TD3
# Prioritized Experience Replay
# path planning
## ANAVI: Audio Noise Awareness using Visuals of Indoor environments for NAVIgation
- **Url**: http://arxiv.org/abs/2410.18932v1
- **Authors**: ['Vidhi Jain', 'Rishi Veerapaneni', 'Yonatan Bisk']
- **Abstrat**: We propose Audio Noise Awareness using Visuals of Indoors for NAVIgation for quieter robot path planning. While humans are naturally aware of the noise they make and its impact on those around them, robots currently lack this awareness. A key challenge in achieving audio awareness for robots is estimating how loud will the robot's actions be at a listener's location? Since sound depends upon the geometry and material composition of rooms, we train the robot to passively perceive loudness using visual observations of indoor environments. To this end, we generate data on how loud an 'impulse' sounds at different listener locations in simulated homes, and train our Acoustic Noise Predictor (ANP). Next, we collect acoustic profiles corresponding to different actions for navigation. Unifying ANP with action acoustics, we demonstrate experiments with wheeled (Hello Robot Stretch) and legged (Unitree Go2) robots so that these robots adhere to the noise constraints of the environment. See code and data at https://anavi-corl24.github.io/





## MazeNet: An Accurate, Fast, and Scalable Deep Learning Solution for Steiner Minimum Trees
- **Url**: http://arxiv.org/abs/2410.18832v1
- **Authors**: ['Gabriel D√≠az Ramos', 'Toros Arikan', 'Richard G. Baraniuk']
- **Abstrat**: The Obstacle Avoiding Rectilinear Steiner Minimum Tree (OARSMT) problem, which seeks the shortest interconnection of a given number of terminals in a rectilinear plane while avoiding obstacles, is a critical task in integrated circuit design, network optimization, and robot path planning. Since OARSMT is NP-hard, exact algorithms scale poorly with the number of terminals, leading practical solvers to sacrifice accuracy for large problems. We propose MazeNet, a deep learning-based method that learns to solve the OARSMT from data. MazeNet reframes OARSMT as a maze-solving task that can be addressed with a recurrent convolutional neural network (RCNN). A key hallmark of MazeNet is its scalability: we only need to train the RCNN blocks on mazes with a small number of terminals; larger mazes can be solved by replicating the same pre-trained blocks to create a larger network. Across a wide range of experiments, MazeNet achieves perfect OARSMT-solving accuracy, significantly reduces runtime compared to classical exact algorithms, and can handle more terminals than state-of-the-art approximate algorithms.





## Path planning for multi-quadrotor 3D boundary surveillance using non-autonomous discrete memristor hyperchaotic system
- **Url**: http://arxiv.org/abs/2410.05215v2
- **Authors**: ['Harisankar R', 'Abhishek Kaushik', 'Sishu Shankar Muni']
- **Abstrat**: Recent studies have shown that chaotic maps are well-suited for applications requiring unpredictable behaviour, making them a valuable tool for enhancing unpredictability and complexity. A method is developed using 3D parametric equations to make boundary surveillance more robust and flexible to effectively cover and adapt to the ever-changing situations of boundary surveillance. A non-autonomous discrete memristor hyperchaotic map is utilized to significantly enhance the unpredictability of trajectories in boundary surveillance. Since the scaling property is an affine transformation, and the hyperchaotic systems are invariant under affine transformations, the adaptability of hyperchaotic trajectories is visually demonstrated by applying the scaling property for robust path planning. Furthermore, hyperchaotic systems enable the generation of mutually independent trajectories with slight variations in initial conditions, allowing multiple quadrotors to operate simultaneously along a single guiding path. This minimizes the chance of overlap and interference, ensuring effective and coordinated surveillance.





## Active Target Tracking Using Bearing-only Measurements With Gaussian Process Learning
- **Url**: http://arxiv.org/abs/2410.18669v1
- **Authors**: ['Yingbo Fu', 'Ziwen Yang', 'Shanying Zhu', 'Cailian Chen', 'Xinping Guan']
- **Abstrat**: This paper studies the tracking problem of target with the partially unknown motion model by an active agent with bearing-only measurements using Gaussian process learning. To address this problem, a learning-planning-control framework is proposed. First, to learn and predict the target motion under mild assumptions, a Gaussian-process-based scheme is proposed, and a probabilistic uniform prediction error bound can be rigorously proved. Second, by analyzing the data dependence of the posterior covariance, we obtain an optimal relative trajectory to achieve efficient sampling. Third, to realize efficient learning, a controller to track the planned path is proposed based on the learned target motion, which can provide guaranteed tracking performance. Theoretical analysis is conducted to prove the the given probabilistic error bounds. Numerical examples and comparison with other typical methods verify the feasibility and superior performance of our proposed framework.




