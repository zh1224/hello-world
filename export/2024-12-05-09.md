# reinforcement learning
## DynaMITE-RL: A Dynamic Model for Improved Temporal Meta-Reinforcement Learning
- **Url**: http://arxiv.org/abs/2402.15957v2
- **Authors**: ['Anthony Liang', 'Guy Tennenholtz', 'Chih-wei Hsu', 'Yinlam Chow', 'Erdem Bıyık', 'Craig Boutilier']
- **Abstrat**: We introduce DynaMITE-RL, a meta-reinforcement learning (meta-RL) approach to approximate inference in environments where the latent state evolves at varying rates. We model episode sessions - parts of the episode where the latent state is fixed - and propose three key modifications to existing meta-RL methods: consistency of latent information within sessions, session masking, and prior latent conditioning. We demonstrate the importance of these modifications in various domains, ranging from discrete Gridworld environments to continuous-control and simulated robot assistive tasks, demonstrating that DynaMITE-RL significantly outperforms state-of-the-art baselines in sample efficiency and inference returns.





## PC-Gym: Benchmark Environments For Process Control Problems
- **Url**: http://arxiv.org/abs/2410.22093v3
- **Authors**: ['Maximilian Bloor', 'José Torraca', 'Ilya Orson Sandoval', 'Akhil Ahmed', 'Martha White', 'Mehmet Mercangöz', 'Calvin Tsay', 'Ehecatl Antonio Del Rio Chanona', 'Max Mowbray']
- **Abstrat**: PC-Gym is an open-source tool for developing and evaluating reinforcement learning (RL) algorithms in chemical process control. It features environments that simulate various chemical processes, incorporating nonlinear dynamics, disturbances, and constraints. The tool includes customizable constraint handling, disturbance generation, reward function design, and enables comparison of RL algorithms against Nonlinear Model Predictive Control (NMPC) across different scenarios. Case studies demonstrate the framework's effectiveness in evaluating RL approaches for systems like continuously stirred tank reactors, multistage extraction processes, and crystallization reactors. The results reveal performance gaps between RL algorithms and NMPC oracles, highlighting areas for improvement and enabling benchmarking. By providing a standardized platform, PC-Gym aims to accelerate research at the intersection of machine learning, control, and process systems engineering. By connecting theoretical RL advances with practical industrial process control applications, offering researchers a tool for exploring data-driven control solutions.





## Towards a Robust Soft Baby Robot With Rich Interaction Ability for Advanced Machine Learning Algorithms
- **Url**: http://arxiv.org/abs/2404.08093v2
- **Authors**: ['Mohannad Alhakami', 'Dylan R. Ashley', 'Joel Dunham', 'Yanning Dai', 'Francesco Faccio', 'Eric Feron', 'Jürgen Schmidhuber']
- **Abstrat**: Advanced machine learning algorithms require platforms that are extremely robust and equipped with rich sensory feedback to handle extensive trial-and-error learning without relying on strong inductive biases. Traditional robotic designs, while well-suited for their specific use cases, are often fragile when used with these algorithms. To address this gap -- and inspired by the vision of enabling curiosity-driven baby robots -- we present a novel robotic limb designed from scratch. Our design has a hybrid soft-hard structure, high redundancy with rich non-contact sensors (exclusively cameras), and easily replaceable failure points. Proof-of-concept experiments using two contemporary reinforcement learning algorithms on a physical prototype demonstrate that our design is able to succeed in a simple target-finding task even under simulated sensor failures, all with minimal human oversight during extended learning periods. We believe this design represents a concrete step toward more tailored robotic designs for achieving general-purpose, generally intelligent robots.





## AI-Driven Day-to-Day Route Choice
- **Url**: http://arxiv.org/abs/2412.03338v1
- **Authors**: ['Leizhen Wang', 'Peibo Duan', 'Zhengbing He', 'Cheng Lyu', 'Xin Chen', 'Nan Zheng', 'Li Yao', 'Zhenliang Ma']
- **Abstrat**: Understanding travelers' route choices can help policymakers devise optimal operational and planning strategies for both normal and abnormal circumstances. However, existing choice modeling methods often rely on predefined assumptions and struggle to capture the dynamic and adaptive nature of travel behavior. Recently, Large Language Models (LLMs) have emerged as a promising alternative, demonstrating remarkable ability to replicate human-like behaviors across various fields. Despite this potential, their capacity to accurately simulate human route choice behavior in transportation contexts remains doubtful. To satisfy this curiosity, this paper investigates the potential of LLMs for route choice modeling by introducing an LLM-empowered agent, "LLMTraveler." This agent integrates an LLM as its core, equipped with a memory system that learns from past experiences and makes decisions by balancing retrieved data and personality traits. The study systematically evaluates the LLMTraveler's ability to replicate human-like decision-making through two stages: (1) analyzing its route-switching behavior in single origin-destination (OD) pair congestion game scenarios, where it demonstrates patterns align with laboratory data but are not fully explained by traditional models, and (2) testing its capacity to model day-to-day (DTD) adaptive learning behaviors on the Ortuzar and Willumsen (OW) network, producing results comparable to Multinomial Logit (MNL) and Reinforcement Learning (RL) models. These experiments demonstrate that the framework can partially replicate human-like decision-making in route choice while providing natural language explanations for its decisions. This capability offers valuable insights for transportation policymaking, such as simulating traveler responses to new policies or changes in the network.





## Tackling Decision Processes with Non-Cumulative Objectives using Reinforcement Learning
- **Url**: http://arxiv.org/abs/2405.13609v2
- **Authors**: ['Maximilian Nägele', 'Jan Olle', 'Thomas Fösel', 'Remmy Zen', 'Florian Marquardt']
- **Abstrat**: Markov decision processes (MDPs) are used to model a wide variety of applications ranging from game playing over robotics to finance. Their optimal policy typically maximizes the expected sum of rewards given at each step of the decision process. However, a large class of problems does not fit straightforwardly into this framework: Non-cumulative Markov decision processes (NCMDPs), where instead of the expected sum of rewards, the expected value of an arbitrary function of the rewards is maximized. Example functions include the maximum of the rewards or their mean divided by their standard deviation. In this work, we introduce a general mapping of NCMDPs to standard MDPs. This allows all techniques developed to find optimal policies for MDPs, such as reinforcement learning or dynamic programming, to be directly applied to the larger class of NCMDPs. Focusing on reinforcement learning, we show applications in a diverse set of tasks, including classical control, portfolio optimization in finance, and discrete optimization problems. Given our approach, we can improve both final performance and training time compared to relying on standard MDPs.





## Rotograb: Combining Biomimetic Hands with Industrial Grippers using a Rotating Thumb
- **Url**: http://arxiv.org/abs/2412.03279v1
- **Authors**: ['Arnaud Bersier', 'Matteo Leonforte', 'Alessio Vanetta', 'Sarah Lia Andrea Wotke', 'Andrea Nappi', 'Yifan Zhou', 'Sebastiano Oliani', 'Alexander M. Kübler', 'Robert K. Katzschmann']
- **Abstrat**: The development of robotic grippers and hands for automation aims to emulate human dexterity without sacrificing the efficiency of industrial grippers. This study introduces Rotograb, a tendon-actuated robotic hand featuring a novel rotating thumb. The aim is to combine the dexterity of human hands with the efficiency of industrial grippers. The rotating thumb enlarges the workspace and allows in-hand manipulation. A novel joint design minimizes movement interference and simplifies kinematics, using a cutout for tendon routing. We integrate teleoperation, using a depth camera for real-time tracking and autonomous manipulation powered by reinforcement learning with proximal policy optimization. Experimental evaluations demonstrate that Rotograb's rotating thumb greatly improves both operational versatility and workspace. It can handle various grasping and manipulation tasks with objects from the YCB dataset, with particularly good results when rotating objects within its grasp. Rotograb represents a notable step towards bridging the capability gap between human hands and industrial grippers. The tendon-routing and thumb-rotating mechanisms allow for a new level of control and dexterity. Integrating teleoperation and autonomous learning underscores Rotograb's adaptability and sophistication, promising substantial advancements in both robotics research and practical applications.





## Reinforcement Learning for Finite Space Mean-Field Type Games
- **Url**: http://arxiv.org/abs/2409.18152v2
- **Authors**: ['Kai Shao', 'Jiacheng Shen', 'Chijie An', 'Mathieu Laurière']
- **Abstrat**: Mean field type games (MFTGs) describe Nash equilibria between large coalitions: each coalition consists of a continuum of cooperative agents who maximize the average reward of their coalition while interacting non-cooperatively with a finite number of other coalitions. Although the theory has been extensively developed, we are still lacking efficient and scalable computational methods. Here, we develop reinforcement learning methods for such games in a finite space setting with general dynamics and reward functions. We start by proving that MFTG solution yields approximate Nash equilibria in finite-size coalition games. We then propose two algorithms. The first is based on quantization of mean-field spaces and Nash Q-learning. We provide convergence and stability analysis. We then propose a deep reinforcement learning algorithm, which can scale to larger spaces. Numerical experiments in 5 environments with mean-field distributions of dimension up to $200$ show the scalability and efficiency of the proposed method.





## Chain-structured neural architecture search for financial time series forecasting
- **Url**: http://arxiv.org/abs/2403.14695v2
- **Authors**: ['Denis Levchenko', 'Efstratios Rappos', 'Shabnam Ataee', 'Biagio Nigro', 'Stephan Robert-Nicoud']
- **Abstrat**: Neural architecture search (NAS) emerged as a way to automatically optimize neural networks for a specific task and dataset. Despite an abundance of research on NAS for images and natural language applications, similar studies for time series data are lacking. Among NAS search spaces, chain-structured are the simplest and most applicable to small datasets like time series. We compare three popular NAS strategies on chain-structured search spaces: Bayesian optimization (specifically Tree-structured Parzen Estimator), the hyperband method, and reinforcement learning in the context of financial time series forecasting. These strategies were employed to optimize simple well-understood neural architectures like the MLP, 1D CNN, and RNN, with more complex temporal fusion transformers (TFT) and their own optimizers included for comparison. We find Bayesian optimization and the hyperband method performing best among the strategies, and RNN and 1D CNN best among the architectures, but all methods were very close to each other with a high variance due to the difficulty of working with financial datasets. We discuss our approach to overcome the variance and provide implementation recommendations for future users and researchers.





## Learning on One Mode: Addressing Multi-Modality in Offline Reinforcement Learning
- **Url**: http://arxiv.org/abs/2412.03258v1
- **Authors**: ['Mianchu Wang', 'Yue Jin', 'Giovanni Montana']
- **Abstrat**: Offline reinforcement learning (RL) seeks to learn optimal policies from static datasets without interacting with the environment. A common challenge is handling multi-modal action distributions, where multiple behaviours are represented in the data. Existing methods often assume unimodal behaviour policies, leading to suboptimal performance when this assumption is violated. We propose Weighted Imitation Learning on One Mode (LOM), a novel approach that focuses on learning from a single, promising mode of the behaviour policy. By using a Gaussian mixture model to identify modes and selecting the best mode based on expected returns, LOM avoids the pitfalls of averaging over conflicting actions. Theoretically, we show that LOM improves performance while maintaining simplicity in policy learning. Empirically, LOM outperforms existing methods on standard D4RL benchmarks and demonstrates its effectiveness in complex, multi-modal scenarios.





## Alignment at Pre-training! Towards Native Alignment for Arabic LLMs
- **Url**: http://arxiv.org/abs/2412.03253v1
- **Authors**: ['Juhao Liang', 'Zhenyang Cai', 'Jianqing Zhu', 'Huang Huang', 'Kewei Zong', 'Bang An', 'Mosen Alharthi', 'Juncai He', 'Lian Zhang', 'Haizhou Li', 'Benyou Wang', 'Jinchao Xu']
- **Abstrat**: The alignment of large language models (LLMs) is critical for developing effective and safe language models. Traditional approaches focus on aligning models during the instruction tuning or reinforcement learning stages, referred to in this paper as `post alignment'. We argue that alignment during the pre-training phase, which we term `native alignment', warrants investigation. Native alignment aims to prevent unaligned content from the beginning, rather than relying on post-hoc processing. This approach leverages extensively aligned pre-training data to enhance the effectiveness and usability of pre-trained models. Our study specifically explores the application of native alignment in the context of Arabic LLMs. We conduct comprehensive experiments and ablation studies to evaluate the impact of native alignment on model performance and alignment stability. Additionally, we release open-source Arabic LLMs that demonstrate state-of-the-art performance on various benchmarks, providing significant benefits to the Arabic LLM community.





## Using Deep Reinforcement Learning to Enhance Channel Sampling Patterns in Integrated Sensing and Communication
- **Url**: http://arxiv.org/abs/2412.03157v1
- **Authors**: ['Federico Mason', 'Jacopo Pegoraro']
- **Abstrat**: In Integrated Sensing And Communication (ISAC) systems, estimating the micro-Doppler (mD) spectrogram of a target requires combining channel estimates retrieved from communication with ad-hoc sensing packets, which cope with the sparsity of the communication traffic. Hence, the mD quality depends on the transmission strategy of the sensing packets, which is still a challenging problem with no known solutions. In this letter, we design a deep Reinforcement Learning (RL) framework that fragments such a problem into a sequence of simpler decisions and takes advantage of the mD temporal evolution for maximizing the reconstruction performance. Our method is the first that learns sampling patterns to directly optimize the mD quality, enabling the adaptation of ISAC systems to variable communication traffic. We validate the proposed approach on a dataset of real channel measurements, reaching up to 40% higher mD reconstruction accuracy and several times lower computational complexity than state-of-the-art methods.





## Adaptive Dense Reward: Understanding the Gap Between Action and Reward Space in Alignment
- **Url**: http://arxiv.org/abs/2411.00809v2
- **Authors**: ['Yanshi Li', 'Shaopan Xiong', 'Gengru Chen', 'Xiaoyang Li', 'Yijia Luo', 'Xingyao Zhang', 'Yanhui Huang', 'Xingyuan Bu', 'Yingshui Tan', 'Chun Yuan', 'Jiamang Wang', 'Wenbo Su', 'Bo Zheng']
- **Abstrat**: Reinforcement Learning from Human Feedback (RLHF) has proven highly effective in aligning Large Language Models (LLMs) with human preferences. However, the original RLHF typically optimizes under an overall reward, which can lead to a suboptimal learning process. This limitation stems from RLHF's lack of awareness regarding which specific tokens should be reinforced or suppressed. Moreover, conflicts in supervision can arise, for instance, when a chosen response includes erroneous tokens, while a rejected response contains accurate elements. To rectify these shortcomings, increasing dense reward methods, such as step-wise and token-wise RLHF, have been proposed. However, these existing methods are limited to specific tasks (like mathematics). In this paper, we propose the ``Adaptive Message-wise RLHF'' method, which robustly applies to various tasks. By defining pivot tokens as key indicators, our approach adaptively identifies essential information and converts sequence-level supervision into fine-grained, subsequence-level supervision. This aligns the density of rewards and action spaces more closely with the information density of the input. Experiments demonstrate that our method can be integrated into various training methods, significantly mitigating hallucinations and catastrophic forgetting problems, while outperforming other methods on multiple evaluation metrics. Our method improves the success rate on adversarial samples by 10\% compared to the sample-wise approach, and achieves a 1.3\% improvement on evaluation benchmarks such as MMLU, GSM8K, HumanEval, etc.





## Experience-driven discovery of planning strategies
- **Url**: http://arxiv.org/abs/2412.03111v1
- **Authors**: ['Ruiqi He', 'Falk Lieder']
- **Abstrat**: One explanation for how people can plan efficiently despite limited cognitive resources is that we possess a set of adaptive planning strategies and know when and how to use them. But how are these strategies acquired? While previous research has studied how individuals learn to choose among existing strategies, little is known about the process of forming new planning strategies. In this work, we propose that new planning strategies are discovered through metacognitive reinforcement learning. To test this, we designed a novel experiment to investigate the discovery of new planning strategies. We then present metacognitive reinforcement learning models and demonstrate their capability for strategy discovery as well as show that they provide a better explanation of human strategy discovery than alternative learning mechanisms. However, when fitted to human data, these models exhibit a slower discovery rate than humans, leaving room for improvement.





# TD3
# Prioritized Experience Replay
# path planning
## Genetic Algorithm Based System for Path Planning with Unmanned Aerial Vehicles Swarms in Cell-Grid Environments
- **Url**: http://arxiv.org/abs/2412.03433v1
- **Authors**: ['Alejandro Puente-Castro', 'Enrique Fernandez-Blanco', 'Daniel Rivero']
- **Abstrat**: Path Planning methods for autonomously controlling swarms of unmanned aerial vehicles (UAVs) are gaining momentum due to their operational advantages. An increasing number of scenarios now require autonomous control of multiple UAVs, as autonomous operation can significantly reduce labor costs. Additionally, obtaining optimal flight paths can lower energy consumption, thereby extending battery life for other critical operations. Many of these scenarios, however, involve obstacles such as power lines and trees, which complicate Path Planning. This paper presents an evolutionary computation-based system employing genetic algorithms to address this problem in environments with obstacles. The proposed approach aims to ensure complete coverage of areas with fixed obstacles, such as in field exploration tasks, while minimizing flight time regardless of map size or the number of UAVs in the swarm. No specific goal points or prior information beyond the provided map is required. The experiments conducted in this study used five maps of varying sizes and obstacle densities, as well as a control map without obstacles, with different numbers of UAVs. The results demonstrate that this method can determine optimal paths for all UAVs during full map traversal, thus minimizing resource consumption. A comparative analysis with other state-of-the-art approach is presented to highlight the advantages and potential limitations of the proposed method.




