# reinforcement learning
## Ethics and Persuasion in Reinforcement Learning from Human Feedback: A Procedural Rhetorical Approach
- **Url**: http://arxiv.org/abs/2505.09576v1
- **Authors**: ['Shannon Lodoen', 'Alexi Orchard']
- **Abstrat**: Since 2022, versions of generative AI chatbots such as ChatGPT and Claude have been trained using a specialized technique called Reinforcement Learning from Human Feedback (RLHF) to fine-tune language model output using feedback from human annotators. As a result, the integration of RLHF has greatly enhanced the outputs of these large language models (LLMs) and made the interactions and responses appear more "human-like" than those of previous versions using only supervised learning. The increasing convergence of human and machine-written text has potentially severe ethical, sociotechnical, and pedagogical implications relating to transparency, trust, bias, and interpersonal relations. To highlight these implications, this paper presents a rhetorical analysis of some of the central procedures and processes currently being reshaped by RLHF-enhanced generative AI chatbots: upholding language conventions, information seeking practices, and expectations for social relationships. Rhetorical investigations of generative AI and LLMs have, to this point, focused largely on the persuasiveness of the content generated. Using Ian Bogost's concept of procedural rhetoric, this paper shifts the site of rhetorical investigation from content analysis to the underlying mechanisms of persuasion built into RLHF-enhanced LLMs. In doing so, this theoretical investigation opens a new direction for further inquiry in AI ethics that considers how procedures rerouted through AI-driven technologies might reinforce hegemonic language use, perpetuate biases, decontextualize learning, and encroach upon human relationships. It will therefore be of interest to educators, researchers, scholars, and the growing number of users of generative AI chatbots.





## TuneNSearch: a hybrid transfer learning and local search approach for solving vehicle routing problems
- **Url**: http://arxiv.org/abs/2503.12662v2
- **Authors**: ['Arthur Corrêa', 'Cristóvão Silva', 'Liming Xu', 'Alexandra Brintrup', 'Samuel Moniz']
- **Abstrat**: This paper introduces TuneNSearch, a hybrid transfer learning and local search approach for addressing different variants of vehicle routing problems (VRP). Recently, multi-task learning has gained much attention for solving VRP variants. However, this adaptability often compromises the performance of the models. To address this challenge, we first pre-train a reinforcement learning model on the multi-depot VRP, followed by a short fine-tuning phase to adapt it to different variants. By leveraging the complexity of the multi-depot VRP, the pre-trained model learns richer node representations and gains more transferable knowledge compared to models trained on simpler routing problems, such as the traveling salesman problem. TuneNSearch employs, in the first stage, a Transformer-based architecture, augmented with a residual edge-graph attention network to capture the impact of edge distances and residual connections between layers. This architecture allows for a more precise capture of graph-structured data, improving the encoding of VRP's features. After inference, our model is also coupled with a second stage composed of a local search algorithm, which yields substantial performance gains with minimal computational overhead added. Results show that TuneNSearch outperforms many existing state-of-the-art models trained for each VRP variant, requiring only one-fifth of the training epochs. Our approach demonstrates strong generalization, achieving high performance across different tasks, distributions and problem sizes, thus addressing a long-standing gap in the literature.





## WavReward: Spoken Dialogue Models With Generalist Reward Evaluators
- **Url**: http://arxiv.org/abs/2505.09558v1
- **Authors**: ['Shengpeng Ji', 'Tianle Liang', 'Yangzhuo Li', 'Jialong Zuo', 'Minghui Fang', 'Jinzheng He', 'Yifu Chen', 'Zhengqing Liu', 'Ziyue Jiang', 'Xize Cheng', 'Siqi Zheng', 'Jin Xu', 'Junyang Lin', 'Zhou Zhao']
- **Abstrat**: End-to-end spoken dialogue models such as GPT-4o-audio have recently garnered significant attention in the speech domain. However, the evaluation of spoken dialogue models' conversational performance has largely been overlooked. This is primarily due to the intelligent chatbots convey a wealth of non-textual information which cannot be easily measured using text-based language models like ChatGPT. To address this gap, we propose WavReward, a reward feedback model based on audio language models that can evaluate both the IQ and EQ of spoken dialogue systems with speech input. Specifically, 1) based on audio language models, WavReward incorporates the deep reasoning process and the nonlinear reward mechanism for post-training. By utilizing multi-sample feedback via the reinforcement learning algorithm, we construct a specialized evaluator tailored to spoken dialogue models. 2) We introduce ChatReward-30K, a preference dataset used to train WavReward. ChatReward-30K includes both comprehension and generation aspects of spoken dialogue models. These scenarios span various tasks, such as text-based chats, nine acoustic attributes of instruction chats, and implicit chats. WavReward outperforms previous state-of-the-art evaluation models across multiple spoken dialogue scenarios, achieving a substantial improvement about Qwen2.5-Omni in objective accuracy from 55.1$\%$ to 91.5$\%$. In subjective A/B testing, WavReward also leads by a margin of 83$\%$. Comprehensive ablation studies confirm the necessity of each component of WavReward. All data and code will be publicly at https://github.com/jishengpeng/WavReward after the paper is accepted.





## Llama-Nemotron: Efficient Reasoning Models
- **Url**: http://arxiv.org/abs/2505.00949v3
- **Authors**: ['Akhiad Bercovich', 'Itay Levy', 'Izik Golan', 'Mohammad Dabbah', 'Ran El-Yaniv', 'Omri Puny', 'Ido Galil', 'Zach Moshe', 'Tomer Ronen', 'Najeeb Nabwani', 'Ido Shahaf', 'Oren Tropp', 'Ehud Karpas', 'Ran Zilberstein', 'Jiaqi Zeng', 'Soumye Singhal', 'Alexander Bukharin', 'Yian Zhang', 'Tugrul Konuk', 'Gerald Shen', 'Ameya Sunil Mahabaleshwarkar', 'Bilal Kartal', 'Yoshi Suhara', 'Olivier Delalleau', 'Zijia Chen', 'Zhilin Wang', 'David Mosallanezhad', 'Adi Renduchintala', 'Haifeng Qian', 'Dima Rekesh', 'Fei Jia', 'Somshubra Majumdar', 'Vahid Noroozi', 'Wasi Uddin Ahmad', 'Sean Narenthiran', 'Aleksander Ficek', 'Mehrzad Samadi', 'Jocelyn Huang', 'Siddhartha Jain', 'Igor Gitman', 'Ivan Moshkov', 'Wei Du', 'Shubham Toshniwal', 'George Armstrong', 'Branislav Kisacanin', 'Matvei Novikov', 'Daria Gitman', 'Evelina Bakhturina', 'Jane Polak Scowcroft', 'John Kamalu', 'Dan Su', 'Kezhi Kong', 'Markus Kliegl', 'Rabeeh Karimi', 'Ying Lin', 'Sanjeev Satheesh', 'Jupinder Parmar', 'Pritam Gundecha', 'Brandon Norick', 'Joseph Jennings', 'Shrimai Prabhumoye', 'Syeda Nahida Akter', 'Mostofa Patwary', 'Abhinav Khattar', 'Deepak Narayanan', 'Roger Waleffe', 'Jimmy Zhang', 'Bor-Yiing Su', 'Guyue Huang', 'Terry Kong', 'Parth Chadha', 'Sahil Jain', 'Christine Harvey', 'Elad Segal', 'Jining Huang', 'Sergey Kashirsky', 'Robert McQueen', 'Izzy Putterman', 'George Lam', 'Arun Venkatesan', 'Sherry Wu', 'Vinh Nguyen', 'Manoj Kilaru', 'Andrew Wang', 'Anna Warno', 'Abhilash Somasamudramath', 'Sandip Bhaskar', 'Maka Dong', 'Nave Assaf', 'Shahar Mor', 'Omer Ullman Argov', 'Scot Junkin', 'Oleksandr Romanenko', 'Pedro Larroy', 'Monika Katariya', 'Marco Rovinelli', 'Viji Balas', 'Nicholas Edelman', 'Anahita Bhiwandiwalla', 'Muthu Subramaniam', 'Smita Ithape', 'Karthik Ramamoorthy', 'Yuting Wu', 'Suguna Varshini Velury', 'Omri Almog', 'Joyjit Daw', 'Denys Fridman', 'Erick Galinkin', 'Michael Evans', 'Shaona Ghosh', 'Katherine Luna', 'Leon Derczynski', 'Nikki Pope', 'Eileen Long', 'Seth Schneider', 'Guillermo Siman', 'Tomasz Grzegorzek', 'Pablo Ribalta', 'Monika Katariya', 'Chris Alexiuk', 'Joey Conway', 'Trisha Saar', 'Ann Guan', 'Krzysztof Pawelec', 'Shyamala Prayaga', 'Oleksii Kuchaiev', 'Boris Ginsburg', 'Oluwatobi Olabiyi', 'Kari Briski', 'Jonathan Cohen', 'Bryan Catanzaro', 'Jonah Alben', 'Yonatan Geifman', 'Eric Chung']
- **Abstrat**: We introduce the Llama-Nemotron series of models, an open family of heterogeneous reasoning models that deliver exceptional reasoning capabilities, inference efficiency, and an open license for enterprise use. The family comes in three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs competitively with state-of-the-art reasoning models such as DeepSeek-R1 while offering superior inference throughput and memory efficiency. In this report, we discuss the training procedure for these models, which entails using neural architecture search from Llama 3 models for accelerated inference, knowledge distillation, and continued pretraining, followed by a reasoning-focused post-training stage consisting of two main parts: supervised fine-tuning and large scale reinforcement learning. Llama-Nemotron models are the first open-source models to support a dynamic reasoning toggle, allowing users to switch between standard chat and reasoning modes during inference. To further support open research and facilitate model development, we provide the following resources: 1. We release the Llama-Nemotron reasoning models --LN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA Open Model License Agreement. 2. We release the complete post-training dataset: Llama-Nemotron-Post-Training-Dataset. 3. We also release our training codebases: NeMo, NeMo-Aligner, and Megatron-LM.





## Distilling Realizable Students from Unrealizable Teachers
- **Url**: http://arxiv.org/abs/2505.09546v1
- **Authors**: ['Yujin Kim', 'Nathaniel Chin', 'Arnav Vasudev', 'Sanjiban Choudhury']
- **Abstrat**: We study policy distillation under privileged information, where a student policy with only partial observations must learn from a teacher with full-state access. A key challenge is information asymmetry: the student cannot directly access the teacher's state space, leading to distributional shifts and policy degradation. Existing approaches either modify the teacher to produce realizable but sub-optimal demonstrations or rely on the student to explore missing information independently, both of which are inefficient. Our key insight is that the student should strategically interact with the teacher --querying only when necessary and resetting from recovery states --to stay on a recoverable path within its own observation space. We introduce two methods: (i) an imitation learning approach that adaptively determines when the student should query the teacher for corrections, and (ii) a reinforcement learning approach that selects where to initialize training for efficient exploration. We validate our methods in both simulated and real-world robotic tasks, demonstrating significant improvements over standard teacher-student baselines in training efficiency and final performance. The project website is available at : https://portal-cornell.github.io/CritiQ_ReTRy/





## Multi-Agent Reinforcement Learning Simulation for Environmental Policy Synthesis
- **Url**: http://arxiv.org/abs/2504.12777v2
- **Authors**: ['James Rudd-Jones', 'Mirco Musolesi', 'María Pérez-Ortiz']
- **Abstrat**: Climate policy development faces significant challenges due to deep uncertainty, complex system dynamics, and competing stakeholder interests. Climate simulation methods, such as Earth System Models, have become valuable tools for policy exploration. However, their typical use is for evaluating potential polices, rather than directly synthesizing them. The problem can be inverted to optimize for policy pathways, but the traditional optimization approaches often struggle with non-linear dynamics, heterogeneous agents, and comprehensive uncertainty quantification. We propose a framework for augmenting climate simulations with Multi-Agent Reinforcement Learning (MARL) to address these limitations. We identify key challenges at the interface between climate simulations and the application of MARL in the context of policy synthesis, including reward definition, scalability with increasing agents and state spaces, uncertainty propagation across linked systems, and solution validation. Additionally, we discuss challenges in making MARL-derived solutions interpretable and useful for policy-makers. Our framework provides a foundation for more sophisticated climate policy exploration while acknowledging important limitations and areas for future research.





## Reinforcement Learning for Individual Optimal Policy from Heterogeneous Data
- **Url**: http://arxiv.org/abs/2505.09496v1
- **Authors**: ['Rui Miao', 'Babak Shahbaba', 'Annie Qu']
- **Abstrat**: Offline reinforcement learning (RL) aims to find optimal policies in dynamic environments in order to maximize the expected total rewards by leveraging pre-collected data. Learning from heterogeneous data is one of the fundamental challenges in offline RL. Traditional methods focus on learning an optimal policy for all individuals with pre-collected data from a single episode or homogeneous batch episodes, and thus, may result in a suboptimal policy for a heterogeneous population. In this paper, we propose an individualized offline policy optimization framework for heterogeneous time-stationary Markov decision processes (MDPs). The proposed heterogeneous model with individual latent variables enables us to efficiently estimate the individual Q-functions, and our Penalized Pessimistic Personalized Policy Learning (P4L) algorithm guarantees a fast rate on the average regret under a weak partial coverage assumption on behavior policies. In addition, our simulation studies and a real data application demonstrate the superior numerical performance of the proposed method compared with existing methods.





## Preserving Plasticity in Continual Learning with Adaptive Linearity Injection
- **Url**: http://arxiv.org/abs/2505.09486v1
- **Authors**: ['Seyed Roozbeh Razavi Rohani', 'Khashayar Khajavi', 'Wesley Chung', 'Mo Chen', 'Sharan Vaswani']
- **Abstrat**: Loss of plasticity in deep neural networks is the gradual reduction in a model's capacity to incrementally learn and has been identified as a key obstacle to learning in non-stationary problem settings. Recent work has shown that deep linear networks tend to be resilient towards loss of plasticity. Motivated by this observation, we propose Adaptive Linearization (AdaLin), a general approach that dynamically adapts each neuron's activation function to mitigate plasticity loss. Unlike prior methods that rely on regularization or periodic resets, AdaLin equips every neuron with a learnable parameter and a gating mechanism that injects linearity into the activation function based on its gradient flow. This adaptive modulation ensures sufficient gradient signal and sustains continual learning without introducing additional hyperparameters or requiring explicit task boundaries. When used with conventional activation functions like ReLU, Tanh, and GeLU, we demonstrate that AdaLin can significantly improve performance on standard benchmarks, including Random Label and Permuted MNIST, Random Label and Shuffled CIFAR-10, and Class-Split CIFAR-100. Furthermore, its efficacy is shown in more complex scenarios, such as class-incremental learning on CIFAR-100 with a ResNet-18 backbone, and in mitigating plasticity loss in off-policy reinforcement learning agents. We perform a systematic set of ablations that show that neuron-level adaptation is crucial for good performance and analyze a number of metrics in the network that might be correlated to loss of plasticity.





## Quantum state-agnostic work extraction (almost) without dissipation
- **Url**: http://arxiv.org/abs/2505.09456v1
- **Authors**: ['Josep Lumbreras', 'Ruo Cheng Huang', 'Yanglin Hu', 'Mile Gu', 'Marco Tomamichel']
- **Abstrat**: We investigate work extraction protocols designed to transfer the maximum possible energy to a battery using sequential access to $N$ copies of an unknown pure qubit state. The core challenge is designing interactions to optimally balance two competing goals: charging of the battery optimally using the qubit in hand, and acquiring more information by qubit to improve energy harvesting in subsequent rounds. Here, we leverage exploration-exploitation trade-off in reinforcement learning to develop adaptive strategies achieving energy dissipation that scales only poly-logarithmically in $N$. This represents an exponential improvement over current protocols based on full state tomography.





## Omni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM?
- **Url**: http://arxiv.org/abs/2505.09439v1
- **Authors**: ['Andrew Rouditchenko', 'Saurabhchand Bhati', 'Edson Araujo', 'Samuel Thomas', 'Hilde Kuehne', 'Rogerio Feris', 'James Glass']
- **Abstrat**: We propose Omni-R1 which fine-tunes a recent multi-modal LLM, Qwen2.5-Omni, on an audio question answering dataset with the reinforcement learning method GRPO. This leads to new State-of-the-Art performance on the recent MMAU benchmark. Omni-R1 achieves the highest accuracies on the sounds, music, speech, and overall average categories, both on the Test-mini and Test-full splits. To understand the performance improvement, we tested models both with and without audio and found that much of the performance improvement from GRPO could be attributed to better text-based reasoning. We also made a surprising discovery that fine-tuning without audio on a text-only dataset was effective at improving the audio-based performance.





## Quantum-Enhanced Parameter-Efficient Learning for Typhoon Trajectory Forecasting
- **Url**: http://arxiv.org/abs/2505.09395v1
- **Authors**: ['Chen-Yu Liu', 'Kuan-Cheng Chen', 'Yi-Chien Chen', 'Samuel Yen-Chi Chen', 'Wei-Hao Huang', 'Wei-Jia Huang', 'Yen-Jui Chang']
- **Abstrat**: Typhoon trajectory forecasting is essential for disaster preparedness but remains computationally demanding due to the complexity of atmospheric dynamics and the resource requirements of deep learning models. Quantum-Train (QT), a hybrid quantum-classical framework that leverages quantum neural networks (QNNs) to generate trainable parameters exclusively during training, eliminating the need for quantum hardware at inference time. Building on QT's success across multiple domains, including image classification, reinforcement learning, flood prediction, and large language model (LLM) fine-tuning, we introduce Quantum Parameter Adaptation (QPA) for efficient typhoon forecasting model learning. Integrated with an Attention-based Multi-ConvGRU model, QPA enables parameter-efficient training while maintaining predictive accuracy. This work represents the first application of quantum machine learning (QML) to large-scale typhoon trajectory prediction, offering a scalable and energy-efficient approach to climate modeling. Our results demonstrate that QPA significantly reduces the number of trainable parameters while preserving performance, making high-performance forecasting more accessible and sustainable through hybrid quantum-classical learning.





## Think Smart, Act SMARL! Analyzing Probabilistic Logic Shields for Multi-Agent Reinforcement Learning
- **Url**: http://arxiv.org/abs/2411.04867v2
- **Authors**: ['Satchit Chatterji', 'Erman Acar']
- **Abstrat**: Safe reinforcement learning (RL) is crucial for real-world applications, and multi-agent interactions introduce additional safety challenges. While Probabilistic Logic Shields (PLS) has been a powerful proposal to enforce safety in single-agent RL, their generalizability to multi-agent settings remains unexplored. In this paper, we address this gap by conducting extensive analyses of PLS within decentralized, multi-agent environments, and in doing so, propose Shielded Multi-Agent Reinforcement Learning (SMARL) as a general framework for steering MARL towards norm-compliant outcomes. Our key contributions are: (1) a novel Probabilistic Logic Temporal Difference (PLTD) update for shielded, independent Q-learning, which incorporates probabilistic constraints directly into the value update process; (2) a probabilistic logic policy gradient method for shielded PPO with formal safety guarantees for MARL; and (3) comprehensive evaluation across symmetric and asymmetrically shielded $n$-player game-theoretic benchmarks, demonstrating fewer constraint violations and significantly better cooperation under normative constraints. These results position SMARL as an effective mechanism for equilibrium selection, paving the way toward safer, socially aligned multi-agent systems.





## TensorRL-QAS: Reinforcement learning with tensor networks for scalable quantum architecture search
- **Url**: http://arxiv.org/abs/2505.09371v1
- **Authors**: ['Akash Kundu', 'Stefano Mangini']
- **Abstrat**: Variational quantum algorithms hold the promise to address meaningful quantum problems already on noisy intermediate-scale quantum hardware, but they face the challenge of designing quantum circuits that both solve the target problem and comply with device limitations. Quantum architecture search (QAS) automates this design process, with reinforcement learning (RL) emerging as a promising approach. Yet, RL-based QAS methods encounter significant scalability issues, as computational and training costs grow rapidly with the number of qubits, circuit depth, and noise, severely impacting performance. To address these challenges, we introduce $\textit{TensorRL-QAS}$, a scalable framework that combines tensor network (TN) methods with RL for designing quantum circuits. By warm-starting the architecture search with a matrix product state approximation of the target solution, TensorRL-QAS effectively narrows the search space to physically meaningful circuits, accelerating convergence to the desired solution. Tested on several quantum chemistry problems of up to 12-qubit, TensorRL-QAS achieves up to a 10-fold reduction in CNOT count and circuit depth compared to baseline methods, while maintaining or surpassing chemical accuracy. It reduces function evaluations by up to 100-fold, accelerates training episodes by up to $98\%$, and achieves up to $50\%$ success probability for 10-qubit systems-far exceeding the $<1\%$ rates of baseline approaches. Robustness and versatility are demonstrated both in the noiseless and noisy scenarios, where we report a simulation of up to 8-qubit. These advancements establish TensorRL-QAS as a promising candidate for a scalable and efficient quantum circuit discovery protocol on near-term quantum hardware.





## Scent of Knowledge: Optimizing Search-Enhanced Reasoning with Information Foraging
- **Url**: http://arxiv.org/abs/2505.09316v1
- **Authors**: ['Hongjin Qian', 'Zheng Liu']
- **Abstrat**: Augmenting large language models (LLMs) with external retrieval has become a standard method to address their inherent knowledge cutoff limitations. However, traditional retrieval-augmented generation methods employ static, pre-inference retrieval strategies, making them inadequate for complex tasks involving ambiguous, multi-step, or evolving information needs. Recent advances in test-time scaling techniques have demonstrated significant potential in enabling LLMs to dynamically interact with external tools, motivating the shift toward adaptive inference-time retrieval. Inspired by Information Foraging Theory (IFT), we propose InForage, a reinforcement learning framework that formalizes retrieval-augmented reasoning as a dynamic information-seeking process. Unlike existing approaches, InForage explicitly rewards intermediate retrieval quality, encouraging LLMs to iteratively gather and integrate information through adaptive search behaviors. To facilitate training, we construct a human-guided dataset capturing iterative search and reasoning trajectories for complex, real-world web tasks. Extensive evaluations across general question answering, multi-hop reasoning tasks, and a newly developed real-time web QA dataset demonstrate InForage's superior performance over baseline methods. These results highlight InForage's effectiveness in building robust, adaptive, and efficient reasoning agents.





## UI-R1: Enhancing Efficient Action Prediction of GUI Agents by Reinforcement Learning
- **Url**: http://arxiv.org/abs/2503.21620v4
- **Authors**: ['Zhengxi Lu', 'Yuxiang Chai', 'Yaxuan Guo', 'Xi Yin', 'Liang Liu', 'Hao Wang', 'Han Xiao', 'Shuai Ren', 'Guanjing Xiong', 'Hongsheng Li']
- **Abstrat**: The recent DeepSeek-R1 has showcased the emergence of reasoning capabilities in LLMs through reinforcement learning (RL) with rule-based rewards. Despite its success in language models, its application in multi-modal domains, particularly in graphic user interface (GUI) agent tasks, remains under-explored. To address this issue, we propose UI-R1, the first framework to explore how rule-based RL can enhance the reasoning capabilities of multimodal large language models (MLLMs) for GUI action prediction tasks. Specifically, UI-R1 introduces a novel rule-based action reward, enabling model optimization via policy-based algorithms such as Group Relative Policy Optimization (GRPO). For efficient training, we curate a small yet high-quality dataset of 136 challenging tasks, encompassing five common action types on mobile devices. Experimental results demonstrate that our proposed UI-R1-3B achieves significant improvements over the base model (i.e. Qwen2.5-VL-3B) on both in-domain (ID) and out-of-domain (OOD) tasks, with average accuracy gains of 22.1% on ScreenSpot, 6.0% on ScreenSpot-Pro, and 12.7% on ANDROIDCONTROL. Furthermore, UI-R1-3B delivers competitive performance compared to larger models (e.g., OS-Atlas-7B) trained via supervised fine-tuning (SFT) on 76K samples. We additionally develop an optimized version, UI-R1-E-3B, which significantly improves both grounding efficiency and accuracy. These results underscore the potential of rule-based reinforcement learning to advance GUI understanding and control, paving the way for future research in this domain. Code website: https://github.com/lll6gg/UI-R1.





## Neural Multivariate Regression: Qualitative Insights from the Unconstrained Feature Model
- **Url**: http://arxiv.org/abs/2505.09308v1
- **Authors**: ['George Andriopoulos', 'Soyuj Jung Basnet', 'Juan Guevara', 'Li Guo', 'Keith Ross']
- **Abstrat**: The Unconstrained Feature Model (UFM) is a mathematical framework that enables closed-form approximations for minimal training loss and related performance measures in deep neural networks (DNNs). This paper leverages the UFM to provide qualitative insights into neural multivariate regression, a critical task in imitation learning, robotics, and reinforcement learning. Specifically, we address two key questions: (1) How do multi-task models compare to multiple single-task models in terms of training performance? (2) Can whitening and normalizing regression targets improve training performance? The UFM theory predicts that multi-task models achieve strictly smaller training MSE than multiple single-task models when the same or stronger regularization is applied to the latter, and our empirical results confirm these findings. Regarding whitening and normalizing regression targets, the UFM theory predicts that they reduce training MSE when the average variance across the target dimensions is less than one, and our empirical results once again confirm these findings. These findings highlight the UFM as a powerful framework for deriving actionable insights into DNN design and data pre-processing strategies.





## Video-R1: Reinforcing Video Reasoning in MLLMs
- **Url**: http://arxiv.org/abs/2503.21776v2
- **Authors**: ['Kaituo Feng', 'Kaixiong Gong', 'Bohao Li', 'Zonghao Guo', 'Yibing Wang', 'Tianshuo Peng', 'Junfei Wu', 'Xiaoying Zhang', 'Benyou Wang', 'Xiangyu Yue']
- **Abstrat**: Inspired by DeepSeek-R1's success in eliciting reasoning abilities through rule-based reinforcement learning (RL), we introduce Video-R1 as the first attempt to systematically explore the R1 paradigm for incentivizing video reasoning within multimodal large language models (MLLMs). However, directly applying RL training with the GRPO algorithm to video reasoning presents two primary challenges: (i) a lack of temporal modeling for video reasoning, and (ii) the scarcity of high-quality video-reasoning data. To address these issues, we first propose the T-GRPO algorithm, which encourages models to utilize temporal information in videos for reasoning. Additionally, instead of relying solely on video data, we incorporate high-quality image-reasoning data into the training process. We have constructed two datasets: Video-R1-CoT-165k for SFT cold start and Video-R1-260k for RL training, both comprising image and video data. Experimental results demonstrate that Video-R1 achieves significant improvements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as well as on general video benchmarks including MVBench and TempCompass, etc. Notably, Video-R1-7B attains a 37.1% accuracy on video spatial reasoning benchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. All code, models, and data are released in: https://github.com/tulerfeng/Video-R1.





## A drone that learns to efficiently find objects in agricultural fields: from simulation to the real world
- **Url**: http://arxiv.org/abs/2505.09278v1
- **Authors**: ['Rick van Essen', 'Gert Kootstra']
- **Abstrat**: Drones are promising for data collection in precision agriculture, however, they are limited by their battery capacity. Efficient path planners are therefore required. This paper presents a drone path planner trained using Reinforcement Learning (RL) on an abstract simulation that uses object detections and uncertain prior knowledge. The RL agent controls the flight direction and can terminate the flight. By using the agent in combination with the drone's flight controller and a detection network to process camera images, it is possible to evaluate the performance of the agent on real-world data. In simulation, the agent yielded on average a 78% shorter flight path compared to a full coverage planner, at the cost of a 14% lower recall. On real-world data, the agent showed a 72% shorter flight path compared to a full coverage planner, however, at the cost of a 25% lower recall. The lower performance on real-world data was attributed to the real-world object distribution and the lower accuracy of prior knowledge, and shows potential for improvement. Overall, we concluded that for applications where it is not crucial to find all objects, such as weed detection, the learned-based path planner is suitable and efficient.





# TD3
# Prioritized Experience Replay
# path planning
## Camera-Only 3D Panoptic Scene Completion for Autonomous Driving through Differentiable Object Shapes
- **Url**: http://arxiv.org/abs/2505.09562v1
- **Authors**: ['Nicola Marinello', 'Simen Cassiman', 'Jonas Heylen', 'Marc Proesmans', 'Luc Van Gool']
- **Abstrat**: Autonomous vehicles need a complete map of their surroundings to plan and act. This has sparked research into the tasks of 3D occupancy prediction, 3D scene completion, and 3D panoptic scene completion, which predict a dense map of the ego vehicle's surroundings as a voxel grid. Scene completion extends occupancy prediction by predicting occluded regions of the voxel grid, and panoptic scene completion further extends this task by also distinguishing object instances within the same class; both aspects are crucial for path planning and decision-making. However, 3D panoptic scene completion is currently underexplored. This work introduces a novel framework for 3D panoptic scene completion that extends existing 3D semantic scene completion models. We propose an Object Module and Panoptic Module that can easily be integrated with 3D occupancy and scene completion methods presented in the literature. Our approach leverages the available annotations in occupancy benchmarks, allowing individual object shapes to be learned as a differentiable problem. The code is available at https://github.com/nicolamarinello/OffsetOcc .





## aUToPath: Unified Planning and Control for Autonomous Vehicles in Urban Environments Using Hybrid Lattice and Free-Space Search
- **Url**: http://arxiv.org/abs/2505.09475v1
- **Authors**: ['Tanmay P. Patel', 'Connor Wilson', 'Ellina R. Zhang', 'Morgan Tran', 'Chang Keun Paik', 'Steven L. Waslander', 'Timothy D. Barfoot']
- **Abstrat**: This paper presents aUToPath, a unified online framework for global path-planning and control to address the challenge of autonomous navigation in cluttered urban environments. A key component of our framework is a novel hybrid planner that combines pre-computed lattice maps with dynamic free-space sampling to efficiently generate optimal driveable corridors in cluttered scenarios. Our system also features sequential convex programming (SCP)-based model predictive control (MPC) to refine the corridors into smooth, dynamically consistent trajectories. A single optimization problem is used to both generate a trajectory and its corresponding control commands; this addresses limitations of decoupled approaches by guaranteeing a safe and feasible path. Simulation results of the novel planner on randomly generated obstacle-rich scenarios demonstrate the success rate of a free-space Adaptively Informed Trees* (AIT*)-based planner, and runtimes comparable to a lattice-based planner. Real-world experiments of the full system on a Chevrolet Bolt EUV further validate performance in dense obstacle fields, demonstrating no violations of traffic, kinematic, or vehicle constraints, and a 100% success rate across eight trials.





## SafePath: Conformal Prediction for Safe LLM-Based Autonomous Navigation
- **Url**: http://arxiv.org/abs/2505.09427v1
- **Authors**: ['Achref Doula', 'Max Mühläuser', 'Alejandro Sanchez Guinea']
- **Abstrat**: Large Language Models (LLMs) show growing promise in autonomous driving by reasoning over complex traffic scenarios to generate path plans. However, their tendencies toward overconfidence, and hallucinations raise critical safety concerns. We introduce SafePath, a modular framework that augments LLM-based path planning with formal safety guarantees using conformal prediction. SafePath operates in three stages. In the first stage, we use an LLM that generates a set of diverse candidate paths, exploring possible trajectories based on agent behaviors and environmental cues. In the second stage, SafePath filters out high-risk trajectories while guaranteeing that at least one safe option is included with a user-defined probability, through a multiple-choice question-answering formulation that integrates conformal prediction. In the final stage, our approach selects the path with the lowest expected collision risk when uncertainty is low or delegates control to a human when uncertainty is high. We theoretically prove that SafePath guarantees a safe trajectory with a user-defined probability, and we show how its human delegation rate can be tuned to balance autonomy and safety. Extensive experiments on nuScenes and Highway-env show that SafePath reduces planning uncertainty by 77\% and collision rates by up to 70\%, demonstrating effectiveness in making LLM-driven path planning more safer.





## VIMPPI: Enhancing Model Predictive Path Integral Control with Variational Integration for Underactuated Systems
- **Url**: http://arxiv.org/abs/2505.05507v2
- **Authors**: ['Igor Alentev', 'Lev Kozlov', 'Ivan Domrachev', 'Simeon Nedelchev', 'Jee-Hwan Ryu']
- **Abstrat**: This paper presents VIMPPI, a novel control approach for underactuated double pendulum systems developed for the AI Olympics competition. We enhance the Model Predictive Path Integral framework by incorporating variational integration techniques, enabling longer planning horizons without additional computational cost. Operating at 500-700 Hz with control interpolation and disturbance detection mechanisms, VIMPPI substantially outperforms both baseline methods and alternative MPPI implementations




