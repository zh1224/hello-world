# reinforcement learning
## Exploring the Limit of Outcome Reward for Learning Mathematical Reasoning
- **Url**: http://arxiv.org/abs/2502.06781v1
- **Authors**: ['Chengqi Lyu', 'Songyang Gao', 'Yuzhe Gu', 'Wenwei Zhang', 'Jianfei Gao', 'Kuikun Liu', 'Ziyi Wang', 'Shuaibin Li', 'Qian Zhao', 'Haian Huang', 'Weihan Cao', 'Jiangning Liu', 'Hongwei Liu', 'Junnan Liu', 'Songyang Zhang', 'Dahua Lin', 'Kai Chen']
- **Abstrat**: Reasoning abilities, especially those for solving complex math problems, are crucial components of general intelligence. Recent advances by proprietary companies, such as o-series models of OpenAI, have made remarkable progress on reasoning tasks. However, the complete technical details remain unrevealed, and the techniques that are believed certainly to be adopted are only reinforcement learning (RL) and the long chain of thoughts. This paper proposes a new RL framework, termed OREAL, to pursue the performance limit that can be achieved through \textbf{O}utcome \textbf{RE}w\textbf{A}rd-based reinforcement \textbf{L}earning for mathematical reasoning tasks, where only binary outcome rewards are easily accessible. We theoretically prove that behavior cloning on positive trajectories from best-of-N (BoN) sampling is sufficient to learn the KL-regularized optimal policy in binary feedback environments. This formulation further implies that the rewards of negative samples should be reshaped to ensure the gradient consistency between positive and negative samples. To alleviate the long-existing difficulties brought by sparse rewards in RL, which are even exacerbated by the partial correctness of the long chain of thought for reasoning tasks, we further apply a token-level reward model to sample important tokens in reasoning trajectories for learning. With OREAL, for the first time, a 7B model can obtain 94.0 pass@1 accuracy on MATH-500 through RL, being on par with 32B models. OREAL-32B also surpasses previous 32B models trained by distillation with 95.0 pass@1 accuracy on MATH-500. Our investigation also indicates the importance of initial policy models and training queries for RL. Code, models, and data will be released to benefit future research\footnote{https://github.com/InternLM/OREAL}.





## On the Emergence of Thinking in LLMs I: Searching for the Right Intuition
- **Url**: http://arxiv.org/abs/2502.06773v1
- **Authors**: ['Guanghao Ye', 'Khiem Duc Pham', 'Xinzhi Zhang', 'Sivakanth Gopi', 'Baolin Peng', 'Beibin Li', 'Janardhan Kulkarni', 'Huseyin A. Inan']
- **Abstrat**: Recent AI advancements, such as OpenAI's new models, are transforming LLMs into LRMs (Large Reasoning Models) that perform reasoning during inference, taking extra time and compute for higher-quality outputs. We aim to uncover the algorithmic framework for training LRMs. Methods like self-consistency, PRM, and AlphaZero suggest reasoning as guided search. We ask: what is the simplest, most scalable way to enable search in LLMs?   We propose a post-training framework called Reinforcement Learning via Self-Play (RLSP). RLSP involves three steps: (1) supervised fine-tuning with human or synthetic demonstrations of the reasoning process, (2) using an exploration reward signal to encourage diverse and efficient reasoning behaviors, and (3) RL training with an outcome verifier to ensure correctness while preventing reward hacking. Our key innovation is to decouple exploration and correctness signals during PPO training, carefully balancing them to improve performance and efficiency.   Empirical studies in the math domain show that RLSP improves reasoning. On the Llama-3.1-8B-Instruct model, RLSP can boost performance by 23% in MATH-500 test set; On AIME 2024 math problems, Qwen2.5-32B-Instruct improved by 10% due to RLSP. However, a more important finding of this work is that the models trained using RLSP, even with the simplest exploration reward that encourages the model to take more intermediate steps, showed several emergent behaviors such as backtracking, exploration of ideas, and verification. These findings demonstrate that RLSP framework might be enough to enable emergence of complex reasoning abilities in LLMs when scaled. Lastly, we propose a theory as to why RLSP search strategy is more suitable for LLMs inspired by a remarkable result that says CoT provably increases computational power of LLMs, which grows as the number of steps in CoT \cite{li2024chain,merrill2023expresssive}.





## ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates
- **Url**: http://arxiv.org/abs/2502.06772v1
- **Authors**: ['Ling Yang', 'Zhaochen Yu', 'Bin Cui', 'Mengdi Wang']
- **Abstrat**: We present that hierarchical LLM reasoning via scaling thought templates can effectively optimize the reasoning search space and outperform the mathematical reasoning capabilities of powerful LLMs like OpenAI o1-preview and DeepSeek V3. We train our ReasonFlux-32B model with only 8 GPUs and introduces three innovations: (i) a structured and generic thought template library, containing around 500 high-level thought templates capable of generalizing to similar or relevant reasoning problems; (ii) performing hierarchical reinforcement learning on a sequence of thought templates instead of long CoTs, optimizing a base LLM to plan out an optimal template trajectory for gradually handling complex problems; (iii) a brand new inference scaling system that enables hierarchical LLM reasoning by adaptively scaling thought templates at inference time. With a template trajectory containing sequential thought templates, our ReasonFlux-32B significantly advances math reasoning capabilities to state-of-the-art levels. Notably, on the MATH benchmark, it achieves an accuracy of 91.2% and surpasses o1-preview by 6.7%. On the USA Math Olympiad (AIME) benchmark, ReasonFlux-32B solves an average of 56.7% of problems, surpassing o1-preview and DeepSeek-V3 by 27% and 45%, respectively. Code: https://github.com/Gen-Verse/ReasonFlux





## Development of COVID-19 Booster Vaccine Policy by Microsimulation and Q-learning
- **Url**: http://arxiv.org/abs/2410.12936v3
- **Authors**: ['Guoxuan Ma', 'Lili Zhao', 'Jian Kang']
- **Abstrat**: The COVID-19 pandemic highlighted the urgent need for effective vaccine policies, but traditional clinical trials often lack sufficient data to capture the diverse population characteristics necessary for comprehensive public health strategies. Ethical concerns around randomized trials during a pandemic further complicate policy development for public health. Reinforcement Learning (RL) offers a promising alternative for vaccine policy development. However, direct online RL exploration in real-world scenarios can result in suboptimal and potentially harmful decisions. This study proposes a novel framework combining tabular Q-learning with microsimulation (i.e., a Recurrent Neural Network (RNN) environment simulator) to address these challenges in public health vaccine policymaking, which enables effective vaccine policy learning without real-world interaction, addressing both ethical and exploration challenges. The RNN environment simulator captures temporal associations between infection and patient characteristics, generating realistic simulation data. Our tabular Q-learning model produces an interpretable policy table that balances the risks of severe infection against vaccination side effects. Applied to COVID-19 booster policies, the learned Q-learning-based policy outperforms current practices, offering a path toward more effective vaccination strategies.





## ACECODER: Acing Coder RL via Automated Test-Case Synthesis
- **Url**: http://arxiv.org/abs/2502.01718v3
- **Authors**: ['Huaye Zeng', 'Dongfu Jiang', 'Haozhe Wang', 'Ping Nie', 'Xiaotong Chen', 'Wenhu Chen']
- **Abstrat**: Most progress in recent coder models has been driven by supervised fine-tuning (SFT), while the potential of reinforcement learning (RL) remains largely unexplored, primarily due to the lack of reliable reward data/model in the code domain. In this paper, we address this challenge by leveraging automated large-scale test-case synthesis to enhance code model training. Specifically, we design a pipeline that generates extensive (question, test-cases) pairs from existing code data. Using these test cases, we construct preference pairs based on pass rates over sampled programs to train reward models with Bradley-Terry loss. It shows an average of 10-point improvement for Llama-3.1-8B-Ins and 5-point improvement for Qwen2.5-Coder-7B-Ins through best-of-32 sampling, making the 7B model on par with 236B DeepSeek-V2.5. Furthermore, we conduct reinforcement learning with both reward models and test-case pass rewards, leading to consistent improvements across HumanEval, MBPP, BigCodeBench, and LiveCodeBench (V4). Notably, we follow the R1-style training to start from Qwen2.5-Coder-base directly and show that our RL training can improve model on HumanEval-plus by over 25\% and MBPP-plus by 6\% for merely 80 optimization steps. We believe our results highlight the huge potential of reinforcement learning in coder models.





## Diverse Preference Optimization
- **Url**: http://arxiv.org/abs/2501.18101v3
- **Authors**: ['Jack Lanchantin', 'Angelica Chen', 'Shehzaad Dhuliawala', 'Ping Yu', 'Jason Weston', 'Sainbayar Sukhbaatar', 'Ilia Kulikov']
- **Abstrat**: Post-training of language models, either through reinforcement learning, preference optimization or supervised finetuning, tends to sharpen the output probability distribution and reduce the diversity of generated responses. This is particularly a problem for creative generative tasks where varied responses are desired. In this work we introduce Diverse Preference Optimization (DivPO), an optimization method which learns to generate much more diverse responses than standard pipelines, while maintaining the quality of the generations. In DivPO, preference pairs are selected by first considering a pool of responses, and a measure of diversity among them, and selecting chosen examples as being more rare but high quality, while rejected examples are more common, but low quality. DivPO results in generating 45.6% more diverse persona attributes, and an 74.6% increase in story diversity, while maintaining similar win rates as standard baselines.





## AgilePilot: DRL-Based Drone Agent for Real-Time Motion Planning in Dynamic Environments by Leveraging Object Detection
- **Url**: http://arxiv.org/abs/2502.06725v1
- **Authors**: ['Roohan Ahmed Khan', 'Valerii Serpiva', 'Demetros Aschalew', 'Aleksey Fedoseev', 'Dzmitry Tsetserukou']
- **Abstrat**: Autonomous drone navigation in dynamic environments remains a critical challenge, especially when dealing with unpredictable scenarios including fast-moving objects with rapidly changing goal positions. While traditional planners and classical optimisation methods have been extensively used to address this dynamic problem, they often face real-time, unpredictable changes that ultimately leads to sub-optimal performance in terms of adaptiveness and real-time decision making. In this work, we propose a novel motion planner, AgilePilot, based on Deep Reinforcement Learning (DRL) that is trained in dynamic conditions, coupled with real-time Computer Vision (CV) for object detections during flight. The training-to-deployment framework bridges the Sim2Real gap, leveraging sophisticated reward structures that promotes both safety and agility depending upon environment conditions. The system can rapidly adapt to changing environments, while achieving a maximum speed of 3.0 m/s in real-world scenarios. In comparison, our approach outperforms classical algorithms such as Artificial Potential Field (APF) based motion planner by 3 times, both in performance and tracking accuracy of dynamic targets by using velocity predictions while exhibiting 90% success rate in 75 conducted experiments. This work highlights the effectiveness of DRL in tackling real-time dynamic navigation challenges, offering intelligent safety and agility.





## Discovery of skill switching criteria for learning agile quadruped locomotion
- **Url**: http://arxiv.org/abs/2502.06676v1
- **Authors**: ['Wanming Yu', 'Fernando Acero', 'Vassil Atanassov', 'Chuanyu Yang', 'Ioannis Havoutis', 'Dimitrios Kanoulas', 'Zhibin Li']
- **Abstrat**: This paper develops a hierarchical learning and optimization framework that can learn and achieve well-coordinated multi-skill locomotion. The learned multi-skill policy can switch between skills automatically and naturally in tracking arbitrarily positioned goals and recover from failures promptly. The proposed framework is composed of a deep reinforcement learning process and an optimization process. First, the contact pattern is incorporated into the reward terms for learning different types of gaits as separate policies without the need for any other references. Then, a higher level policy is learned to generate weights for individual policies to compose multi-skill locomotion in a goal-tracking task setting. Skills are automatically and naturally switched according to the distance to the goal. The proper distances for skill switching are incorporated in reward calculation for learning the high level policy and updated by an outer optimization loop as learning progresses. We first demonstrated successful multi-skill locomotion in comprehensive tasks on a simulated Unitree A1 quadruped robot. We also deployed the learned policy in the real world showcasing trotting, bounding, galloping, and their natural transitions as the goal position changes. Moreover, the learned policy can react to unexpected failures at any time, perform prompt recovery, and resume locomotion successfully. Compared to discrete switch between single skills which failed to transition to galloping in the real world, our proposed approach achieves all the learned agile skills, with smoother and more continuous skill transitions.





## Free Agent in Agent-Based Mixture-of-Experts Generative AI Framework
- **Url**: http://arxiv.org/abs/2501.17903v2
- **Authors**: ['Jung-Hua Liu']
- **Abstrat**: Multi-agent systems commonly distribute tasks among specialized, autonomous agents, yet they often lack mechanisms to replace or reassign underperforming agents in real time. Inspired by the free-agency model of Major League Baseball, the Reinforcement Learning Free Agent (RLFA) algorithm introduces a reward-based mechanism to detect and remove agents exhibiting persistent underperformance and seamlessly insert more capable ones. Each agent internally uses a mixture-of-experts (MoE) approach, delegating incoming tasks to specialized sub-models under the guidance of a gating function. A primary use case is fraud detection, where RLFA promptly swaps out an agent whose detection accuracy dips below a preset threshold. A new agent is tested in a probationary mode, and upon demonstrating superior performance, fully replaces the underperformer. This dynamic, free-agency cycle ensures sustained accuracy, quicker adaptation to emerging threats, and minimal disruption to ongoing operations. By continually refreshing its roster of agents, the system fosters ongoing improvements and more resilient collaboration in multi-agent Generative AI environments.





## Deep Reinforcement Learning based Triggering Function for Early Classifiers of Time Series
- **Url**: http://arxiv.org/abs/2502.06584v1
- **Authors**: ['Aurélien Renault', 'Alexis Bondu', 'Antoine Cornuéjols', 'Vincent Lemaire']
- **Abstrat**: Early Classification of Time Series (ECTS) has been recognized as an important problem in many areas where decisions have to be taken as soon as possible, before the full data availability, while time pressure increases. Numerous ECTS approaches have been proposed, based on different triggering functions, each taking into account various pieces of information related to the incoming time series and/or the output of a classifier. Although their performances have been empirically compared in the literature, no studies have been carried out on the optimality of these triggering functions that involve ``man-tailored'' decision rules. Based on the same information, could there be better triggering functions? This paper presents one way to investigate this question by showing first how to translate ECTS problems into Reinforcement Learning (RL) ones, where the very same information is used in the state space. A thorough comparison of the performance obtained by ``handmade'' approaches and their ``RL-based'' counterparts has been carried out. A second question investigated in this paper is whether a different combination of information, defining the state space in RL systems, can achieve even better performance. Experiments show that the system we describe, called \textsc{Alert}, significantly outperforms its state-of-the-art competitors on a large number of datasets.





## Calibrated Unsupervised Anomaly Detection in Multivariate Time-series using Reinforcement Learning
- **Url**: http://arxiv.org/abs/2502.03245v2
- **Authors**: ['Saba Sanami', 'Amir G. Aghdam']
- **Abstrat**: This paper investigates unsupervised anomaly detection in multivariate time-series data using reinforcement learning (RL) in the latent space of an autoencoder. A significant challenge is the limited availability of anomalous data, often leading to misclassifying anomalies as normal events, thus raising false negatives. RL can help overcome this limitation by promoting exploration and balancing exploitation during training, effectively preventing overfitting. Wavelet analysis is also utilized to enhance anomaly detection, enabling time-series data decomposition into both time and frequency domains. This approach captures anomalies at multiple resolutions, with wavelet coefficients extracted to detect both sudden and subtle shifts in the data, thereby refining the anomaly detection process. We calibrate the decision boundary by generating synthetic anomalies and embedding a supervised framework within the model. This supervised element aids the unsupervised learning process by fine-tuning the decision boundary and increasing the model's capacity to distinguish between normal and anomalous patterns effectively.





## Closed-form Solutions: A New Perspective on Solving Differential Equations
- **Url**: http://arxiv.org/abs/2405.14620v2
- **Authors**: ['Shu Wei', 'Yanjie Li', 'Lina Yu', 'Weijun Li', 'Min Wu', 'Linjun Sun', 'Jufeng Han', 'Yan Pang']
- **Abstrat**: The pursuit of analytical solutions for differential equations has historically been limited by the need for extensive prior knowledge and mathematical prowess; however, machine learning methods like genetic algorithms have recently been applied to this end, albeit with issues of significant time consumption and complexity. This paper presents a novel machine learning-based solver, SSDE (Symbolic Solver for Differential Equations), which employs reinforcement learning to derive symbolic closed-form solutions for various differential equations. Our evaluations on a range of ordinary and partial differential equations demonstrate that SSDE provides superior performance in achieving analytical solutions compared to other machine learning approaches.





## Ignore the KL Penalty! Boosting Exploration on Critical Tokens to Enhance RL Fine-Tuning
- **Url**: http://arxiv.org/abs/2502.06533v1
- **Authors**: ['Jean Vassoyan', 'Nathanaël Beau', 'Roman Plaud']
- **Abstrat**: The ability to achieve long-term goals is a key challenge in the current development of large language models (LLMs). To address this, pre-trained LLMs can be fine-tuned with reinforcement learning (RL) to explore solutions that optimize a given goal. However, exploration with LLMs is difficult, as a balance has to be struck between discovering new solutions and staying close enough to the pre-trained model, so as not to degrade basic capabilities. This is typically controlled with a Kullback-Leibler (KL) penalty. In this paper, we investigate the exploration dynamics of a small language model on a simple arithmetic task. We show how varying degrees of pre-training influence exploration and demonstrate the importance of "critical tokens" which have a dramatic impact on the final outcome. Consequently, we introduce a simple modification to the KL penalty that favors exploration on critical tokens, increasing the efficiency of the RL fine-tuning stage.





## Model-Based Offline Reinforcement Learning with Reliability-Guaranteed Sequence Modeling
- **Url**: http://arxiv.org/abs/2502.06491v1
- **Authors**: ['Shenghong He']
- **Abstrat**: Model-based offline reinforcement learning (MORL) aims to learn a policy by exploiting a dynamics model derived from an existing dataset. Applying conservative quantification to the dynamics model, most existing works on MORL generate trajectories that approximate the real data distribution to facilitate policy learning by using current information (e.g., the state and action at time step $t$). However, these works neglect the impact of historical information on environmental dynamics, leading to the generation of unreliable trajectories that may not align with the real data distribution. In this paper, we propose a new MORL algorithm \textbf{R}eliability-guaranteed \textbf{T}ransformer (RT), which can eliminate unreliable trajectories by calculating the cumulative reliability of the generated trajectory (i.e., using a weighted variational distance away from the real data). Moreover, by sampling candidate actions with high rewards, RT can efficiently generate high-return trajectories from the existing offline data. We theoretically prove the performance guarantees of RT in policy learning, and empirically demonstrate its effectiveness against state-of-the-art model-based methods on several benchmark tasks.





## Automated Data Augmentation for Few-Shot Time Series Forecasting: A Reinforcement Learning Approach Guided by a Model Zoo
- **Url**: http://arxiv.org/abs/2409.06282v2
- **Authors**: ['Haochen Yuan', 'Yutong Wang', 'Yihong Chen', 'Yunbo Wang']
- **Abstrat**: Time series forecasting, particularly in few-shot learning scenarios, is challenging due to the limited availability of high-quality training data. To address this, we present a pilot study on using reinforcement learning (RL) for time series data augmentation. Our method, ReAugment, tackles three critical questions: which parts of the training set should be augmented, how the augmentation should be performed, and what advantages RL brings to the process. Specifically, our approach maintains a forecasting model zoo, and by measuring prediction diversity across the models, we identify samples with higher probabilities for overfitting and use them as the anchor points for augmentation. Leveraging RL, our method adaptively transforms the overfit-prone samples into new data that not only enhances training set diversity but also directs the augmented data to target regions where the forecasting models are prone to overfitting. We validate the effectiveness of ReAugment across a wide range of base models, showing its advantages in both standard time series forecasting and few-shot learning tasks.





## STRIDE: Automating Reward Design, Deep Reinforcement Learning Training and Feedback Optimization in Humanoid Robotics Locomotion
- **Url**: http://arxiv.org/abs/2502.04692v2
- **Authors**: ['Zhenwei Wu', 'Jinxiong Lu', 'Yuxiao Chen', 'Yunxin Liu', 'Yueting Zhuang', 'Luhui Hu']
- **Abstrat**: Humanoid robotics presents significant challenges in artificial intelligence, requiring precise coordination and control of high-degree-of-freedom systems. Designing effective reward functions for deep reinforcement learning (DRL) in this domain remains a critical bottleneck, demanding extensive manual effort, domain expertise, and iterative refinement. To overcome these challenges, we introduce STRIDE, a novel framework built on agentic engineering to automate reward design, DRL training, and feedback optimization for humanoid robot locomotion tasks. By combining the structured principles of agentic engineering with large language models (LLMs) for code-writing, zero-shot generation, and in-context optimization, STRIDE generates, evaluates, and iteratively refines reward functions without relying on task-specific prompts or templates. Across diverse environments featuring humanoid robot morphologies, STRIDE outperforms the state-of-the-art reward design framework EUREKA, achieving significant improvements in efficiency and task performance. Using STRIDE-generated rewards, simulated humanoid robots achieve sprint-level locomotion across complex terrains, highlighting its ability to advance DRL workflows and humanoid robotics research.





## SIGMA: Sheaf-Informed Geometric Multi-Agent Pathfinding
- **Url**: http://arxiv.org/abs/2502.06440v1
- **Authors**: ['Shuhao Liao', 'Weihang Xia', 'Yuhong Cao', 'Weiheng Dai', 'Chengyang He', 'Wenjun Wu', 'Guillaume Sartoretti']
- **Abstrat**: The Multi-Agent Path Finding (MAPF) problem aims to determine the shortest and collision-free paths for multiple agents in a known, potentially obstacle-ridden environment. It is the core challenge for robotic deployments in large-scale logistics and transportation. Decentralized learning-based approaches have shown great potential for addressing the MAPF problems, offering more reactive and scalable solutions. However, existing learning-based MAPF methods usually rely on agents making decisions based on a limited field of view (FOV), resulting in short-sighted policies and inefficient cooperation in complex scenarios. There, a critical challenge is to achieve consensus on potential movements between agents based on limited observations and communications. To tackle this challenge, we introduce a new framework that applies sheaf theory to decentralized deep reinforcement learning, enabling agents to learn geometric cross-dependencies between each other through local consensus and utilize them for tightly cooperative decision-making. In particular, sheaf theory provides a mathematical proof of conditions for achieving global consensus through local observation. Inspired by this, we incorporate a neural network to approximately model the consensus in latent space based on sheaf theory and train it through self-supervised learning. During the task, in addition to normal features for MAPF as in previous works, each agent distributedly reasons about a learned consensus feature, leading to efficient cooperation on pathfinding and collision avoidance. As a result, our proposed method demonstrates significant improvements over state-of-the-art learning-based MAPF planners, especially in relatively large and complex scenarios, demonstrating its superiority over baselines in various simulations and real-world robot experiments.





## Habitizing Diffusion Planning for Efficient and Effective Decision Making
- **Url**: http://arxiv.org/abs/2502.06401v1
- **Authors**: ['Haofei Lu', 'Yifei Shen', 'Dongsheng Li', 'Junliang Xing', 'Dongqi Han']
- **Abstrat**: Diffusion models have shown great promise in decision-making, also known as diffusion planning. However, the slow inference speeds limit their potential for broader real-world applications. Here, we introduce Habi, a general framework that transforms powerful but slow diffusion planning models into fast decision-making models, which mimics the cognitive process in the brain that costly goal-directed behavior gradually transitions to efficient habitual behavior with repetitive practice. Even using a laptop CPU, the habitized model can achieve an average 800+ Hz decision-making frequency (faster than previous diffusion planners by orders of magnitude) on standard offline reinforcement learning benchmarks D4RL, while maintaining comparable or even higher performance compared to its corresponding diffusion planner. Our work proposes a fresh perspective of leveraging powerful diffusion models for real-world decision-making tasks. We also provide robust evaluations and analysis, offering insights from both biological and engineering perspectives for efficient and effective decision-making.





## RESC: A Reinforcement Learning Based Search-to-Control Framework for Quadrotor Local Planning in Dense Environments
- **Url**: http://arxiv.org/abs/2408.00275v4
- **Authors**: ['Zhaohong Liu', 'Wenxuan Gao', 'Yinshuai Sun', 'Peng Dong']
- **Abstrat**: Agile flight in complex environments poses significant challenges to current motion planning methods, as they often fail to fully leverage the quadrotor dynamic potential, leading to performance failures and reduced efficiency during aggressive maneuvers.Existing approaches frequently decouple trajectory optimization from control generation and neglect the dynamics, further limiting their ability to generate aggressive and feasible motions.To address these challenges, we introduce an enhanced Search-to-Control planning framework that integrates visibility path searching with reinforcement learning (RL) control generation, directly accounting for dynamics and bridging the gap between planning and control.Our method first extracts control points from collision-free paths using a proposed heuristic search, which are then refined by an RL policy to generate low-level control commands for the quadrotor controller, utilizing reduced-dimensional obstacle observations for efficient inference with lightweight neural networks.We validate the framework through simulations and real-world experiments, demonstrating improved time efficiency and dynamic maneuverability compared to existing methods, while confirming its robustness and applicability.





# TD3
# Prioritized Experience Replay
# path planning
## RESC: A Reinforcement Learning Based Search-to-Control Framework for Quadrotor Local Planning in Dense Environments
- **Url**: http://arxiv.org/abs/2408.00275v4
- **Authors**: ['Zhaohong Liu', 'Wenxuan Gao', 'Yinshuai Sun', 'Peng Dong']
- **Abstrat**: Agile flight in complex environments poses significant challenges to current motion planning methods, as they often fail to fully leverage the quadrotor dynamic potential, leading to performance failures and reduced efficiency during aggressive maneuvers.Existing approaches frequently decouple trajectory optimization from control generation and neglect the dynamics, further limiting their ability to generate aggressive and feasible motions.To address these challenges, we introduce an enhanced Search-to-Control planning framework that integrates visibility path searching with reinforcement learning (RL) control generation, directly accounting for dynamics and bridging the gap between planning and control.Our method first extracts control points from collision-free paths using a proposed heuristic search, which are then refined by an RL policy to generate low-level control commands for the quadrotor controller, utilizing reduced-dimensional obstacle observations for efficient inference with lightweight neural networks.We validate the framework through simulations and real-world experiments, demonstrating improved time efficiency and dynamic maneuverability compared to existing methods, while confirming its robustness and applicability.





## AVSim - Realistic Simulation Framework for Airborne and Vector-Borne Disease Dynamics
- **Url**: http://arxiv.org/abs/2502.06212v1
- **Authors**: ['Pandula Thennakoon', 'Mario De Silva', 'M. Mahesha Viduranga', 'Sashini Liyanage', 'Roshan Godaliyadda', 'Mervyn Parakrama Ekanayake', 'Vijitha Herath', 'Anuruddhika Rathnayake', 'Ganga Thilakarathne', 'Janaka Ekanayake', 'Samath Dharmarathne']
- **Abstrat**: The COVID-19 pandemic underscored the critical need for rapid epidemic trend identification and effective intervention strategies to mitigate disease progression and its socio-economic impact. Concurrent with emerging threats, endemic diseases like dengue continue to strain healthcare systems, particularly in populous, economically challenged nations. This paper introduces AVSim (Airborne and Vectorborne Simulator), an agent-based model designed to provide granular insights for optimizing resource allocation within existing healthcare management frameworks. AVSim leverages realistic human mobility and behavioral patterns to simulate disease propagation within a detailed, scalable environment encompassing homes, schools, hospitals, and commercial venues. Human movement is modeled based on occupational and behavioral patterns, including age-specific activities. The simulator incorporates age- and environment-specific disease outcomes, host-host and host-vector interactions, and multiple disease stages, including mild, severe, and critical phases. Immunity, quarantine, and hospitalization are also modeled. Furthermore, AVSim supports tracing the path of disease spread, providing micro-level insights into transmission dynamics. Implemented in Python, AVSim offers flexibility and extensibility, enabling users to create highly customized scenarios for airborne and vector-borne disease modeling. Case studies demonstrating AVSim's application to COVID-19 and dengue illustrate its potential for generating actionable epidemic insights, thereby enhancing public health planning and response.




