# reinforcement learning
## SpatialReasoner: Towards Explicit and Generalizable 3D Spatial Reasoning
- **Url**: http://arxiv.org/abs/2504.20024v1
- **Authors**: ['Wufei Ma', 'Yu-Cheng Chou', 'Qihao Liu', 'Xingrui Wang', 'Celso de Melo', 'Jieneng Chen', 'Jianwen Xie', 'Alan Yuille']
- **Abstrat**: Recent studies in 3D spatial reasoning explore data-driven approaches and achieve enhanced spatial reasoning performance with reinforcement learning (RL). However, these methods typically perform spatial reasoning in an implicit manner, and it remains underexplored whether the acquired 3D knowledge generalizes to unseen question types at any stage of the training. In this work we introduce SpatialReasoner, a novel large vision-language model (LVLM) that address 3D spatial reasoning with explicit 3D representations shared between stages -- 3D perception, computation, and reasoning. Explicit 3D representations provide a coherent interface that supports advanced 3D spatial reasoning and enable us to study the factual errors made by LVLMs. Results show that our SpatialReasoner achieve improved performance on a variety of spatial reasoning benchmarks and generalizes better when evaluating on novel 3D spatial reasoning questions. Our study bridges the 3D parsing capabilities of prior visual foundation models with the powerful reasoning abilities of large language models, opening new directions for 3D spatial reasoning.





## Socially-Aware Autonomous Driving: Inferring Yielding Intentions for Safer Interactions
- **Url**: http://arxiv.org/abs/2504.20004v1
- **Authors**: ['Jing Wang', 'Yan Jin', 'Hamid Taghavifar', 'Fei Ding', 'Chongfeng Wei']
- **Abstrat**: Since the emergence of autonomous driving technology, it has advanced rapidly over the past decade. It is becoming increasingly likely that autonomous vehicles (AVs) would soon coexist with human-driven vehicles (HVs) on the roads. Currently, safety and reliable decision-making remain significant challenges, particularly when AVs are navigating lane changes and interacting with surrounding HVs. Therefore, precise estimation of the intentions of surrounding HVs can assist AVs in making more reliable and safe lane change decision-making. This involves not only understanding their current behaviors but also predicting their future motions without any direct communication. However, distinguishing between the passing and yielding intentions of surrounding HVs still remains ambiguous. To address the challenge, we propose a social intention estimation algorithm rooted in Directed Acyclic Graph (DAG), coupled with a decision-making framework employing Deep Reinforcement Learning (DRL) algorithms. To evaluate the method's performance, the proposed framework can be tested and applied in a lane-changing scenario within a simulated environment. Furthermore, the experiment results demonstrate how our approach enhances the ability of AVs to navigate lane changes safely and efficiently on roads.





## Accurate and Diverse LLM Mathematical Reasoning via Automated PRM-Guided GFlowNets
- **Url**: http://arxiv.org/abs/2504.19981v1
- **Authors**: ['Adam Younsi', 'Abdalgader Abubaker', 'Mohamed El Amine Seddik', 'Hakim Hacid', 'Salem Lahlou']
- **Abstrat**: Achieving both accuracy and diverse reasoning remains challenging for Large Language Models (LLMs) in complex domains like mathematics. A key bottleneck is evaluating intermediate reasoning steps to guide generation without costly human annotations. To address this, we first introduce a novel Process Reward Model (PRM) trained automatically using Monte Carlo Tree Search coupled with a similarity-based data augmentation technique, effectively capturing step-level reasoning quality. Leveraging this PRM, we then adapt Generative Flow Networks (GFlowNets) to operate at the reasoning step level. Unlike traditional reinforcement learning focused on maximizing a single reward, GFlowNets naturally sample diverse, high-quality solutions proportional to their rewards, as measured by our PRM. Empirical evaluation shows strong improvements in both accuracy and solution diversity on challenging mathematical benchmarks (e.g., +2.59% absolute accuracy on MATH Level 5 for Llama3.2-3B), with effective generalization to unseen datasets (+9.4% absolute on SAT MATH). Our work demonstrates the potential of PRM-guided, step-level GFlowNets for developing more robust and versatile mathematical reasoning in LLMs.





## Mesh-Learner: Texturing Mesh with Spherical Harmonics
- **Url**: http://arxiv.org/abs/2504.19938v1
- **Authors**: ['Yunfei Wan', 'Jianheng Liu', 'Jiarong Lin', 'Fu Zhang']
- **Abstrat**: In this paper, we present a 3D reconstruction and rendering framework termed Mesh-Learner that is natively compatible with traditional rasterization pipelines. It integrates mesh and spherical harmonic (SH) texture (i.e., texture filled with SH coefficients) into the learning process to learn each mesh s view-dependent radiance end-to-end. Images are rendered by interpolating surrounding SH Texels at each pixel s sampling point using a novel interpolation method. Conversely, gradients from each pixel are back-propagated to the related SH Texels in SH textures. Mesh-Learner exploits graphic features of rasterization pipeline (texture sampling, deferred rendering) to render, which makes Mesh-Learner naturally compatible with tools (e.g., Blender) and tasks (e.g., 3D reconstruction, scene rendering, reinforcement learning for robotics) that are based on rasterization pipelines. Our system can train vast, unlimited scenes because we transfer only the SH textures within the frustum to the GPU for training. At other times, the SH textures are stored in CPU RAM, which results in moderate GPU memory usage. The rendering results on interpolation and extrapolation sequences in the Replica and FAST-LIVO2 datasets achieve state-of-the-art performance compared to existing state-of-the-art methods (e.g., 3D Gaussian Splatting and M2-Mapping). To benefit the society, the code will be available at https://github.com/hku-mars/Mesh-Learner.





## Automated decision-making for dynamic task assignment at scale
- **Url**: http://arxiv.org/abs/2504.19933v1
- **Authors**: ['Riccardo Lo Bianco', 'Willem van Jaarsveld', 'Jeroen Middelhuis', 'Luca Begnardi', 'Remco Dijkman']
- **Abstrat**: The Dynamic Task Assignment Problem (DTAP) concerns matching resources to tasks in real time while minimizing some objectives, like resource costs or task cycle time. In this work, we consider a DTAP variant where every task is a case composed of a stochastic sequence of activities. The DTAP, in this case, involves the decision of which employee to assign to which activity to process requests as quickly as possible. In recent years, Deep Reinforcement Learning (DRL) has emerged as a promising tool for tackling this DTAP variant, but most research is limited to solving small-scale, synthetic problems, neglecting the challenges posed by real-world use cases. To bridge this gap, this work proposes a DRL-based Decision Support System (DSS) for real-world scale DTAPS. To this end, we introduce a DRL agent with two novel elements: a graph structure for observations and actions that can effectively represent any DTAP and a reward function that is provably equivalent to the objective of minimizing the average cycle time of tasks. The combination of these two novelties allows the agent to learn effective and generalizable assignment policies for real-world scale DTAPs. The proposed DSS is evaluated on five DTAP instances whose parameters are extracted from real-world logs through process mining. The experimental evaluation shows how the proposed DRL agent matches or outperforms the best baseline in all DTAP instances and generalizes on different time horizons and across instances.





## RUMOR: Reinforcement learning for Understanding a Model of the Real World for Navigation in Dynamic Environments
- **Url**: http://arxiv.org/abs/2404.16672v2
- **Authors**: ['Diego Martinez-Baselga', 'Luis Riazuelo', 'Luis Montano']
- **Abstrat**: Autonomous navigation in dynamic environments is a complex but essential task for autonomous robots, with recent deep reinforcement learning approaches showing promising results. However, the complexity of the real world makes it infeasible to train agents in every possible scenario configuration. Moreover, existing methods typically overlook factors such as robot kinodynamic constraints, or assume perfect knowledge of the environment. In this work, we present RUMOR, a novel planner for differential-drive robots that uses deep reinforcement learning to navigate in highly dynamic environments. Unlike other end-to-end DRL planners, it uses a descriptive robocentric velocity space model to extract the dynamic environment information, enhancing training effectiveness and scenario interpretation. Additionally, we propose an action space that inherently considers robot kinodynamics and train it in a simulator that reproduces the real world problematic aspects, reducing the gap between the reality and simulation. We extensively compare RUMOR with other state-of-the-art approaches, demonstrating a better performance, and provide a detailed analysis of the results. Finally, we validate RUMOR's performance in real-world settings by deploying it on a ground robot. Our experiments, conducted in crowded scenarios and unseen environments, confirm the algorithm's robustness and transferability.





## FedSlate:A Federated Deep Reinforcement Learning Recommender System
- **Url**: http://arxiv.org/abs/2409.14872v2
- **Authors**: ['Yongxin Deng', 'Xihe Qiu', 'Xiaoyu Tan', 'Yaochu Jin']
- **Abstrat**: Reinforcement learning methods have been used to optimize long-term user engagement in recommendation systems. However, existing reinforcement learning-based recommendation systems do not fully exploit the relevance of individual user behavior across different platforms. One potential solution is to aggregate data from various platforms in a centralized location and use the aggregated data for training. However, this approach raises economic and legal concerns, including increased communication costs and potential threats to user privacy. To address these challenges, we propose \textbf{FedSlate}, a federated reinforcement learning recommendation algorithm that effectively utilizes information that is prohibited from being shared at a legal level. We employ the SlateQ algorithm to assist FedSlate in learning users' long-term behavior and evaluating the value of recommended content. We extend the existing application scope of recommendation systems from single-user single-platform to single-user multi-platform and address cross-platform learning challenges by introducing federated learning. We use RecSim to construct a simulation environment for evaluating FedSlate and compare its performance with state-of-the-art benchmark recommendation models. Experimental results demonstrate the superior effects of FedSlate over baseline methods in various environmental settings, and FedSlate facilitates the learning of recommendation strategies in scenarios where baseline methods are completely inapplicable. Code is available at \textit{https://github.com/TianYaDY/FedSlate}.





## GenCLS++: Pushing the Boundaries of Generative Classification in LLMs Through Comprehensive SFT and RL Studies Across Diverse Datasets
- **Url**: http://arxiv.org/abs/2504.19898v1
- **Authors**: ['Mingqian He', 'Fei Zhao', 'Chonggang Lu', 'Ziyan Liu', 'Yue Wang', 'Haofu Qian']
- **Abstrat**: As a fundamental task in machine learning, text classification plays a crucial role in many areas. With the rapid scaling of Large Language Models (LLMs), particularly through reinforcement learning (RL), there is a growing need for more capable discriminators. Consequently, advances in classification are becoming increasingly vital for enhancing the overall capabilities of LLMs. Traditional discriminative methods map text to labels but overlook LLMs' intrinsic generative strengths. Generative classification addresses this by prompting the model to directly output labels. However, existing studies still rely on simple SFT alone, seldom probing the interplay between training and inference prompts, and no work has systematically leveraged RL for generative text classifiers and unified SFT, RL, and inference-time prompting in one framework. We bridge this gap with GenCLS++, a framework that jointly optimizes SFT and RL while systematically exploring five high-level strategy dimensions-in-context learning variants, category definitions, explicit uncertainty labels, semantically irrelevant numeric labels, and perplexity-based decoding-during both training and inference. After an SFT "policy warm-up," we apply RL with a simple rule-based reward, yielding sizable extra gains. Across seven datasets, GenCLS++ achieves an average accuracy improvement of 3.46% relative to the naive SFT baseline; on public datasets, this improvement rises to 4.00%. Notably, unlike reasoning-intensive tasks that benefit from explicit thinking processes, we find that classification tasks perform better without such reasoning steps. These insights into the role of explicit reasoning provide valuable guidance for future LLM applications.





## CHARMS: A Cognitive Hierarchical Agent for Reasoning and Motion Stylization in Autonomous Driving
- **Url**: http://arxiv.org/abs/2504.02450v3
- **Authors**: ['Jingyi Wang', 'Duanfeng Chu', 'Zejian Deng', 'Liping Lu', 'Jinxiang Wang', 'Chen Sun']
- **Abstrat**: To address the challenge of insufficient interactivity and behavioral diversity in autonomous driving decision-making, this paper proposes a Cognitive Hierarchical Agent for Reasoning and Motion Stylization (CHARMS). By leveraging Level-k game theory, CHARMS captures human-like reasoning patterns through a two-stage training pipeline comprising reinforcement learning pretraining and supervised fine-tuning. This enables the resulting models to exhibit diverse and human-like behaviors, enhancing their decision-making capacity and interaction fidelity in complex traffic environments. Building upon this capability, we further develop a scenario generation framework that utilizes the Poisson cognitive hierarchy theory to control the distribution of vehicles with different driving styles through Poisson and binomial sampling. Experimental results demonstrate that CHARMS is capable of both making intelligent driving decisions as an ego vehicle and generating diverse, realistic driving scenarios as environment vehicles. The code for CHARMS is released at https://github.com/chuduanfeng/CHARMS.





## I Can Hear You Coming: RF Sensing for Uncooperative Satellite Evasion
- **Url**: http://arxiv.org/abs/2504.03983v2
- **Authors**: ['Cameron Mehlman', 'Gregory Falco']
- **Abstrat**: This work presents a novel method for leveraging intercepted Radio Frequency (RF) signals to inform a constrained Reinforcement Learning (RL) policy for robust control of a satellite operating in contested environments. Uncooperative satellite engagements with nation-state actors prompts the need for enhanced maneuverability and agility on-orbit. However, robust, autonomous and rapid adversary avoidance capabilities for the space environment is seldom studied. Further, the capability constrained nature of many space vehicles does not afford robust space situational awareness capabilities that can be used for well informed maneuvering. We present a "Cat & Mouse" system for training optimal adversary avoidance algorithms using RL. We propose the novel approach of utilizing intercepted radio frequency communication and dynamic spacecraft state as multi-modal input that could inform paths for a mouse to outmaneuver the cat satellite. Given the current ubiquitous use of RF communications, our proposed system can be applicable to a diverse array of satellites. In addition to providing a comprehensive framework for training and implementing a constrained RL policy capable of providing control for robust adversary avoidance, we also explore several optimization based methods for adversarial avoidance. These methods were then tested on real-world data obtained from the Space Surveillance Network (SSN) to analyze the benefits and limitations of different avoidance methods.





## Optimizing the Charging of Open Quantum Batteries using Long Short-Term Memory-Driven Reinforcement Learning
- **Url**: http://arxiv.org/abs/2504.19840v1
- **Authors**: ['Shadab Zakavati', 'Shahriar Salimi', 'Behrouz Arash']
- **Abstrat**: Controlling the charging process of a quantum battery involves strategies to efficiently transfer, store, and retain energy, while mitigating decoherence, energy dissipation, and inefficiencies caused by surrounding interactions. We develop a model to study the charging process of a quantum battery in an open quantum setting, where the battery interacts with a charger and a structured reservoir. To overcome the limitations of static charging protocols, a reinforcement learning (RL) charging strategy is proposed, which utilizes the deep deterministic policy gradient algorithm alongside long short-term memory (LSTM) networks. The LSTM networks enable the RL model to capture temporal correlations driven by non-Markovian dynamics, facilitating a continuous, adaptive charging strategy. The RL protocols consistently outperform conventional fixed heuristic strategies by real-time controlling the driving field amplitude and coupling parameters. By penalizing battery-to-charger backflow in the reward function, the RL-optimized charging strategy promotes efficient unidirectional energy transfer from charger to battery, achieving higher and more stable extractable work. The proposed RL controller would provide a framework for designing efficient charging schemes in broader configurations and multi-cell quantum batteries.





## LLM-Powered GUI Agents in Phone Automation: Surveying Progress and Prospects
- **Url**: http://arxiv.org/abs/2504.19838v1
- **Authors**: ['Guangyi Liu', 'Pengxiang Zhao', 'Liang Liu', 'Yaxuan Guo', 'Han Xiao', 'Weifeng Lin', 'Yuxiang Chai', 'Yue Han', 'Shuai Ren', 'Hao Wang', 'Xiaoyu Liang', 'Wenhao Wang', 'Tianze Wu', 'Linghao Li', 'Hao Wang', 'Guanjing Xiong', 'Yong Liu', 'Hongsheng Li']
- **Abstrat**: With the rapid rise of large language models (LLMs), phone automation has undergone transformative changes. This paper systematically reviews LLM-driven phone GUI agents, highlighting their evolution from script-based automation to intelligent, adaptive systems. We first contextualize key challenges, (i) limited generality, (ii) high maintenance overhead, and (iii) weak intent comprehension, and show how LLMs address these issues through advanced language understanding, multimodal perception, and robust decision-making. We then propose a taxonomy covering fundamental agent frameworks (single-agent, multi-agent, plan-then-act), modeling approaches (prompt engineering, training-based), and essential datasets and benchmarks. Furthermore, we detail task-specific architectures, supervised fine-tuning, and reinforcement learning strategies that bridge user intent and GUI operations. Finally, we discuss open challenges such as dataset diversity, on-device deployment efficiency, user-centric adaptation, and security concerns, offering forward-looking insights into this rapidly evolving field. By providing a structured overview and identifying pressing research gaps, this paper serves as a definitive reference for researchers and practitioners seeking to harness LLMs in designing scalable, user-friendly phone GUI agents.





## Reinforcement Learning-Based Heterogeneous Multi-Task Optimization in Semantic Broadcast Communications
- **Url**: http://arxiv.org/abs/2504.19806v1
- **Authors**: ['Zhilin Lu', 'Rongpeng Li', 'Zhifeng Zhao', 'Honggang Zhang']
- **Abstrat**: Semantic broadcast communications (Semantic BC) for image transmission have achieved significant performance gains for single-task scenarios. Nevertheless, extending these methods to multi-task scenarios remains challenging, as different tasks typically require distinct objective functions, leading to potential conflicts within the shared encoder. In this paper, we propose a tri-level reinforcement learning (RL)-based multi-task Semantic BC framework, termed SemanticBC-TriRL, which effectively resolves such conflicts and enables the simultaneous support of multiple downstream tasks at the receiver side, including image classification and content reconstruction tasks. Specifically, the proposed framework employs a bottom-up tri-level alternating learning strategy, formulated as a constrained multi-objective optimization problem. At the first level, task-specific decoders are locally optimized using supervised learning. At the second level, the shared encoder is updated via proximal policy optimization (PPO), guided by task-oriented rewards. At the third level, a multi-gradient aggregation-based task weighting module adaptively adjusts task priorities and steers the encoder optimization. Through this hierarchical learning process, the encoder and decoders are alternately trained, and the three levels are cohesively integrated via constrained learning objective. Besides, the convergence of SemanticBC-TriRL is also theoretically established. Extensive simulation results demonstrate the superior performance of the proposed framework across diverse channel conditions, particularly in low SNR regimes, and confirm its scalability with increasing numbers of receivers.





## Model-based controller assisted domain randomization in deep reinforcement learning: application to nonlinear powertrain control
- **Url**: http://arxiv.org/abs/2504.19715v1
- **Authors**: ['Heisei Yonezawa', 'Ansei Yonezawa', 'Itsuro Kajiwara']
- **Abstrat**: Complex mechanical systems such as vehicle powertrains are inherently subject to multiple nonlinearities and uncertainties arising from parametric variations. Modeling and calibration errors are therefore unavoidable, making the transfer of control systems from simulation to real-world systems a critical challenge. Traditional robust controls have limitations in handling certain types of nonlinearities and uncertainties, requiring a more practical approach capable of comprehensively compensating for these various constraints. This study proposes a new robust control approach using the framework of deep reinforcement learning (DRL). The key strategy lies in the synergy among domain randomization-based DRL, long short-term memory (LSTM)-based actor and critic networks, and model-based control (MBC). The problem setup is modeled via the latent Markov decision process (LMDP), a set of vanilla MDPs, for a controlled system subject to uncertainties and nonlinearities. In LMDP, the dynamics of an environment simulator is randomized during training to improve the robustness of the control system to real testing environments. The randomization increases training difficulties as well as conservativeness of the resultant control system; therefore, progress is assisted by concurrent use of a model-based controller based on a nominal system model. Compared to traditional DRL-based controls, the proposed controller design is smarter in that we can achieve a high level of generalization ability with a more compact neural network architecture and a smaller amount of training data. The proposed approach is verified via practical application to active damping for a complex powertrain system with nonlinearities and parametric variations. Comparative tests demonstrate the high robustness of the proposed approach.





## From LLM Reasoning to Autonomous AI Agents: A Comprehensive Review
- **Url**: http://arxiv.org/abs/2504.19678v1
- **Authors**: ['Mohamed Amine Ferrag', 'Norbert Tihanyi', 'Merouane Debbah']
- **Abstrat**: Large language models and autonomous AI agents have evolved rapidly, resulting in a diverse array of evaluation benchmarks, frameworks, and collaboration protocols. However, the landscape remains fragmented and lacks a unified taxonomy or comprehensive survey. Therefore, we present a side-by-side comparison of benchmarks developed between 2019 and 2025 that evaluate these models and agents across multiple domains. In addition, we propose a taxonomy of approximately 60 benchmarks that cover general and academic knowledge reasoning, mathematical problem-solving, code generation and software engineering, factual grounding and retrieval, domain-specific evaluations, multimodal and embodied tasks, task orchestration, and interactive assessments. Furthermore, we review AI-agent frameworks introduced between 2023 and 2025 that integrate large language models with modular toolkits to enable autonomous decision-making and multi-step reasoning. Moreover, we present real-world applications of autonomous AI agents in materials science, biomedical research, academic ideation, software engineering, synthetic data generation, chemical reasoning, mathematical problem-solving, geographic information systems, multimedia, healthcare, and finance. We then survey key agent-to-agent collaboration protocols, namely the Agent Communication Protocol (ACP), the Model Context Protocol (MCP), and the Agent-to-Agent Protocol (A2A). Finally, we discuss recommendations for future research, focusing on advanced reasoning strategies, failure modes in multi-agent LLM systems, automated scientific discovery, dynamic tool integration via reinforcement learning, integrated search capabilities, and security vulnerabilities in agent protocols.





## CAIMAN: Causal Action Influence Detection for Sample-efficient Loco-manipulation
- **Url**: http://arxiv.org/abs/2502.00835v2
- **Authors**: ['Yuanchen Yuan', 'Jin Cheng', 'Núria Armengol Urpí', 'Stelian Coros']
- **Abstrat**: Enabling legged robots to perform non-prehensile loco-manipulation is crucial for enhancing their versatility. Learning behaviors such as whole-body object pushing often requires sophisticated planning strategies or extensive task-specific reward shaping, especially in unstructured environments. In this work, we present CAIMAN, a practical reinforcement learning framework that encourages the agent to gain control over other entities in the environment. CAIMAN leverages causal action influence as an intrinsic motivation objective, allowing legged robots to efficiently acquire object pushing skills even under sparse task rewards. We employ a hierarchical control strategy, combining a low-level locomotion module with a high-level policy that generates task-relevant velocity commands and is trained to maximize the intrinsic reward. To estimate causal action influence, we learn the dynamics of the environment by integrating a kinematic prior with data collected during training.We empirically demonstrate CAIMAN's superior sample efficiency and adaptability to diverse scenarios in simulation, as well as its successful transfer to real-world systems without further fine-tuning.





## Decentralization of Generative AI via Mixture of Experts for Wireless Networks: A Comprehensive Survey
- **Url**: http://arxiv.org/abs/2504.19660v1
- **Authors**: ['Yunting Xu', 'Jiacheng Wang', 'Ruichen Zhang', 'Changyuan Zhao', 'Dusit Niyato', 'Jiawen Kang', 'Zehui Xiong', 'Bo Qian', 'Haibo Zhou', 'Shiwen Mao', 'Abbas Jamalipour', 'Xuemin Shen', 'Dong In Kim']
- **Abstrat**: Mixture of Experts (MoE) has emerged as a promising paradigm for scaling model capacity while preserving computational efficiency, particularly in large-scale machine learning architectures such as large language models (LLMs). Recent advances in MoE have facilitated its adoption in wireless networks to address the increasing complexity and heterogeneity of modern communication systems. This paper presents a comprehensive survey of the MoE framework in wireless networks, highlighting its potential in optimizing resource efficiency, improving scalability, and enhancing adaptability across diverse network tasks. We first introduce the fundamental concepts of MoE, including various gating mechanisms and the integration with generative AI (GenAI) and reinforcement learning (RL). Subsequently, we discuss the extensive applications of MoE across critical wireless communication scenarios, such as vehicular networks, unmanned aerial vehicles (UAVs), satellite communications, heterogeneous networks, integrated sensing and communication (ISAC), and mobile edge networks. Furthermore, key applications in channel prediction, physical layer signal processing, radio resource management, network optimization, and security are thoroughly examined. Additionally, we present a detailed overview of open-source datasets that are widely used in MoE-based models to support diverse machine learning tasks. Finally, this survey identifies crucial future research directions for MoE, emphasizing the importance of advanced training techniques, resource-aware gating strategies, and deeper integration with emerging 6G technologies.





## Transformation & Translation Occupancy Grid Mapping: 2-Dimensional Deep Learning Refined SLAM
- **Url**: http://arxiv.org/abs/2504.19654v1
- **Authors**: ['Leon Davies', 'Baihua Li', 'Mohamad Saada', 'Simon Sølvsten', 'Qinggang Meng']
- **Abstrat**: SLAM (Simultaneous Localisation and Mapping) is a crucial component for robotic systems, providing a map of an environment, the current location and previous trajectory of a robot. While 3D LiDAR SLAM has received notable improvements in recent years, 2D SLAM lags behind. Gradual drifts in odometry and pose estimation inaccuracies hinder modern 2D LiDAR-odometry algorithms in large complex environments. Dynamic robotic motion coupled with inherent estimation based SLAM processes introduce noise and errors, degrading map quality. Occupancy Grid Mapping (OGM) produces results that are often noisy and unclear. This is due to the fact that evidence based mapping represents maps according to uncertain observations. This is why OGMs are so popular in exploration or navigation tasks. However, this also limits OGMs' effectiveness for specific mapping based tasks such as floor plan creation in complex scenes. To address this, we propose our novel Transformation and Translation Occupancy Grid Mapping (TT-OGM). We adapt and enable accurate and robust pose estimation techniques from 3D SLAM to the world of 2D and mitigate errors to improve map quality using Generative Adversarial Networks (GANs). We introduce a novel data generation method via deep reinforcement learning (DRL) to build datasets large enough for training a GAN for SLAM error correction. We demonstrate our SLAM in real-time on data collected at Loughborough University. We also prove its generalisability on a variety of large complex environments on a collection of large scale well-known 2D occupancy maps. Our novel approach enables the creation of high quality OGMs in complex scenes, far surpassing the capabilities of current SLAM algorithms in terms of quality, accuracy and reliability.





## Fooling the Decoder: An Adversarial Attack on Quantum Error Correction
- **Url**: http://arxiv.org/abs/2504.19651v1
- **Authors**: ['Jerome Lenssen', 'Alexandru Paler']
- **Abstrat**: Neural network decoders are becoming essential for achieving fault-tolerant quantum computations. However, their internal mechanisms are poorly understood, hindering our ability to ensure their reliability and security against adversarial attacks. Leading machine learning decoders utilize recurrent and transformer models (e.g., AlphaQubit), with reinforcement learning (RL) playing a key role in training advanced transformer models (e.g., DeepSeek R1). In this work, we target a basic RL surface code decoder (DeepQ) to create the first adversarial attack on quantum error correction. By applying state-of-the-art white-box methods, we uncover vulnerabilities in this decoder, demonstrating an attack that reduces the logical qubit lifetime in memory experiments by up to five orders of magnitude. We validate that this attack exploits a genuine weakness, as the decoder exhibits robustness against noise fluctuations, is largely unaffected by substituting the referee decoder, responsible for episode termination, with an MWPM decoder, and demonstrates fault tolerance at checkable code distances. This attack highlights the susceptibility of machine learning-based QEC and underscores the importance of further research into robust QEC methods.





# TD3
# Prioritized Experience Replay
# path planning
## Clustering-based Recurrent Neural Network Controller synthesis under Signal Temporal Logic Specifications
- **Url**: http://arxiv.org/abs/2504.19846v1
- **Authors**: ['Kazunobu Serizawa', 'Kazumune Hashimoto', 'Wataru Hashimoto', 'Masako Kishida', 'Shigemasa Takai']
- **Abstrat**: Autonomous robotic systems require advanced control frameworks to achieve complex temporal objectives that extend beyond conventional stability and trajectory tracking. Signal Temporal Logic (STL) provides a formal framework for specifying such objectives, with robustness metrics widely employed for control synthesis. Existing optimization-based approaches using neural network (NN)-based controllers often rely on a single NN for both learning and control. However, variations in initial states and obstacle configurations can lead to discontinuous changes in the optimization solution, thereby degrading generalization and control performance. To address this issue, this study proposes a method to enhance recurrent neural network (RNN)-based control by clustering solution trajectories that satisfy STL specifications under diverse initial conditions. The proposed approach utilizes trajectory similarity metrics to generate clustering labels, which are subsequently used to train a classification network. This network assigns new initial states and obstacle configurations to the appropriate cluster, enabling the selection of a specialized controller. By explicitly accounting for variations in solution trajectories, the proposed method improves both estimation accuracy and control performance. Numerical experiments on a dynamic vehicle path planning problem demonstrate the effectiveness of the approach.





## Fast algorithm for centralized multi-agent maze exploration
- **Url**: http://arxiv.org/abs/2310.02121v2
- **Authors**: ['Bojan Crnković', 'Stefan Ivić', 'Mila Zovko']
- **Abstrat**: Recent advances in robotics have paved the way for robots to replace humans in perilous situations, such as searching for victims in burning buildings, in earthquake-damaged structures, in uncharted caves, traversing minefields or patrolling crime-ridden streets. These challenges can be generalized as problems where agents have to explore unknown mazes. We propose a cooperative multi-agent system of automated mobile agents for exploring unknown mazes and localizing stationary targets. The Heat Equation-Driven Area Coverage (HEDAC) algorithm for maze exploration employs a potential field to guide the exploration of the maze and integrates cooperative behaviors of the agents such as collision avoidance, coverage coordination, and path planning. In contrast to previous applications for continuous static domains, we adapt the HEDAC method for mazes on expanding rectilinear grids. The proposed algorithm guarantees the exploration of the entire maze and can ensure the avoidance of collisions and deadlocks. Moreover, this is the first application of the HEDAC algorithm to domains that expand over time. To cope with the dynamically changing domain, succesive over-relaxation (SOR) iterative linear solver has been adapted and implemented, which significantly reduced the computational complexity of the presented algorithm when compared to standard direct and iterative linear solvers. The results highlight significant improvements and show the applicability of the algorithm in different mazes. They confirm its robustness, adaptability, scalability and simplicity, which enables centralized parallel computation to control multiple agents/robots in the maze.





## An Incremental Sampling and Segmentation-Based Approach for Motion Planning Infeasibility
- **Url**: http://arxiv.org/abs/2501.11434v2
- **Authors**: ['Antony Thomas', 'Fulvio Mastrogiovanni', 'Marco Baglietto']
- **Abstrat**: We present a simple and easy-to-implement algorithm to detect plan infeasibility in kinematic motion planning. Our method involves approximating the robot's configuration space to a discrete space, where each degree of freedom has a finite set of values. The obstacle region separates the free configuration space into different connected regions. For a path to exist between the start and goal configurations, they must lie in the same connected region of the free space. Thus, to ascertain plan infeasibility, we merely need to sample adequate points from the obstacle region that isolate start and goal. Accordingly, we progressively construct the configuration space by sampling from the discretized space and updating the bitmap cells representing obstacle regions. Subsequently, we partition this partially built configuration space to identify different connected components within it and assess the connectivity of the start and goal cells. We illustrate this methodology on five different scenarios with configuration spaces having up to 5 degree-of-freedom (DOF).





## Structuring the Environment Nudges Participants Toward Hierarchical Over Shortest Path Planning
- **Url**: http://arxiv.org/abs/2502.10098v2
- **Authors**: ['Valeria Simonelli', 'Davide Nuzzi', 'Gian Luca Lancia', 'Giovanni Pezzulo']
- **Abstrat**: Effective planning is crucial for navigating complex environments and achieving goals efficiently. In this study, we investigated how environmental structure influences the selection of planning strategies. Forty-two participants navigated a space station to collect colored spheres, with environments either structured (spheres grouped by color) or unstructured (spheres scattered randomly). We tested three types of plans: hierarchical (grouping spheres by color), shortest path (minimizing travel distance), and neutral (none of the above). By manipulating environmental structure, we were able to nudge participants toward a preference for hierarchical planning in structured environments, while shortest path plans were favored in unstructured environments. A mismatch between self-reported preferences and actual choices indicated that participants often adopted implicit strategies, unaware of their decision-making processes. These findings highlight the powerful effect of environmental cues on planning and suggest that even subtle changes in structure can guide the selection of planning strategies.





## A Time-dependent Risk-aware distributed Multi-Agent Path Finder based on A*
- **Url**: http://arxiv.org/abs/2504.19593v1
- **Authors**: ['S Nordström', 'Y Bai', 'B Lindqvist', 'G Nikolakopoulos']
- **Abstrat**: Multi-Agent Path-Finding (MAPF) focuses on the collaborative planning of paths for multiple agents within shared spaces, aiming for collision-free navigation. Conventional planning methods often overlook the presence of other agents, which can result in conflicts. In response, this article introduces the A$^*_+$T algorithm, a distributed approach that improves coordination among agents by anticipating their positions based on their movement speeds. The algorithm also considers dynamic obstacles, assessing potential collisions with respect to observed speeds and trajectories, thereby facilitating collision-free path planning in environments populated by other agents and moving objects. It incorporates a risk layer surrounding both dynamic and static entities, enhancing its utility in real-world applications. Each agent functions autonomously while being mindful of the paths chosen by others, effectively addressing the complexities inherent in multi-agent situations. The performance of A$^*_+$T has been rigorously tested in the Gazebo simulation environment and benchmarked against established approaches such as CBS, ECBS, and SIPP. Furthermore, the algorithm has shown competence in single-agent experiments, with results demonstrating its effectiveness in managing dynamic obstacles and affirming its practical relevance across various scenarios.




