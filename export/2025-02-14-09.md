# reinforcement learning
## DexTrack: Towards Generalizable Neural Tracking Control for Dexterous Manipulation from Human References
- **Url**: http://arxiv.org/abs/2502.09614v1
- **Authors**: ['Xueyi Liu', 'Jianibieke Adalibieke', 'Qianwei Han', 'Yuzhe Qin', 'Li Yi']
- **Abstrat**: We address the challenge of developing a generalizable neural tracking controller for dexterous manipulation from human references. This controller aims to manage a dexterous robot hand to manipulate diverse objects for various purposes defined by kinematic human-object interactions. Developing such a controller is complicated by the intricate contact dynamics of dexterous manipulation and the need for adaptivity, generalizability, and robustness. Current reinforcement learning and trajectory optimization methods often fall short due to their dependence on task-specific rewards or precise system models. We introduce an approach that curates large-scale successful robot tracking demonstrations, comprising pairs of human references and robot actions, to train a neural controller. Utilizing a data flywheel, we iteratively enhance the controller's performance, as well as the number and quality of successful tracking demonstrations. We exploit available tracking demonstrations and carefully integrate reinforcement learning and imitation learning to boost the controller's performance in dynamic environments. At the same time, to obtain high-quality tracking demonstrations, we individually optimize per-trajectory tracking by leveraging the learned tracking controller in a homotopy optimization method. The homotopy optimization, mimicking chain-of-thought, aids in solving challenging trajectory tracking problems to increase demonstration diversity. We showcase our success by training a generalizable neural controller and evaluating it in both simulation and real world. Our method achieves over a 10% improvement in success rates compared to leading baselines. The project website with animated results is available at https://meowuu7.github.io/DexTrack/.





## OGBench: Benchmarking Offline Goal-Conditioned RL
- **Url**: http://arxiv.org/abs/2410.20092v2
- **Authors**: ['Seohong Park', 'Kevin Frans', 'Benjamin Eysenbach', 'Sergey Levine']
- **Abstrat**: Offline goal-conditioned reinforcement learning (GCRL) is a major problem in reinforcement learning (RL) because it provides a simple, unsupervised, and domain-agnostic way to acquire diverse behaviors and representations from unlabeled data without rewards. Despite the importance of this setting, we lack a standard benchmark that can systematically evaluate the capabilities of offline GCRL algorithms. In this work, we propose OGBench, a new, high-quality benchmark for algorithms research in offline goal-conditioned RL. OGBench consists of 8 types of environments, 85 datasets, and reference implementations of 6 representative offline GCRL algorithms. We have designed these challenging and realistic environments and datasets to directly probe different capabilities of algorithms, such as stitching, long-horizon reasoning, and the ability to handle high-dimensional inputs and stochasticity. While representative algorithms may rank similarly on prior benchmarks, our experiments reveal stark strengths and weaknesses in these different capabilities, providing a strong foundation for building new algorithms. Project page: https://seohong.me/projects/ogbench





## Coupled Rendezvous and Docking Maneuver control of satellite using Reinforcement learning-based Adaptive Fixed-Time Sliding Mode Controller
- **Url**: http://arxiv.org/abs/2502.09517v1
- **Authors**: ['Rakesh Kumar Sahoo', 'Manoranjan Sinha']
- **Abstrat**: Satellite dynamics in unknown environments are inherently uncertain due to factors such as varying gravitational fields, atmospheric drag, and unpredictable interactions with space debris or other celestial bodies. Traditional sliding mode controllers with fixed parameters often struggle to maintain optimal performance under these fluctuating conditions. Therefore, an adaptive controller is essential to address these challenges by continuously tuning its gains in real-time. In this paper, we have tuned the slopes of the Fixed-time Sliding surface adaptively using reinforcement learning for coupled rendezvous and docking maneuver of chaser satellite with the target satellite in an unknown space environment. The neural network model is used to determine the optimal gains of reaching law of the fixed-time sliding surface. We have assumed that we don't have an accurate model of the system so we have added noise in the tangent space instead of directly on the manifold to preserve the geometric structure of the system while ensuring mathematically consistent uncertainty propagation. The reinforcement learning is used as an approximator to represent the value function of the agent to estimate the dynamical model of the system using the Actor-Critic method. The proposed control algorithm integrates a neural network and a sliding mode controller in a cascade loop architecture, where the tracking error dynamically tunes the sliding surface gains. Global fixed-time stability of the closed-loop feedback system is proved within the Lyapunov framework. This comprehensive approach of fixed-time sliding mode controller using a Reinforcement Learning based ensures the completion of the mission efficiently while addressing the critical challenges posed by the uncertain environment. The simulation results presented support the claims made.





## Sable: a Performant, Efficient and Scalable Sequence Model for MARL
- **Url**: http://arxiv.org/abs/2410.01706v2
- **Authors**: ['Omayma Mahjoub', 'Sasha Abramowitz', 'Ruan de Kock', 'Wiem Khlifi', 'Simon du Toit', 'Jemma Daniel', 'Louay Ben Nessir', 'Louise Beyers', 'Claude Formanek', 'Liam Clark', 'Arnu Pretorius']
- **Abstrat**: As multi-agent reinforcement learning (MARL) progresses towards solving larger and more complex problems, it becomes increasingly important that algorithms exhibit the key properties of (1) strong performance, (2) memory efficiency and (3) scalability. In this work, we introduce Sable, a performant, memory efficient and scalable sequence modeling approach to MARL. Sable works by adapting the retention mechanism in Retentive Networks to achieve computationally efficient processing of multi-agent observations with long context memory for temporal reasoning. Through extensive evaluations across six diverse environments, we demonstrate how Sable is able to significantly outperform existing state-of-the-art methods in a large number of diverse tasks (34 out of 45 tested). Furthermore, Sable maintains performance as we scale the number of agents, handling environments with more than a thousand agents while exhibiting a linear increase in memory usage. Finally, we conduct ablation studies to isolate the source of Sable's performance gains and confirm its efficient computational memory usage.





## A Survey of Reinforcement Learning for Optimization in Automation
- **Url**: http://arxiv.org/abs/2502.09417v1
- **Authors**: ['Ahmad Farooq', 'Kamran Iqbal']
- **Abstrat**: Reinforcement Learning (RL) has become a critical tool for optimization challenges within automation, leading to significant advancements in several areas. This review article examines the current landscape of RL within automation, with a particular focus on its roles in manufacturing, energy systems, and robotics. It discusses state-of-the-art methods, major challenges, and upcoming avenues of research within each sector, highlighting RL's capacity to solve intricate optimization challenges. The paper reviews the advantages and constraints of RL-driven optimization methods in automation. It points out prevalent challenges encountered in RL optimization, including issues related to sample efficiency and scalability; safety and robustness; interpretability and trustworthiness; transfer learning and meta-learning; and real-world deployment and integration. It further explores prospective strategies and future research pathways to navigate these challenges. Additionally, the survey includes a comprehensive list of relevant research papers, making it an indispensable guide for scholars and practitioners keen on exploring this domain.





## Generalizable Reinforcement Learning with Biologically Inspired Hyperdimensional Occupancy Grid Maps for Exploration and Goal-Directed Path Planning
- **Url**: http://arxiv.org/abs/2502.09393v1
- **Authors**: ['Shay Snyder', 'Ryan Shea', 'Andrew Capodieci', 'David Gorsich', 'Maryam Parsa']
- **Abstrat**: Real-time autonomous systems utilize multi-layer computational frameworks to perform critical tasks such as perception, goal finding, and path planning. Traditional methods implement perception using occupancy grid mapping (OGM), segmenting the environment into discretized cells with probabilistic information. This classical approach is well-established and provides a structured input for downstream processes like goal finding and path planning algorithms. Recent approaches leverage a biologically inspired mathematical framework known as vector symbolic architectures (VSA), commonly known as hyperdimensional computing, to perform probabilistic OGM in hyperdimensional space. This approach, VSA-OGM, provides native compatibility with spiking neural networks, positioning VSA-OGM as a potential neuromorphic alternative to conventional OGM. However, for large-scale integration, it is essential to assess the performance implications of VSA-OGM on downstream tasks compared to established OGM methods. This study examines the efficacy of VSA-OGM against a traditional OGM approach, Bayesian Hilbert Maps (BHM), within reinforcement learning based goal finding and path planning frameworks, across a controlled exploration environment and an autonomous driving scenario inspired by the F1-Tenth challenge. Our results demonstrate that VSA-OGM maintains comparable learning performance across single and multi-scenario training configurations while improving performance on unseen environments by approximately 47%. These findings highlight the increased generalizability of policy networks trained with VSA-OGM over BHM, reinforcing its potential for real-world deployment in diverse environments.





## Machine learning for modelling unstructured grid data in computational physics: a review
- **Url**: http://arxiv.org/abs/2502.09346v1
- **Authors**: ['Sibo Cheng', 'Marc Bocquet', 'Weiping Ding', 'Tobias Sebastian Finn', 'Rui Fu', 'Jinlong Fu', 'Yike Guo', 'Eleda Johnson', 'Siyi Li', 'Che Liu', 'Eric Newton Moro', 'Jie Pan', 'Matthew Piggott', 'Cesar Quilodran', 'Prakhar Sharma', 'Kun Wang', 'Dunhui Xiao', 'Xiao Xue', 'Yong Zeng', 'Mingrui Zhang', 'Hao Zhou', 'Kewei Zhu', 'Rossella Arcucci']
- **Abstrat**: Unstructured grid data are essential for modelling complex geometries and dynamics in computational physics. Yet, their inherent irregularity presents significant challenges for conventional machine learning (ML) techniques. This paper provides a comprehensive review of advanced ML methodologies designed to handle unstructured grid data in high-dimensional dynamical systems. Key approaches discussed include graph neural networks, transformer models with spatial attention mechanisms, interpolation-integrated ML methods, and meshless techniques such as physics-informed neural networks. These methodologies have proven effective across diverse fields, including fluid dynamics and environmental simulations. This review is intended as a guidebook for computational scientists seeking to apply ML approaches to unstructured grid data in their domains, as well as for ML researchers looking to address challenges in computational physics. It places special focus on how ML methods can overcome the inherent limitations of traditional numerical techniques and, conversely, how insights from computational physics can inform ML development. To support benchmarking, this review also provides a summary of open-access datasets of unstructured grid data in computational physics. Finally, emerging directions such as generative models with unstructured data, reinforcement learning for mesh generation, and hybrid physics-data-driven paradigms are discussed to inspire future advancements in this evolving field.





## Revisiting Topological Interference Management: A Learning-to-Code on Graphs Perspective
- **Url**: http://arxiv.org/abs/2502.09344v1
- **Authors**: ['Zhiwei Shan', 'Xinping Yi', 'Han Yu', 'Chung-Shou Liao', 'Shi Jin']
- **Abstrat**: The advance of topological interference management (TIM) has been one of the driving forces of recent developments in network information theory. However, state-of-the-art coding schemes for TIM are usually handcrafted for specific families of network topologies, relying critically on experts' domain knowledge and sophisticated treatments. The lack of systematic and automatic generation of solutions inevitably restricts their potential wider applications to wireless communication systems, due to the limited generalizability of coding schemes to wider network configurations. To address such an issue, this work makes the first attempt to advocate revisiting topological interference alignment (IA) from a novel learning-to-code perspective. Specifically, we recast the one-to-one and subspace IA conditions as vector assignment policies and propose a unifying learning-to-code on graphs (LCG) framework by leveraging graph neural networks (GNNs) for capturing topological structures and reinforcement learning (RL) for decision-making of IA beamforming vector assignment. Interestingly, the proposed LCG framework is capable of recovering known one-to-one scalar/vector IA solutions for a significantly wider range of network topologies, and more remarkably of discovering new subspace IA coding schemes for multiple-antenna cases that are challenging to be handcrafted. The extensive experiments demonstrate that the LCG framework is an effective way to automatically produce systematic coding solutions to the TIM instances with arbitrary network topologies, and at the same time, the underlying learning algorithm is efficient with respect to online inference time and possesses excellent generalizability and transferability for practical deployment.





## Convex Is Back: Solving Belief MDPs With Convexity-Informed Deep Reinforcement Learning
- **Url**: http://arxiv.org/abs/2502.09298v1
- **Authors**: ['Daniel Koutas', 'Daniel Hettegger', 'Kostas G. Papakonstantinou', 'Daniel Straub']
- **Abstrat**: We present a novel method for Deep Reinforcement Learning (DRL), incorporating the convex property of the value function over the belief space in Partially Observable Markov Decision Processes (POMDPs). We introduce hard-and soft-enforced convexity as two different approaches, and compare their performance against standard DRL on two well-known POMDP environments, namely the Tiger and FieldVisionRockSample problems. Our findings show that including the convexity feature can substantially increase performance of the agents, as well as increase robustness over the hyperparameter space, especially when testing on out-of-distribution domains. The source code for this work can be found at https://github.com/Dakout/Convex_DRL.





## Statistical Inference for Temporal Difference Learning with Linear Function Approximation
- **Url**: http://arxiv.org/abs/2410.16106v2
- **Authors**: ['Weichen Wu', 'Gen Li', 'Yuting Wei', 'Alessandro Rinaldo']
- **Abstrat**: Statistical inference with finite-sample validity for the value function of a given policy in Markov decision processes (MDPs) is crucial for ensuring the reliability of reinforcement learning. Temporal Difference (TD) learning, arguably the most widely used algorithm for policy evaluation, serves as a natural framework for this purpose. In this paper, we study the consistency properties of TD learning with Polyak-Ruppert averaging and linear function approximation, and obtain three significant improvements over existing results. First, we derive a novel sharp high-dimensional probability convergence guarantee that depends explicitly on the asymptotic variance and holds under weak conditions. We further establish refined high-dimensional Berry-Esseen bounds over the class of convex sets that guarantee faster rates than those in the literature. Finally, we propose a plug-in estimator for the asymptotic covariance matrix, designed for efficient online computation. These results enable the construction of confidence regions and simultaneous confidence intervals for the linear parameters of the value function, with guaranteed finite-sample coverage. We demonstrate the applicability of our theoretical findings through numerical experiments.





## Geometry-aware RL for Manipulation of Varying Shapes and Deformable Objects
- **Url**: http://arxiv.org/abs/2502.07005v3
- **Authors**: ['Tai Hoang', 'Huy Le', 'Philipp Becker', 'Vien Anh Ngo', 'Gerhard Neumann']
- **Abstrat**: Manipulating objects with varying geometries and deformable objects is a major challenge in robotics. Tasks such as insertion with different objects or cloth hanging require precise control and effective modelling of complex dynamics. In this work, we frame this problem through the lens of a heterogeneous graph that comprises smaller sub-graphs, such as actuators and objects, accompanied by different edge types describing their interactions. This graph representation serves as a unified structure for both rigid and deformable objects tasks, and can be extended further to tasks comprising multiple actuators. To evaluate this setup, we present a novel and challenging reinforcement learning benchmark, including rigid insertion of diverse objects, as well as rope and cloth manipulation with multiple end-effectors. These tasks present a large search space, as both the initial and target configurations are uniformly sampled in 3D space. To address this issue, we propose a novel graph-based policy model, dubbed Heterogeneous Equivariant Policy (HEPi), utilizing $SE(3)$ equivariant message passing networks as the main backbone to exploit the geometric symmetry. In addition, by modeling explicit heterogeneity, HEPi can outperform Transformer-based and non-heterogeneous equivariant policies in terms of average returns, sample efficiency, and generalization to unseen objects.





## Progressive-Resolution Policy Distillation: Leveraging Coarse-Resolution Simulations for Time-Efficient Fine-Resolution Policy Learning
- **Url**: http://arxiv.org/abs/2412.07477v2
- **Authors**: ['Yuki Kadokawa', 'Hirotaka Tahara', 'Takamitsu Matsubara']
- **Abstrat**: In earthwork and construction, excavators often encounter large rocks mixed with various soil conditions, requiring skilled operators. This paper presents a framework for achieving autonomous excavation using reinforcement learning (RL) through a rock excavation simulator. In the simulation, resolution can be defined by the particle size/number in the whole soil space. Fine-resolution simulations closely mimic real-world behavior but demand significant calculation time and challenging sample collection, while coarse-resolution simulations enable faster sample collection but deviate from real-world behavior. To combine the advantages of both resolutions, we explore using policies developed in coarse-resolution simulations for pre-training in fine-resolution simulations. To this end, we propose a novel policy learning framework called Progressive-Resolution Policy Distillation (PRPD), which progressively transfers policies through some middle-resolution simulations with conservative policy transfer to avoid domain gaps that could lead to policy transfer failure. Validation in a rock excavation simulator and nine real-world rock environments demonstrated that PRPD reduced sampling time to less than 1/7 while maintaining task success rates comparable to those achieved through policy learning in a fine-resolution simulation.





## Autonomous Task Completion Based on Goal-directed Answer Set Programming
- **Url**: http://arxiv.org/abs/2502.09208v1
- **Authors**: ['Alexis R. Tudor']
- **Abstrat**: Task planning for autonomous agents has typically been done using deep learning models and simulation-based reinforcement learning. This research proposes combining inductive learning techniques with goal-directed answer set programming to increase the explainability and reliability of systems for task breakdown and completion. Preliminary research has led to the creation of a Python harness that utilizes s(CASP) to solve task problems in a computationally efficient way. Although this research is in the early stages, we are exploring solutions to complex problems in simulated task completion.





## Model-free reinforcement learning with noisy actions for automated experimental control in optics
- **Url**: http://arxiv.org/abs/2405.15421v2
- **Authors**: ['Lea Richtmann', 'Viktoria-S. Schmiesing', 'Dennis Wilken', 'Jan Heine', 'Aaron Tranter', 'Avishek Anand', 'Tobias J. Osborne', 'Mich√®le Heurs']
- **Abstrat**: Setting up and controlling optical systems is often a challenging and tedious task. The high number of degrees of freedom to control mirrors, lenses, or phases of light makes automatic control challenging, especially when the complexity of the system cannot be adequately modeled due to noise or non-linearities. Here, we show that reinforcement learning (RL) can overcome these challenges when coupling laser light into an optical fiber, using a model-free RL approach that trains directly on the experiment without pre-training. By utilizing the sample-efficient algorithms Soft Actor-Critic (SAC) or Truncated Quantile Critics (TQC), our agent learns to couple with 90% efficiency, comparable to the human expert. We demonstrate that direct training on an experiment can replace extensive system modeling. Our result exemplifies RL's potential to tackle problems in optics, paving the way for more complex applications where full noise modeling is not feasible.





## Logical Reasoning in Large Language Models: A Survey
- **Url**: http://arxiv.org/abs/2502.09100v1
- **Authors**: ['Hanmeng Liu', 'Zhizhang Fu', 'Mengru Ding', 'Ruoxi Ning', 'Chaoli Zhang', 'Xiaozhang Liu', 'Yue Zhang']
- **Abstrat**: With the emergence of advanced reasoning models like OpenAI o3 and DeepSeek-R1, large language models (LLMs) have demonstrated remarkable reasoning capabilities. However, their ability to perform rigorous logical reasoning remains an open question. This survey synthesizes recent advancements in logical reasoning within LLMs, a critical area of AI research. It outlines the scope of logical reasoning in LLMs, its theoretical foundations, and the benchmarks used to evaluate reasoning proficiency. We analyze existing capabilities across different reasoning paradigms - deductive, inductive, abductive, and analogical - and assess strategies to enhance reasoning performance, including data-centric tuning, reinforcement learning, decoding strategies, and neuro-symbolic approaches. The review concludes with future directions, emphasizing the need for further exploration to strengthen logical reasoning in AI systems.





## An Evolved Universal Transformer Memory
- **Url**: http://arxiv.org/abs/2410.13166v4
- **Authors**: ['Edoardo Cetin', 'Qi Sun', 'Tianyu Zhao', 'Yujin Tang']
- **Abstrat**: Prior methods propose to offset the escalating costs of modern foundation models by dropping specific parts of their contexts with hand-designed rules, while attempting to preserve their original performance. We overcome this trade-off with Neural Attention Memory Models (NAMMs), introducing a learned network for memory management that improves both the performance and efficiency of transformers. We evolve NAMMs atop pre-trained transformers to provide different latent contexts focusing on the most relevant information for individual layers and attention heads. NAMMs are universally applicable to any model using self-attention as they condition exclusively on the values in the produced attention matrices. Learning NAMMs on a small set of problems, we achieve substantial performance improvements across multiple long-context benchmarks while cutting the model's input contexts up to a fraction of the original sizes. We show the generality of our conditioning enables zero-shot transfer of NAMMs trained only on language to entirely new transformer architectures even across input modalities, with their benefits carrying over to vision and reinforcement learning.





## Temporal Representation Alignment: Successor Features Enable Emergent Compositionality in Robot Instruction Following
- **Url**: http://arxiv.org/abs/2502.05454v2
- **Authors**: ['Vivek Myers', 'Bill Chunyuan Zheng', 'Anca Dragan', 'Kuan Fang', 'Sergey Levine']
- **Abstrat**: Effective task representations should facilitate compositionality, such that after learning a variety of basic tasks, an agent can perform compound tasks consisting of multiple steps simply by composing the representations of the constituent steps together. While this is conceptually simple and appealing, it is not clear how to automatically learn representations that enable this sort of compositionality. We show that learning to associate the representations of current and future states with a temporal alignment loss can improve compositional generalization, even in the absence of any explicit subtask planning or reinforcement learning. We evaluate our approach across diverse robotic manipulation tasks as well as in simulation, showing substantial improvements for tasks specified with either language or goal images.





## Typhoon T1: An Open Thai Reasoning Model
- **Url**: http://arxiv.org/abs/2502.09042v1
- **Authors**: ['Pittawat Taveekitworachai', 'Potsawee Manakul', 'Kasima Tharnpipitchai', 'Kunat Pipatanakul']
- **Abstrat**: This paper introduces Typhoon T1, an open effort to develop an open Thai reasoning model. A reasoning model is a relatively new type of generative model built on top of large language models (LLMs). A reasoning model generates a long chain of thought before arriving at a final answer, an approach found to improve performance on complex tasks. However, details on developing such a model are limited, especially for reasoning models that can generate traces in a low-resource language. Typhoon T1 presents an open effort that dives into the details of developing a reasoning model in a more cost-effective way by leveraging supervised fine-tuning using open datasets, instead of reinforcement learning. This paper shares the details about synthetic data generation and training, as well as our dataset and model weights. Additionally, we provide insights gained from developing a reasoning model that generalizes across domains and is capable of generating reasoning traces in a low-resource language, using Thai as an example. We hope this open effort provides a foundation for further research in this field.





# TD3
# Prioritized Experience Replay
# path planning
## Real-Time Fast Marching Tree for Mobile Robot Motion Planning in Dynamic Environments
- **Url**: http://arxiv.org/abs/2502.09556v1
- **Authors**: ['Jefferson Silveira', 'Kleber Cabral', 'Sidney Givigi', 'Joshua A. Marshall']
- **Abstrat**: This paper proposes the Real-Time Fast Marching Tree (RT-FMT), a real-time planning algorithm that features local and global path generation, multiple-query planning, and dynamic obstacle avoidance. During the search, RT-FMT quickly looks for the global solution and, in the meantime, generates local paths that can be used by the robot to start execution faster. In addition, our algorithm constantly rewires the tree to keep branches from forming inside the dynamic obstacles and to maintain the tree root near the robot, which allows the tree to be reused multiple times for different goals. Our algorithm is based on the planners Fast Marching Tree (FMT*) and Real-time Rapidly-Exploring Random Tree (RT-RRT*). We show via simulations that RT-FMT outperforms RT- RRT* in both execution cost and arrival time, in most cases. Moreover, we also demonstrate via simulation that it is worthwhile taking the local path before the global path is available in order to reduce arrival time, even though there is a small possibility of taking an inferior path.





## Speed planning by minimizing travel time and energy consumption
- **Url**: http://arxiv.org/abs/2502.09468v1
- **Authors**: ['Stefano Ardizzoni', 'Luca Consolini', 'Mattia Laurini', 'Marco Locatelli']
- **Abstrat**: In this paper we address the speed planning problem for a vehicle over an assigned path with the aim of minimizing a weighted sum of travel time and energy consumption under suitable constraints (maximum allowed speed, maximum traction or braking force, maximum power consumption). The resulting mathematical model is a non--convex optimization problem. We prove that, under some mild assumptions, a convex reformulation of the non--convex problem is exact. In particular, the convex reformulation is a Second Order Cone Programming (SOCP) problem, for which efficient solvers exist. Through the numerical experiments we confirm that the convex relaxation can be solved very efficiently and, moreover, we also provide the Pareto front of the trade-off between the two objectives (travel time and energy consumption).





## Generalizable Reinforcement Learning with Biologically Inspired Hyperdimensional Occupancy Grid Maps for Exploration and Goal-Directed Path Planning
- **Url**: http://arxiv.org/abs/2502.09393v1
- **Authors**: ['Shay Snyder', 'Ryan Shea', 'Andrew Capodieci', 'David Gorsich', 'Maryam Parsa']
- **Abstrat**: Real-time autonomous systems utilize multi-layer computational frameworks to perform critical tasks such as perception, goal finding, and path planning. Traditional methods implement perception using occupancy grid mapping (OGM), segmenting the environment into discretized cells with probabilistic information. This classical approach is well-established and provides a structured input for downstream processes like goal finding and path planning algorithms. Recent approaches leverage a biologically inspired mathematical framework known as vector symbolic architectures (VSA), commonly known as hyperdimensional computing, to perform probabilistic OGM in hyperdimensional space. This approach, VSA-OGM, provides native compatibility with spiking neural networks, positioning VSA-OGM as a potential neuromorphic alternative to conventional OGM. However, for large-scale integration, it is essential to assess the performance implications of VSA-OGM on downstream tasks compared to established OGM methods. This study examines the efficacy of VSA-OGM against a traditional OGM approach, Bayesian Hilbert Maps (BHM), within reinforcement learning based goal finding and path planning frameworks, across a controlled exploration environment and an autonomous driving scenario inspired by the F1-Tenth challenge. Our results demonstrate that VSA-OGM maintains comparable learning performance across single and multi-scenario training configurations while improving performance on unseen environments by approximately 47%. These findings highlight the increased generalizability of policy networks trained with VSA-OGM over BHM, reinforcing its potential for real-world deployment in diverse environments.





## FLAF: Focal Line and Feature-constrained Active View Planning for Visual Teach and Repeat
- **Url**: http://arxiv.org/abs/2409.03457v4
- **Authors**: ['Changfei Fu', 'Weinan Chen', 'Hong Zhang']
- **Abstrat**: This paper presents FLAF, a focal line and feature-constrained active view planning method for tracking failure avoidance in feature-based visual navigation of mobile robots. Our FLAF-based visual navigation is built upon a feature-based visual teach and repeat (VT\&R) framework, which supports many robotic applications by teaching a robot to navigate on various paths that cover a significant portion of daily autonomous navigation requirements. However, tracking failure in feature-based visual simultaneous localization and mapping (VSLAM) caused by textureless regions in human-made environments is still limiting VT\&R to be adopted in the real world. To address this problem, the proposed view planner is integrated into a feature-based visual SLAM system to build up an active VT\&R system that avoids tracking failure. In our system, a pan-tilt unit (PTU)-based active camera is mounted on the mobile robot. Using FLAF, the active camera-based VSLAM operates during the teaching phase to construct a complete path map and in the repeat phase to maintain stable localization. FLAF orients the robot toward more map points to avoid mapping failures during path learning and toward more feature-identifiable map points beneficial for localization while following the learned trajectory. Experiments in real scenarios demonstrate that FLAF outperforms the methods that do not consider feature-identifiability, and our active VT\&R system performs well in complex environments by effectively dealing with low-texture regions.




