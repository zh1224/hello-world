# reinforcement learning
## RILe: Reinforced Imitation Learning
- **Url**: http://arxiv.org/abs/2406.08472v2
- **Authors**: ['Mert Albaba', 'Sammy Christen', 'Thomas Langarek', 'Christoph Gebhardt', 'Otmar Hilliges', 'Michael J. Black']
- **Abstrat**: Reinforcement Learning has achieved significant success in generating complex behavior but often requires extensive reward function engineering. Adversarial variants of Imitation Learning and Inverse Reinforcement Learning offer an alternative by learning policies from expert demonstrations via a discriminator. However, these methods struggle in complex tasks where randomly sampling expert-like behaviors is challenging. This limitation stems from their reliance on policy-agnostic discriminators, which provide insufficient guidance for agent improvement, especially as task complexity increases and expert behavior becomes more distinct. We introduce RILe (Reinforced Imitation Learning environment), a novel trainer-student system that learns a dynamic reward function based on the student's performance and alignment with expert demonstrations. In RILe, the student learns an action policy while the trainer, using reinforcement learning, continuously updates itself via the discriminator's feedback to optimize the alignment between the student and the expert. The trainer optimizes for long-term cumulative rewards from the discriminator, enabling it to provide nuanced feedback that accounts for the complexity of the task and the student's current capabilities. This approach allows for greater exploration of agent actions by providing graduated feedback rather than binary expert/non-expert classifications. By reducing dependence on policy-agnostic discriminators, RILe enables better performance in complex settings where traditional methods falter, outperforming existing methods by 2x in complex simulated robot-locomotion tasks.





## Knowledge Transfer from Simple to Complex: A Safe and Efficient Reinforcement Learning Framework for Autonomous Driving Decision-Making
- **Url**: http://arxiv.org/abs/2410.14468v2
- **Authors**: ['Rongliang Zhou', 'Jiakun Huang', 'Mingjun Li', 'Hepeng Li', 'Haotian Cao', 'Xiaolin Song']
- **Abstrat**: A safe and efficient decision-making system is crucial for autonomous vehicles. However, the complexity and variability of driving environments limit the effectiveness of many rule-based and machine learning-based decision-making approaches. Reinforcement Learning in autonomous driving offers a promising solution to these challenges. Nevertheless, concerns regarding safety and efficiency during training remain major obstacles to its widespread application. To address these concerns, we propose a novel RL framework named Simple to Complex Collaborative Decision. First, we rapidly train the teacher model using the Proximal Policy Optimization algorithm in a lightweight simulation environment. In the more intricate simulation environment, the teacher model intervenes when the student agent exhibits suboptimal behavior by assessing the value of actions to avert dangerous situations. We also introduce an innovative RL algorithm called Adaptive Clipping PPO, which is trained using a combination of samples generated by both teacher and student policies, and employs dynamic clipping strategies based on sample importance. Additionally, we employ the KL divergence as a constraint on policy optimization, transforming it into an unconstrained problem to accelerate the student's learning of the teacher's policy. Finally, a gradual weaning strategy is employed to ensure that, over time, the student agent learns to explore independently. Simulation experiments in highway lane-change scenarios demonstrate that the S2CD framework enhances learning efficiency, reduces training costs, and significantly improves safety during training when compared with state-of-the-art baseline algorithms. This approach also ensures effective knowledge transfer between teacher and student models, and even when the teacher model is suboptimal.





## Improve Vision Language Model Chain-of-thought Reasoning
- **Url**: http://arxiv.org/abs/2410.16198v1
- **Authors**: ['Ruohong Zhang', 'Bowen Zhang', 'Yanghao Li', 'Haotian Zhang', 'Zhiqing Sun', 'Zhe Gan', 'Yinfei Yang', 'Ruoming Pang', 'Yiming Yang']
- **Abstrat**: Chain-of-thought (CoT) reasoning in vision language models (VLMs) is crucial for improving interpretability and trustworthiness. However, current training recipes lack robust CoT reasoning data, relying on datasets dominated by short annotations with minimal rationales. In this work, we show that training VLM on short answers does not generalize well to reasoning tasks that require more detailed responses. To address this, we propose a two-fold approach. First, we distill rationales from GPT-4o model to enrich the training data and fine-tune VLMs, boosting their CoT performance. Second, we apply reinforcement learning to further calibrate reasoning quality. Specifically, we construct positive (correct) and negative (incorrect) pairs of model-generated reasoning chains, by comparing their predictions with annotated short answers. Using this pairwise data, we apply the Direct Preference Optimization algorithm to refine the model's reasoning abilities. Our experiments demonstrate significant improvements in CoT reasoning on benchmark datasets and better generalization to direct answer prediction as well. This work emphasizes the importance of incorporating detailed rationales in training and leveraging reinforcement learning to strengthen the reasoning capabilities of VLMs.





## RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style
- **Url**: http://arxiv.org/abs/2410.16184v1
- **Authors**: ['Yantao Liu', 'Zijun Yao', 'Rui Min', 'Yixin Cao', 'Lei Hou', 'Juanzi Li']
- **Abstrat**: Reward models are critical in techniques like Reinforcement Learning from Human Feedback (RLHF) and Inference Scaling Laws, where they guide language model alignment and select optimal responses. Despite their importance, existing reward model benchmarks often evaluate models by asking them to distinguish between responses generated by models of varying power. However, this approach fails to assess reward models on subtle but critical content changes and variations in style, resulting in a low correlation with policy model performance. To this end, we introduce RM-Bench, a novel benchmark designed to evaluate reward models based on their sensitivity to subtle content differences and resistance to style biases. Extensive experiments demonstrate that RM-Bench strongly correlates with policy model performance, making it a reliable reference for selecting reward models to align language models effectively. We evaluate nearly 40 reward models on RM-Bench. Our results reveal that even state-of-the-art models achieve an average performance of only 46.6%, which falls short of random-level accuracy (50%) when faced with style bias interference. These findings highlight the significant room for improvement in current reward models. Related code and data are available at https://github.com/THU-KEG/RM-Bench.





## SleeperNets: Universal Backdoor Poisoning Attacks Against Reinforcement Learning Agents
- **Url**: http://arxiv.org/abs/2405.20539v2
- **Authors**: ['Ethan Rathbun', 'Christopher Amato', 'Alina Oprea']
- **Abstrat**: Reinforcement learning (RL) is an actively growing field that is seeing increased usage in real-world, safety-critical applications -- making it paramount to ensure the robustness of RL algorithms against adversarial attacks. In this work we explore a particularly stealthy form of training-time attacks against RL -- backdoor poisoning. Here the adversary intercepts the training of an RL agent with the goal of reliably inducing a particular action when the agent observes a pre-determined trigger at inference time. We uncover theoretical limitations of prior work by proving their inability to generalize across domains and MDPs. Motivated by this, we formulate a novel poisoning attack framework which interlinks the adversary's objectives with those of finding an optimal policy -- guaranteeing attack success in the limit. Using insights from our theoretical analysis we develop ``SleeperNets'' as a universal backdoor attack which exploits a newly proposed threat model and leverages dynamic reward poisoning techniques. We evaluate our attack in 6 environments spanning multiple domains and demonstrate significant improvements in attack success over existing methods, while preserving benign episodic return.





## Adaptive $Q$-Network: On-the-fly Target Selection for Deep Reinforcement Learning
- **Url**: http://arxiv.org/abs/2405.16195v2
- **Authors**: ['Th√©o Vincent', 'Fabian Wahren', 'Jan Peters', 'Boris Belousov', "Carlo D'Eramo"]
- **Abstrat**: Deep Reinforcement Learning (RL) is well known for being highly sensitive to hyperparameters, requiring practitioners substantial efforts to optimize them for the problem at hand. This also limits the applicability of RL in real-world scenarios. In recent years, the field of automated Reinforcement Learning (AutoRL) has grown in popularity by trying to address this issue. However, these approaches typically hinge on additional samples to select well-performing hyperparameters, hindering sample-efficiency and practicality. Furthermore, most AutoRL methods are heavily based on already existing AutoML methods, which were originally developed neglecting the additional challenges inherent to RL due to its non-stationarities. In this work, we propose a new approach for AutoRL, called Adaptive $Q$-Network (AdaQN), that is tailored to RL to take into account the non-stationarity of the optimization procedure without requiring additional samples. AdaQN learns several $Q$-functions, each one trained with different hyperparameters, which are updated online using the $Q$-function with the smallest approximation error as a shared target. Our selection scheme simultaneously handles different hyperparameters while coping with the non-stationarity induced by the RL optimization procedure and being orthogonal to any critic-based RL algorithm. We demonstrate that AdaQN is theoretically sound and empirically validate it in MuJoCo control problems and Atari $2600$ games, showing benefits in sample-efficiency, overall performance, robustness to stochasticity and training stability.





## Adversarial Inception for Bounded Backdoor Poisoning in Deep Reinforcement Learning
- **Url**: http://arxiv.org/abs/2410.13995v2
- **Authors**: ['Ethan Rathbun', 'Christopher Amato', 'Alina Oprea']
- **Abstrat**: Recent works have demonstrated the vulnerability of Deep Reinforcement Learning (DRL) algorithms against training-time, backdoor poisoning attacks. These attacks induce pre-determined, adversarial behavior in the agent upon observing a fixed trigger during deployment while allowing the agent to solve its intended task during training. Prior attacks rely on arbitrarily large perturbations to the agent's rewards to achieve both of these objectives -leaving them open to detection. Thus, in this work, we propose a new class of backdoor attacks against DRL which achieve state of the art performance while minimally altering the agent's rewards. These "inception" attacks train the agent to associate the targeted adversarial behavior with high returns by inducing a disjunction between the agent's chosen action and the true action executed in the environment during training. We formally define these attacks and prove they can achieve both adversarial objectives. We then devise an online inception attack which significantly out-performs prior attacks under bounded reward constraints.





## SMART: Self-learning Meta-strategy Agent for Reasoning Tasks
- **Url**: http://arxiv.org/abs/2410.16128v1
- **Authors**: ['Rongxing Liu', 'Kumar Shridhar', 'Manish Prajapat', 'Patrick Xia', 'Mrinmaya Sachan']
- **Abstrat**: Tasks requiring deductive reasoning, especially those involving multiple steps, often demand adaptive strategies such as intermediate generation of rationales or programs, as no single approach is universally optimal. While Language Models (LMs) can enhance their outputs through iterative self-refinement and strategy adjustments, they frequently fail to apply the most effective strategy in their first attempt. This inefficiency raises the question: Can LMs learn to select the optimal strategy in the first attempt, without a need for refinement? To address this challenge, we introduce SMART (Self-learning Meta-strategy Agent for Reasoning Tasks), a novel framework that enables LMs to autonomously learn and select the most effective strategies for various reasoning tasks. We model the strategy selection process as a Markov Decision Process and leverage reinforcement learning-driven continuous self-improvement to allow the model to find the suitable strategy to solve a given task. Unlike traditional self-refinement methods that rely on multiple inference passes or external feedback, SMART allows an LM to internalize the outcomes of its own reasoning processes and adjust its strategy accordingly, aiming for correct solutions on the first attempt. Our experiments across various reasoning datasets and with different model architectures demonstrate that SMART significantly enhances the ability of models to choose optimal strategies without external guidance (+15 points on the GSM8K dataset). By achieving higher accuracy with a single inference pass, SMART not only improves performance but also reduces computational costs for refinement-based strategies, paving the way for more efficient and intelligent reasoning in LMs.





## Statistical Inference for Temporal Difference Learning with Linear Function Approximation
- **Url**: http://arxiv.org/abs/2410.16106v1
- **Authors**: ['Weichen Wu', 'Gen Li', 'Yuting Wei', 'Alessandro Rinaldo']
- **Abstrat**: Statistical inference with finite-sample validity for the value function of a given policy in Markov decision processes (MDPs) is crucial for ensuring the reliability of reinforcement learning. Temporal Difference (TD) learning, arguably the most widely used algorithm for policy evaluation, serves as a natural framework for this purpose.In this paper, we study the consistency properties of TD learning with Polyak-Ruppert averaging and linear function approximation, and obtain three significant improvements over existing results. First, we derive a novel sharp high-dimensional probability convergence guarantee that depends explicitly on the asymptotic variance and holds under weak conditions. We further establish refined high-dimensional Berry-Esseen bounds over the class of convex sets that guarantee faster rates than those in the literature. Finally, we propose a plug-in estimator for the asymptotic covariance matrix, designed for efficient online computation. These results enable the construction of confidence regions and simultaneous confidence intervals for the linear parameters of the value function, with guaranteed finite-sample coverage. We demonstrate the applicability of our theoretical findings through numerical experiments.





## A New Approach to Solving SMAC Task: Generating Decision Tree Code from Large Language Models
- **Url**: http://arxiv.org/abs/2410.16024v1
- **Authors**: ['Yue Deng', 'Weiyu Ma', 'Yuxin Fan', 'Yin Zhang', 'Haifeng Zhang', 'Jian Zhao']
- **Abstrat**: StarCraft Multi-Agent Challenge (SMAC) is one of the most commonly used experimental environments in multi-agent reinforcement learning (MARL), where the specific task is to control a set number of allied units to defeat enemy forces. Traditional MARL algorithms often require interacting with the environment for up to 1 million steps to train a model, and the resulting policies are typically non-interpretable with weak transferability. In this paper, we propose a novel approach to solving SMAC tasks called LLM-SMAC. In our framework, agents leverage large language models (LLMs) to generate decision tree code by providing task descriptions. The model is further self-reflection using feedback from the rewards provided by the environment. We conduct experiments in the SMAC and demonstrate that our method can produce high-quality, interpretable decision trees with minimal environmental exploration. Moreover, these models exhibit strong transferability, successfully applying to similar SMAC environments without modification. We believe this approach offers a new direction for solving decision-making tasks in the future.





## Dynamics of Moral Behavior in Heterogeneous Populations of Learning Agents
- **Url**: http://arxiv.org/abs/2403.04202v6
- **Authors**: ['Elizaveta Tennant', 'Stephen Hailes', 'Mirco Musolesi']
- **Abstrat**: Growing concerns about safety and alignment of AI systems highlight the importance of embedding moral capabilities in artificial agents: a promising solution is the use of learning from experience, i.e., Reinforcement Learning. In multi-agent (social) environments, complex population-level phenomena may emerge from interactions between individual learning agents. Many of the existing studies rely on simulated social dilemma environments to study the interactions of independent learning agents; however, they tend to ignore the moral heterogeneity that is likely to be present in societies of agents in practice. For example, at different points in time a single learning agent may face opponents who are consequentialist (i.e., focused on maximizing outcomes over time), norm-based (i.e., conforming to specific norms), or virtue-based (i.e., considering a combination of different virtues). The extent to which agents' co-development may be impacted by such moral heterogeneity in populations is not well understood. In this paper, we present a study of the learning dynamics of morally heterogeneous populations interacting in a social dilemma setting. Using an Iterated Prisoner's Dilemma environment with a partner selection mechanism, we investigate the extent to which the prevalence of diverse moral agents in populations affects individual agents' learning behaviors and emergent population-level outcomes. We observe several types of non-trivial interactions between pro-social and anti-social agents, and find that certain types of moral agents are able to steer selfish agents towards more cooperative behavior.





## Molecular Quantum Control Algorithm Design by Reinforcement Learning
- **Url**: http://arxiv.org/abs/2410.11839v2
- **Authors**: ['Anastasia Pipi', 'Xuecheng Tao', 'Prineha Narang', 'David R. Leibrandt']
- **Abstrat**: Precision measurements of polyatomic molecules offer an unparalleled paradigm to probe physics beyond the Standard Model. The rich internal structure within these molecules makes them exquisite sensors for detecting fundamental symmetry violations, local position invariance, and dark matter. While trapping and control of diatomic and a few very simple polyatomic molecules have been experimentally demonstrated, leveraging the complex rovibrational structure of more general polyatomics demands the development of robust and efficient quantum control schemes. In this study, we present a general, reinforcement-learning-designed, quantum logic approach to prepare molecular ions in a single, pure quantum state. The reinforcement learning agent optimizes the pulse sequence, each followed by a projective measurement, and probabilistically manipulates the collapse of the quantum system to a single state. The performance of the control algorithm is numerically demonstrated in the case of a CaH$^+$ ion, with up to 96 thermally populated eigenstates and under the disturbance of environmental thermal radiation. We expect that the method developed, with physics-informed learning, will be directly implemented for quantum control of polyatomic molecular ions with densely populated structures, enabling new experimental tests of fundamental theories.





## Analyzing Closed-loop Training Techniques for Realistic Traffic Agent Models in Autonomous Highway Driving Simulations
- **Url**: http://arxiv.org/abs/2410.15987v1
- **Authors**: ['Matthias Bitzer', 'Reinis Cimurs', 'Benjamin Coors', 'Johannes Goth', 'Sebastian Ziesche', 'Philipp Geiger', 'Maximilian Naumann']
- **Abstrat**: Simulation plays a crucial role in the rapid development and safe deployment of autonomous vehicles. Realistic traffic agent models are indispensable for bridging the gap between simulation and the real world. Many existing approaches for imitating human behavior are based on learning from demonstration. However, these approaches are often constrained by focusing on individual training strategies. Therefore, to foster a broader understanding of realistic traffic agent modeling, in this paper, we provide an extensive comparative analysis of different training principles, with a focus on closed-loop methods for highway driving simulation. We experimentally compare (i) open-loop vs. closed-loop multi-agent training, (ii) adversarial vs. deterministic supervised training, (iii) the impact of reinforcement losses, and (iv) the impact of training alongside log-replayed agents to identify suitable training techniques for realistic agent modeling. Furthermore, we identify promising combinations of different closed-loop training methods.





## Learning Quadrotor Control From Visual Features Using Differentiable Simulation
- **Url**: http://arxiv.org/abs/2410.15979v1
- **Authors**: ['Johannes Heeg', 'Yunlong Song', 'Davide Scaramuzza']
- **Abstrat**: The sample inefficiency of reinforcement learning (RL) remains a significant challenge in robotics. RL requires large-scale simulation and, still, can cause long training times, slowing down research and innovation. This issue is particularly pronounced in vision-based control tasks where reliable state estimates are not accessible. Differentiable simulation offers an alternative by enabling gradient back-propagation through the dynamics model, providing low-variance analytical policy gradients and, hence, higher sample efficiency. However, its usage for real-world robotic tasks has yet been limited. This work demonstrates the great potential of differentiable simulation for learning quadrotor control. We show that training in differentiable simulation significantly outperforms model-free RL in terms of both sample efficiency and training time, allowing a policy to learn to recover a quadrotor in seconds when providing vehicle state and in minutes when relying solely on visual features. The key to our success is two-fold. First, the use of a simple surrogate model for gradient computation greatly accelerates training without sacrificing control performance. Second, combining state representation learning with policy learning enhances convergence speed in tasks where only visual features are observable. These findings highlight the potential of differentiable simulation for real-world robotics and offer a compelling alternative to conventional RL approaches.





## SLR: Learning Quadruped Locomotion without Privileged Information
- **Url**: http://arxiv.org/abs/2406.04835v2
- **Authors**: ['Shiyi Chen', 'Zeyu Wan', 'Shiyang Yan', 'Chun Zhang', 'Weiyi Zhang', 'Qiang Li', 'Debing Zhang', 'Fasih Ud Din Farrukh']
- **Abstrat**: The recent mainstream reinforcement learning control for quadruped robots often relies on privileged information, demanding meticulous selection and precise estimation, thereby imposing constraints on the development process. This work proposes a Self-learning Latent Representation (SLR) method, which achieves high-performance control policy learning without the need for privileged information. To enhance the credibility of the proposed method's evaluation, SLR was directly compared with state-of-the-art algorithms using their open-source code repositories and original configuration parameters. Remarkably, SLR surpasses the performance of previous methods using only limited proprioceptive data, demonstrating significant potential for future applications. Ultimately, the trained policy and encoder empower the quadruped robot to traverse various challenging terrains. Videos of our results can be found on our website: https://11chens.github.io/SLR/





## FlickerFusion: Intra-trajectory Domain Generalizing Multi-Agent RL
- **Url**: http://arxiv.org/abs/2410.15876v1
- **Authors**: ['Woosung Koh', 'Wonbeen Oh', 'Siyeol Kim', 'Suhin Shin', 'Hyeongjin Kim', 'Jaein Jang', 'Junghyun Lee', 'Se-Young Yun']
- **Abstrat**: Multi-agent reinforcement learning has demonstrated significant potential in addressing complex cooperative tasks across various real-world applications. However, existing MARL approaches often rely on the restrictive assumption that the number of entities (e.g., agents, obstacles) remains constant between training and inference. This overlooks scenarios where entities are dynamically removed or added during the inference trajectory -- a common occurrence in real-world environments like search and rescue missions and dynamic combat situations. In this paper, we tackle the challenge of intra-trajectory dynamic entity composition under zero-shot out-of-domain (OOD) generalization, where such dynamic changes cannot be anticipated beforehand. Our empirical studies reveal that existing MARL methods suffer significant performance degradation and increased uncertainty in these scenarios. In response, we propose FlickerFusion, a novel OOD generalization method that acts as a universally applicable augmentation technique for MARL backbone methods. Our results show that FlickerFusion not only achieves superior inference rewards but also uniquely reduces uncertainty vis-\`a-vis the backbone, compared to existing methods. For standardized evaluation, we introduce MPEv2, an enhanced version of Multi Particle Environments (MPE), consisting of 12 benchmarks. Benchmarks, implementations, and trained models are organized and open-sourced at flickerfusion305.github.io, accompanied by ample demo video renderings.





## Towards Efficient Collaboration via Graph Modeling in Reinforcement Learning
- **Url**: http://arxiv.org/abs/2410.15841v1
- **Authors**: ['Wenzhe Fan', 'Zishun Yu', 'Chengdong Ma', 'Changye Li', 'Yaodong Yang', 'Xinhua Zhang']
- **Abstrat**: In multi-agent reinforcement learning, a commonly considered paradigm is centralized training with decentralized execution. However, in this framework, decentralized execution restricts the development of coordinated policies due to the local observation limitation. In this paper, we consider the cooperation among neighboring agents during execution and formulate their interactions as a graph. Thus, we introduce a novel encoder-decoder architecture named Factor-based Multi-Agent Transformer ($f$-MAT) that utilizes a transformer to enable the communication between neighboring agents during both training and execution. By dividing agents into different overlapping groups and representing each group with a factor, $f$-MAT fulfills efficient message passing among agents through factor-based attention layers. Empirical results on networked systems such as traffic scheduling and power control demonstrate that $f$-MAT achieves superior performance compared to strong baselines, thereby paving the way for handling complex collaborative problems.





# TD3
## Long-distance Geomagnetic Navigation in GNSS-denied Environments with Deep Reinforcement Learning
- **Url**: http://arxiv.org/abs/2410.15837v1
- **Authors**: ['Wenqi Bai', 'Xiaohui Zhang', 'Shiliang Zhang', 'Songnan Yang', 'Yushuai Li', 'Tingwen Huang']
- **Abstrat**: Geomagnetic navigation has drawn increasing attention with its capacity in navigating through complex environments and its independence from external navigation services like global navigation satellite systems (GNSS). Existing studies on geomagnetic navigation, i.e., matching navigation and bionic navigation, rely on pre-stored map or extensive searches, leading to limited applicability or reduced navigation efficiency in unexplored areas. To address the issues with geomagnetic navigation in areas where GNSS is unavailable, this paper develops a deep reinforcement learning (DRL)-based mechanism, especially for long-distance geomagnetic navigation. The designed mechanism trains an agent to learn and gain the magnetoreception capacity for geomagnetic navigation, rather than using any pre-stored map or extensive and expensive searching approaches. Particularly, we integrate the geomagnetic gradient-based parallel approach into geomagnetic navigation. This integration mitigates the over-exploration of the learning agent by adjusting the geomagnetic gradient, such that the obtained gradient is aligned towards the destination. We explore the effectiveness of the proposed approach via detailed numerical simulations, where we implement twin delayed deep deterministic policy gradient (TD3) in realizing the proposed approach. The results demonstrate that our approach outperforms existing metaheuristic and bionic navigation methods in long-distance missions under diverse navigation conditions.





# Prioritized Experience Replay
# path planning
## Collaborative Goal Tracking of Multiple Mobile Robots Based on Geometric Graph Neural Network
- **Url**: http://arxiv.org/abs/2311.07105v2
- **Authors**: ['Weining Lu', 'Qingquan Lin', 'Litong Meng', 'Chenxi Li', 'Bin Liang']
- **Abstrat**: Multiple mobile robots play a significant role in various spatially distributed tasks, highlighting the importance of collaborative path planning to enhance operational efficiency. In unfamiliar and non-repetitive scenarios, reconstructing the global map can be time-inefficient and sometimes unrealistic. Therefore, research has focused on achieving real-time collaborative planning by utilizing sensor data from multiple robots located at different positions, without relying on a global map. This paper introduces a Multi-Robot Collaborative Path Planning method based on a Geometric Graph Neural Network (MRPP-GeoGNN). First, the features of each neighboring robot's sensory data are extracted, and the relative positions of neighboring robots are integrated into each interaction layer to incorporate obstacle information along with location details. Subsequently, GeoGNN maps the amalgamated local environment features to multiple forward directions for the robot's actual movement. An expert data generation method is devised for the robot to advance step by step in the physical environment, generating different expert data in ROS to train the network. We conducted both simulations and physical experiments to validate the effectiveness of the proposed method. Simulation results demonstrate approximately a 5% improvement in accuracy compared to the model based solely on CNN using expert datasets. In the ROS simulation test, the success rate is enhanced by about 4% compared to CNN, and the flow time increase is reduced by approximately 8%, surpassing other GNN models. The physical experimental results indicate that the proposed method enables the robot to navigate successfully in the actual environment and achieve the shortest average path length compared to the benchmark method.





## Efficient Non-Myopic Layered Bayesian Optimization For Large-Scale Bathymetric Informative Path Planning
- **Url**: http://arxiv.org/abs/2410.15720v1
- **Authors**: ['Alexander Kiessling', 'Ignacio Torroba', 'Chelsea Rose Sidrane', 'Ivan Stenius', 'Jana Tumova', 'John Folkesson']
- **Abstrat**: Informative path planning (IPP) applied to bathymetric mapping allows AUVs to focus on feature-rich areas to quickly reduce uncertainty and increase mapping efficiency. Existing methods based on Bayesian optimization (BO) over Gaussian Process (GP) maps work well on small scenarios but they are short-sighted and computationally heavy when mapping larger areas, hindering deployment in real applications. To overcome this, we present a 2-layered BO IPP method that performs non-myopic, real-time planning in a tree search fashion over large Stochastic Variational GP maps, while respecting the AUV motion constraints and accounting for localization uncertainty. Our framework outperforms the standard industrial lawn-mowing pattern and a myopic baseline in a set of hardware in the loop (HIL) experiments in an embedded platform over real bathymetry.





## Hierarchical Search-Based Cooperative Motion Planning
- **Url**: http://arxiv.org/abs/2410.15710v1
- **Authors**: ['Yuchen Wu', 'Yifan Yang', 'Gang Xu', 'Junjie Cao', 'Yansong Chen', 'Licheng Wen', 'Yong Liu']
- **Abstrat**: Cooperative path planning, a crucial aspect of multi-agent systems research, serves a variety of sectors, including military, agriculture, and industry. Many existing algorithms, however, come with certain limitations, such as simplified kinematic models and inadequate support for multiple group scenarios. Focusing on the planning problem associated with a nonholonomic Ackermann model for Unmanned Ground Vehicles (UGV), we propose a leaderless, hierarchical Search-Based Cooperative Motion Planning (SCMP) method. The high-level utilizes a binary conflict search tree to minimize runtime, while the low-level fabricates kinematically feasible, collision-free paths that are shape-constrained. Our algorithm can adapt to scenarios featuring multiple groups with different shapes, outlier agents, and elaborate obstacles. We conduct algorithm comparisons, performance testing, simulation, and real-world testing, verifying the effectiveness and applicability of our algorithm. The implementation of our method will be open-sourced at https://github.com/WYCUniverStar/SCMP.




