# reinforcement learning
## Advancing Humanoid Locomotion: Mastering Challenging Terrains with Denoising World Model Learning
- **Url**: http://arxiv.org/abs/2408.14472v1
- **Authors**: ['Xinyang Gu', 'Yen-Jen Wang', 'Xiang Zhu', 'Chengming Shi', 'Yanjiang Guo', 'Yichen Liu', 'Jianyu Chen']
- **Abstrat**: Humanoid robots, with their human-like skeletal structure, are especially suited for tasks in human-centric environments. However, this structure is accompanied by additional challenges in locomotion controller design, especially in complex real-world environments. As a result, existing humanoid robots are limited to relatively simple terrains, either with model-based control or model-free reinforcement learning. In this work, we introduce Denoising World Model Learning (DWL), an end-to-end reinforcement learning framework for humanoid locomotion control, which demonstrates the world's first humanoid robot to master real-world challenging terrains such as snowy and inclined land in the wild, up and down stairs, and extremely uneven terrains. All scenarios run the same learned neural network with zero-shot sim-to-real transfer, indicating the superior robustness and generalization capability of the proposed method.





## Graph Reinforcement Learning for Power Grids: A Comprehensive Survey
- **Url**: http://arxiv.org/abs/2407.04522v3
- **Authors**: ['Mohamed Hassouna', 'Clara Holzhüter', 'Pawel Lytaev', 'Josephine Thomas', 'Bernhard Sick', 'Christoph Scholz']
- **Abstrat**: The rise of renewable energy and distributed generation requires new approaches to overcome the limitations of traditional methods. In this context, Graph Neural Networks are promising due to their ability to learn from graph-structured data. Combined with Reinforcement Learning, they can serve as control approaches to determine remedial network actions. This review analyses how Graph Reinforcement Learning (GRL) can improve representation learning and decision making in power grid use cases. Although GRL has demonstrated adaptability to unpredictable events and noisy data, it is primarily at a proof-of-concept stage. We highlight open challenges and limitations with respect to real-world applications.





## Equivariant Reinforcement Learning under Partial Observability
- **Url**: http://arxiv.org/abs/2408.14336v1
- **Authors**: ['Hai Nguyen', 'Andrea Baisero', 'David Klee', 'Dian Wang', 'Robert Platt', 'Christopher Amato']
- **Abstrat**: Incorporating inductive biases is a promising approach for tackling challenging robot learning domains with sample-efficient solutions. This paper identifies partially observable domains where symmetries can be a useful inductive bias for efficient learning. Specifically, by encoding the equivariance regarding specific group symmetries into the neural networks, our actor-critic reinforcement learning agents can reuse solutions in the past for related scenarios. Consequently, our equivariant agents outperform non-equivariant approaches significantly in terms of sample efficiency and final performance, demonstrated through experiments on a range of robotic tasks in simulation and real hardware.





## Advantageous and disadvantageous inequality aversion can be taught through vicarious learning of others' preferences
- **Url**: http://arxiv.org/abs/2405.06500v2
- **Authors**: ['Shen Zhang', 'Oriel FeldmanHall', 'Sébastien Hétu', 'A. Ross Otto']
- **Abstrat**: While enforcing egalitarian social norms is critical for human society, punishing social norm violators often incurs a cost to the self. This cost looms even larger when one can benefit from an unequal distribution of resources (i.e. advantageous inequity), as in receiving a higher salary than a colleague with the identical role. In the Ultimatum Game, a classic test bed for fairness norm enforcement, individuals rarely reject (punish) such unequal proposed divisions of resources because doing so entails a sacrifice of one's own benefit. Recent work has demonstrated that observing another's punitive responses to unfairness can efficiently alter the punitive preferences of an observer. It remains an open question, however, whether such contagion is powerful enough to impart advantageous inequity aversion to individuals. Using a variant of the Ultimatum Game in which participants are tasked with responding to fairness violations on behalf of another 'Teacher' - whose aversion to advantageous (versus disadvantageous) inequity was systematically manipulated-we probe whether individuals subsequently increase their punishment unfair after experiencing fairness violations on their own behalf. In two experiments, we found individuals can acquire aversion to advantageous inequity 'vicariously' through observing (and implementing) the Teacher's preferences. Computationally, these learning effects were best characterized by a model which learns the latent structure of the Teacher's preferences, rather than a simple Reinforcement Learning account. In summary, our study is the first to demonstrate that people can swiftly and readily acquire another's preferences for advantageous inequity, suggesting in turn that behavioral contagion may be one promising mechanism through which social norm enforcement - which people rarely implement in the case of advantageous inequality - can be enhanced.





## Whole-body Humanoid Robot Locomotion with Human Reference
- **Url**: http://arxiv.org/abs/2402.18294v4
- **Authors**: ['Qiang Zhang', 'Peter Cui', 'David Yan', 'Jingkai Sun', 'Yiqun Duan', 'Gang Han', 'Wen Zhao', 'Weining Zhang', 'Yijie Guo', 'Arthur Zhang', 'Renjing Xu']
- **Abstrat**: Recently, humanoid robots have made significant advances in their ability to perform challenging tasks due to the deployment of Reinforcement Learning (RL), however, the inherent complexity of humanoid robots, including the difficulty of designing complicated reward functions and training entire sophisticated systems, still poses a notable challenge. To conquer these challenges, after many iterations and in-depth investigations, we have meticulously developed a full-size humanoid robot, "Adam", whose innovative structural design greatly improves the efficiency and effectiveness of the imitation learning process. In addition, we have developed a novel imitation learning framework based on an adversarial motion prior, which applies not only to Adam but also to humanoid robots in general. Using the framework, Adam can exhibit unprecedented human-like characteristics in locomotion tasks. Our experimental results demonstrate that the proposed framework enables Adam to achieve human-comparable performance in complex locomotion tasks, marking the first time that human locomotion data has been used for imitation learning in a full-size humanoid robot.





## Efficient Active Flow Control Strategy for Confined Square Cylinder Wake Using Deep Learning-Based Surrogate Model and Reinforcement Learning
- **Url**: http://arxiv.org/abs/2408.14232v1
- **Authors**: ['Meng Zhang', 'Mustafa Z. Yousif', 'Minze Xu', 'Haifeng Zhou', 'Linqi Yu', 'HeeChang Lim']
- **Abstrat**: This study presents a deep learning model-based reinforcement learning (DL-MBRL) approach for active control of two-dimensional (2D) wake flow past a square cylinder using antiphase jets. The DL-MBRL framework alternates between interacting with a deep learning surrogate model (DL-SM) and computational fluid dynamics (CFD) simulations to suppress wake vortex shedding, significantly reducing computational costs. The DL-SM, which combines a Transformer and a multiscale enhanced super-resolution generative adversarial network (MS-ESRGAN), effectively models complex flow dynamics, efficiently emulating the CFD environment. Trained on 2D direct numerical simulation (DNS) data, the Transformer and MS-ESRGAN demonstrated excellent agreement with DNS results, validating the DL-SM's accuracy. Error analysis suggests replacing the DL-SM with CFD every five interactions to maintain reliability. While DL-MBRL showed less robust convergence than model-free reinforcement learning (MFRL) during training, it reduced training time by 49.2%, from 41.87 hours to 20.62 hours. Both MFRL and DL-MBRL achieved a 98% reduction in shedding energy and a 95% reduction in the standard deviation of the lift coefficient (C_L). However, MFRL exhibited a nonzero mean lift coefficient due to insufficient exploration, whereas DL-MBRL improved exploration by leveraging the randomness of the DL-SM, resolving the nonzero mean C_L issue. This study demonstrates that DL-MBRL is not only comparably effective but also superior to MFRL in flow stabilization, with significantly reduced training time, highlighting the potential of combining deep reinforcement learning with DL-SM for enhanced active flow control.





## Demystifying the Recency Heuristic in Temporal-Difference Learning
- **Url**: http://arxiv.org/abs/2406.12284v2
- **Authors**: ['Brett Daley', 'Marlos C. Machado', 'Martha White']
- **Abstrat**: The recency heuristic in reinforcement learning is the assumption that stimuli that occurred closer in time to an acquired reward should be more heavily reinforced. The recency heuristic is one of the key assumptions made by TD($\lambda$), which reinforces recent experiences according to an exponentially decaying weighting. In fact, all other widely used return estimators for TD learning, such as $n$-step returns, satisfy a weaker (i.e., non-monotonic) recency heuristic. Why is the recency heuristic effective for temporal credit assignment? What happens when credit is assigned in a way that violates this heuristic? In this paper, we analyze the specific mathematical implications of adopting the recency heuristic in TD learning. We prove that any return estimator satisfying this heuristic: 1) is guaranteed to converge to the correct value function, 2) has a relatively fast contraction rate, and 3) has a long window of effective credit assignment, yet bounded worst-case variance. We also give a counterexample where on-policy, tabular TD methods violating the recency heuristic diverge. Our results offer some of the first theoretical evidence that credit assignment based on the recency heuristic facilitates learning.





## DynamicRouteGPT: A Real-Time Multi-Vehicle Dynamic Navigation Framework Based on Large Language Models
- **Url**: http://arxiv.org/abs/2408.14185v1
- **Authors**: ['Ziai Zhou', 'Bin Zhou', 'Hao Liu']
- **Abstrat**: Real-time dynamic path planning in complex traffic environments presents challenges, such as varying traffic volumes and signal wait times. Traditional static routing algorithms like Dijkstra and A* compute shortest paths but often fail under dynamic conditions. Recent Reinforcement Learning (RL) approaches offer improvements but tend to focus on local optima, risking dead-ends or boundary issues. This paper proposes a novel approach based on causal inference for real-time dynamic path planning, balancing global and local optimality. We first use the static Dijkstra algorithm to compute a globally optimal baseline path. A distributed control strategy then guides vehicles along this path. At intersections, DynamicRouteGPT performs real-time decision-making for local path selection, considering real-time traffic, driving preferences, and unexpected events. DynamicRouteGPT integrates Markov chains, Bayesian inference, and large-scale pretrained language models like Llama3 8B to provide an efficient path planning solution. It dynamically adjusts to traffic scenarios and driver preferences and requires no pre-training, offering broad applicability across road networks. A key innovation is the construction of causal graphs for counterfactual reasoning, optimizing path decisions. Experimental results show that our method achieves state-of-the-art performance in real-time dynamic path planning for multiple vehicles while providing explainable path selections, offering a novel and efficient solution for complex traffic environments.





## Averaging $n$-step Returns Reduces Variance in Reinforcement Learning
- **Url**: http://arxiv.org/abs/2402.03903v3
- **Authors**: ['Brett Daley', 'Martha White', 'Marlos C. Machado']
- **Abstrat**: Multistep returns, such as $n$-step returns and $\lambda$-returns, are commonly used to improve the sample efficiency of reinforcement learning (RL) methods. The variance of the multistep returns becomes the limiting factor in their length; looking too far into the future increases variance and reverses the benefits of multistep learning. In our work, we demonstrate the ability of compound returns -- weighted averages of $n$-step returns -- to reduce variance. We prove for the first time that any compound return with the same contraction modulus as a given $n$-step return has strictly lower variance. We additionally prove that this variance-reduction property improves the finite-sample complexity of temporal-difference learning under linear function approximation. Because general compound returns can be expensive to implement, we introduce two-bootstrap returns which reduce variance while remaining efficient, even when using minibatched experience replay. We conduct experiments showing that compound returns often increase the sample efficiency of $n$-step deep RL agents like DQN and PPO.





## Bridging the gap between Learning-to-plan, Motion Primitives and Safe Reinforcement Learning
- **Url**: http://arxiv.org/abs/2408.14063v1
- **Authors**: ['Piotr Kicki', 'Davide Tateo', 'Puze Liu', 'Jonas Guenster', 'Jan Peters', 'Krzysztof Walas']
- **Abstrat**: Trajectory planning under kinodynamic constraints is fundamental for advanced robotics applications that require dexterous, reactive, and rapid skills in complex environments. These constraints, which may represent task, safety, or actuator limitations, are essential for ensuring the proper functioning of robotic platforms and preventing unexpected behaviors. Recent advances in kinodynamic planning demonstrate that learning-to-plan techniques can generate complex and reactive motions under intricate constraints. However, these techniques necessitate the analytical modeling of both the robot and the entire task, a limiting assumption when systems are extremely complex or when constructing accurate task models is prohibitive. This paper addresses this limitation by combining learning-to-plan methods with reinforcement learning, resulting in a novel integration of black-box learning of motion primitives and optimization. We evaluate our approach against state-of-the-art safe reinforcement learning methods, showing that our technique, particularly when exploiting task structure, outperforms baseline methods in challenging scenarios such as planning to hit in robot air hockey. This work demonstrates the potential of our integrated approach to enhance the performance and safety of robots operating under complex kinodynamic constraints.





## Compressed Federated Reinforcement Learning with a Generative Model
- **Url**: http://arxiv.org/abs/2404.10635v4
- **Authors**: ['Ali Beikmohammadi', 'Sarit Khirirat', 'Sindri Magnússon']
- **Abstrat**: Reinforcement learning has recently gained unprecedented popularity, yet it still grapples with sample inefficiency. Addressing this challenge, federated reinforcement learning (FedRL) has emerged, wherein agents collaboratively learn a single policy by aggregating local estimations. However, this aggregation step incurs significant communication costs. In this paper, we propose CompFedRL, a communication-efficient FedRL approach incorporating both \textit{periodic aggregation} and (direct/error-feedback) compression mechanisms. Specifically, we consider compressed federated $Q$-learning with a generative model setup, where a central server learns an optimal $Q$-function by periodically aggregating compressed $Q$-estimates from local agents. For the first time, we characterize the impact of these two mechanisms (which have remained elusive) by providing a finite-time analysis of our algorithm, demonstrating strong convergence behaviors when utilizing either direct or error-feedback compression. Our bounds indicate improved solution accuracy concerning the number of agents and other federated hyperparameters while simultaneously reducing communication costs. To corroborate our theory, we also conduct in-depth numerical experiments to verify our findings, considering Top-$K$ and Sparsified-$K$ sparsification operators.





## Reinforcement Learning for Versatile, Dynamic, and Robust Bipedal Locomotion Control
- **Url**: http://arxiv.org/abs/2401.16889v2
- **Authors**: ['Zhongyu Li', 'Xue Bin Peng', 'Pieter Abbeel', 'Sergey Levine', 'Glen Berseth', 'Koushil Sreenath']
- **Abstrat**: This paper presents a comprehensive study on using deep reinforcement learning (RL) to create dynamic locomotion controllers for bipedal robots. Going beyond focusing on a single locomotion skill, we develop a general control solution that can be used for a range of dynamic bipedal skills, from periodic walking and running to aperiodic jumping and standing. Our RL-based controller incorporates a novel dual-history architecture, utilizing both a long-term and short-term input/output (I/O) history of the robot. This control architecture, when trained through the proposed end-to-end RL approach, consistently outperforms other methods across a diverse range of skills in both simulation and the real world. The study also delves into the adaptivity and robustness introduced by the proposed RL system in developing locomotion controllers. We demonstrate that the proposed architecture can adapt to both time-invariant dynamics shifts and time-variant changes, such as contact events, by effectively using the robot's I/O history. Additionally, we identify task randomization as another key source of robustness, fostering better task generalization and compliance to disturbances. The resulting control policies can be successfully deployed on Cassie, a torque-controlled human-sized bipedal robot. This work pushes the limits of agility for bipedal robots through extensive real-world experiments. We demonstrate a diverse range of locomotion skills, including: robust standing, versatile walking, fast running with a demonstration of a 400-meter dash, and a diverse set of jumping skills, such as standing long jumps and high jumps.





## Optimizing TD3 for 7-DOF Robotic Arm Grasping: Overcoming Suboptimality with Exploration-Enhanced Contrastive Learning
- **Url**: http://arxiv.org/abs/2408.14009v1
- **Authors**: ['Wen-Han Hsieh', 'Jen-Yuan Chang']
- **Abstrat**: In actor-critic-based reinforcement learning algorithms such as Twin Delayed Deep Deterministic policy gradient (TD3), insufficient exploration of the spatial space can result in suboptimal policies when controlling 7-DOF robotic arms. To address this issue, we propose a novel Exploration-Enhanced Contrastive Learning (EECL) module that improves exploration by providing additional rewards for encountering novel states. Our module stores previously explored states in a buffer and identifies new states by comparing them with historical data using Euclidean distance within a K-dimensional tree (KDTree) framework. When the agent explores new states, exploration rewards are assigned. These rewards are then integrated into the TD3 algorithm, ensuring that the Q-learning process incorporates these signals, promoting more effective strategy optimization. We evaluate our method on the robosuite panda lift task, demonstrating that it significantly outperforms the baseline TD3 in terms of both efficiency and convergence speed in the tested environment.





## Quantitative Representation of Scenario Difficulty for Autonomous Driving Based on Adversarial Policy Search
- **Url**: http://arxiv.org/abs/2408.14000v1
- **Authors**: ['Shuo Yang', 'Caojun Wang', 'Yuanjian Zhang', 'Yuming Yin', 'Yanjun Huang', 'Shengbo Eben Li', 'Hong Chen']
- **Abstrat**: Adversarial scenario generation is crucial for autonomous driving testing because it can efficiently simulate various challenge and complex traffic conditions. However, it is difficult to control current existing methods to generate desired scenarios, such as the ones with different conflict levels. Therefore, this paper proposes a data-driven quantitative method to represent scenario difficulty. Compared with rule-based discrete scenario difficulty representation method, the proposed algorithm can achieve continuous difficulty representation. Specifically, the environment agent is introduced, and a reinforcement learning method combined with mechanism knowledge is constructed for policy search to obtain an agent with adversarial behavior. The model parameters of the environment agent at different stages in the training process are extracted to construct a policy group, and then the agents with different adversarial intensity are obtained, which are used to realize data generation in different difficulty scenarios through the simulation environment. Finally, a data-driven scenario difficulty quantitative representation model is constructed, which is used to output the environment agent policy under different difficulties. The result analysis shows that the proposed algorithm can generate reasonable and interpretable scenarios with high discrimination, and can provide quantifiable difficulty representation without any expert logic rule design. The video link is https://www.youtube.com/watch?v=GceGdqAm9Ys.





## Research Advances and New Paradigms for Biology-inspired Spiking Neural Networks
- **Url**: http://arxiv.org/abs/2408.13996v1
- **Authors**: ['Tianyu Zheng', 'Liyuan Han', 'Tielin Zhang']
- **Abstrat**: Spiking neural networks (SNNs) are gaining popularity in the computational simulation and artificial intelligence fields owing to their biological plausibility and computational efficiency. This paper explores the historical development of SNN and concludes that these two fields are intersecting and merging rapidly. Following the successful application of Dynamic Vision Sensors (DVS) and Dynamic Audio Sensors (DAS), SNNs have found some proper paradigms, such as continuous visual signal tracking, automatic speech recognition, and reinforcement learning for continuous control, that have extensively supported their key features, including spike encoding, neuronal heterogeneity, specific functional circuits, and multiscale plasticity. Compared to these real-world paradigms, the brain contains a spiking version of the biology-world paradigm, which exhibits a similar level of complexity and is usually considered a mirror of the real world. Considering the projected rapid development of invasive and parallel Brain-Computer Interface (BCI), as well as the new BCI-based paradigms that include online pattern recognition and stimulus control of biological spike trains, SNNs naturally leverage their advantages in energy efficiency, robustness, and flexibility. The biological brain has inspired the present study of SNNs and effective SNN machine-learning algorithms, which can help enhance neuroscience discoveries in the brain by applying them to the new BCI paradigm. Such two-way interactions with positive feedback can accelerate brain science research and brain-inspired intelligence technology.





# TD3
## Optimizing TD3 for 7-DOF Robotic Arm Grasping: Overcoming Suboptimality with Exploration-Enhanced Contrastive Learning
- **Url**: http://arxiv.org/abs/2408.14009v1
- **Authors**: ['Wen-Han Hsieh', 'Jen-Yuan Chang']
- **Abstrat**: In actor-critic-based reinforcement learning algorithms such as Twin Delayed Deep Deterministic policy gradient (TD3), insufficient exploration of the spatial space can result in suboptimal policies when controlling 7-DOF robotic arms. To address this issue, we propose a novel Exploration-Enhanced Contrastive Learning (EECL) module that improves exploration by providing additional rewards for encountering novel states. Our module stores previously explored states in a buffer and identifies new states by comparing them with historical data using Euclidean distance within a K-dimensional tree (KDTree) framework. When the agent explores new states, exploration rewards are assigned. These rewards are then integrated into the TD3 algorithm, ensuring that the Q-learning process incorporates these signals, promoting more effective strategy optimization. We evaluate our method on the robosuite panda lift task, demonstrating that it significantly outperforms the baseline TD3 in terms of both efficiency and convergence speed in the tested environment.





# Prioritized Experience Replay
# path planning
## Evaluating Large Language Models on Spatial Tasks: A Multi-Task Benchmarking Study
- **Url**: http://arxiv.org/abs/2408.14438v1
- **Authors**: ['Liuchang Xu Shuo Zhao', 'Qingming Lin', 'Luyao Chen', 'Qianqian Luo', 'Sensen Wu', 'Xinyue Ye', 'Hailin Feng', 'Zhenhong Du']
- **Abstrat**: The advent of large language models such as ChatGPT, Gemini, and others has underscored the importance of evaluating their diverse capabilities, ranging from natural language understanding to code generation. However, their performance on spatial tasks has not been comprehensively assessed. This study addresses this gap by introducing a novel multi-task spatial evaluation dataset, designed to systematically explore and compare the performance of several advanced models on spatial tasks. The dataset encompasses twelve distinct task types, including spatial understanding and path planning, each with verified, accurate answers. We evaluated multiple models, including OpenAI's gpt-3.5-turbo, gpt-4o, and ZhipuAI's glm-4, through a two-phase testing approach. Initially, we conducted zero-shot testing, followed by categorizing the dataset by difficulty and performing prompt tuning tests. Results indicate that gpt-4o achieved the highest overall accuracy in the first phase, with an average of 71.3%. Although moonshot-v1-8k slightly underperformed overall, it surpassed gpt-4o in place name recognition tasks. The study also highlights the impact of prompt strategies on model performance in specific tasks. For example, the Chain-of-Thought (COT) strategy increased gpt-4o's accuracy in path planning from 12.4% to 87.5%, while a one-shot strategy enhanced moonshot-v1-8k's accuracy in mapping tasks from 10.1% to 76.3%.





## Model Predictive Parkour Control of a Monoped Hopper in Dynamically Changing Environments
- **Url**: http://arxiv.org/abs/2408.14362v1
- **Authors**: ['Maximilian Albracht', 'Shivesh Kumar', 'Shubham Vyas', 'Frank Kirchner']
- **Abstrat**: A great advantage of legged robots is their ability to operate on particularly difficult and obstructed terrain, which demands dynamic, robust, and precise movements. The study of obstacle courses provides invaluable insights into the challenges legged robots face, offering a controlled environment to assess and enhance their capabilities. Traversing it with a one-legged hopper introduces intricate challenges, such as planning over contacts and dealing with flight phases, which necessitates a sophisticated controller. A novel model predictive parkour controller is introduced, that finds an optimal path through a real-time changing obstacle course with mixed integer motion planning. The execution of this optimized path is then achieved through a state machine employing a PD control scheme with feedforward torques, ensuring robust and accurate performance.





## DynamicRouteGPT: A Real-Time Multi-Vehicle Dynamic Navigation Framework Based on Large Language Models
- **Url**: http://arxiv.org/abs/2408.14185v1
- **Authors**: ['Ziai Zhou', 'Bin Zhou', 'Hao Liu']
- **Abstrat**: Real-time dynamic path planning in complex traffic environments presents challenges, such as varying traffic volumes and signal wait times. Traditional static routing algorithms like Dijkstra and A* compute shortest paths but often fail under dynamic conditions. Recent Reinforcement Learning (RL) approaches offer improvements but tend to focus on local optima, risking dead-ends or boundary issues. This paper proposes a novel approach based on causal inference for real-time dynamic path planning, balancing global and local optimality. We first use the static Dijkstra algorithm to compute a globally optimal baseline path. A distributed control strategy then guides vehicles along this path. At intersections, DynamicRouteGPT performs real-time decision-making for local path selection, considering real-time traffic, driving preferences, and unexpected events. DynamicRouteGPT integrates Markov chains, Bayesian inference, and large-scale pretrained language models like Llama3 8B to provide an efficient path planning solution. It dynamically adjusts to traffic scenarios and driver preferences and requires no pre-training, offering broad applicability across road networks. A key innovation is the construction of causal graphs for counterfactual reasoning, optimizing path decisions. Experimental results show that our method achieves state-of-the-art performance in real-time dynamic path planning for multiple vehicles while providing explainable path selections, offering a novel and efficient solution for complex traffic environments.





## Combining Safe Intervals and RRT* for Efficient Multi-Robot Path Planning in Complex Environments
- **Url**: http://arxiv.org/abs/2404.01752v2
- **Authors**: ['Joonyeol Sim', 'Joonkyung Kim', 'Changjoo Nam']
- **Abstrat**: In this paper, we consider the problem of Multi-Robot Path Planning (MRPP) in continuous space to find conflict-free paths. The difficulty of the problem arises from two primary factors. First, the involvement of multiple robots leads to combinatorial decision-making, which escalates the search space exponentially. Second, the continuous space presents potentially infinite states and actions. For this problem, we propose a two-level approach where the low level is a sampling-based planner Safe Interval RRT* (SI-RRT*) that finds a collision-free trajectory for individual robots. The high level can use any method that can resolve inter-robot conflicts where we employ two representative methods that are Prioritized Planning (SI-CPP) and Conflict Based Search (SI-CCBS). Experimental results show that SI-RRT* can find a high-quality solution quickly with a small number of samples. SI-CPP exhibits improved scalability while SI-CCBS produces higher-quality solutions compared to the state-of-the-art planners for continuous space. Compared to the most scalable existing algorithm, SI-CPP achieves a success rate that is up to 94% higher with 100 robots while maintaining solution quality (i.e., flowtime, the sum of travel times of all robots) without significant compromise. SI-CPP also decreases the makespan up to 45%. SI-CCBS decreases the flowtime by 9% compared to the competitor, albeit exhibiting a 14% lower success rate.




